{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_resnet_18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3SjeQ6q7AbF"
      },
      "source": [
        "import math\n",
        "from tensorflow.keras import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lvc5yKZBH30"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 10\n",
        "image_height = 32\n",
        "image_width = 32\n",
        "channels = 3\n",
        "save_model_dir = \"saved_model/model\"\n",
        "dataset_dir = \"dataset/\"\n",
        "train_dir = dataset_dir + \"train\"\n",
        "valid_dir = dataset_dir + \"valid\"\n",
        "test_dir = dataset_dir + \"test\"\n",
        "\n",
        "# choose a network\n",
        "# model = \"resnet18\"\n",
        "# model = \"resnet34\"\n",
        "model = \"resnet50\"\n",
        "# model = \"resnet101\"\n",
        "# model = \"resnet152\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3tA8T3u8VwR"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BasicBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=stride,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=1,\n",
        "                                            padding=\"same\")\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        if stride != 1:\n",
        "            self.downsample = tf.keras.Sequential()\n",
        "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                                       kernel_size=(1, 1),\n",
        "                                                       strides=stride))\n",
        "            self.downsample.add(tf.keras.layers.BatchNormalization())\n",
        "        else:\n",
        "            self.downsample = lambda x: x\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        residual = self.downsample(inputs)\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PreActBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, filter_num, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=stride,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                            kernel_size=(3, 3),\n",
        "                                            strides=1,\n",
        "                                            padding=\"same\")\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        if stride != 1:\n",
        "            self.downsample = tf.keras.Sequential()\n",
        "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
        "                                                       kernel_size=(1, 1),\n",
        "                                                       strides=stride))\n",
        "        else:\n",
        "            self.downsample = lambda x: x\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        x = tf.nn.relu(self.bn1(inputs, training=training))\n",
        "        residual = self.downsample(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def make_basic_block_layer(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(BasicBlock(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(BasicBlock(filter_num, stride=1))\n",
        "\n",
        "    return res_block\n",
        "\n",
        "\n",
        "def make_preact_block_layer(filter_num, blocks, stride=1):\n",
        "    res_block = tf.keras.Sequential()\n",
        "    res_block.add(PreActBlock(filter_num, stride=stride))\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        res_block.add(PreActBlock(filter_num, stride=1))\n",
        "\n",
        "    return res_block\n",
        "\n",
        "class ResNetTypeI(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(ResNetTypeI, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_basic_block_layer(filter_num=64,\n",
        "                                             blocks=layer_params[0])\n",
        "        self.layer2 = make_basic_block_layer(filter_num=128,\n",
        "                                             blocks=layer_params[1],\n",
        "                                             stride=2)\n",
        "        self.layer3 = make_basic_block_layer(filter_num=256,\n",
        "                                             blocks=layer_params[2],\n",
        "                                             stride=2)\n",
        "        self.layer4 = make_basic_block_layer(filter_num=512,\n",
        "                                             blocks=layer_params[3],\n",
        "                                             stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PreActResNet(tf.keras.Model):\n",
        "    def __init__(self, layer_params):\n",
        "        super(PreActResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64,\n",
        "                                            kernel_size=(7, 7),\n",
        "                                            strides=2,\n",
        "                                            padding=\"same\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
        "                                               strides=2,\n",
        "                                               padding=\"same\")\n",
        "\n",
        "        self.layer1 = make_preact_block_layer(filter_num=64,\n",
        "                                             blocks=layer_params[0])\n",
        "        self.layer2 = make_preact_block_layer(filter_num=128,\n",
        "                                             blocks=layer_params[1],\n",
        "                                             stride=2)\n",
        "        self.layer3 = make_preact_block_layer(filter_num=256,\n",
        "                                             blocks=layer_params[2],\n",
        "                                             stride=2)\n",
        "        self.layer4 = make_preact_block_layer(filter_num=512,\n",
        "                                             blocks=layer_params[3],\n",
        "                                             stride=2)\n",
        "\n",
        "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES, activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x, training=training)\n",
        "        x = self.layer2(x, training=training)\n",
        "        x = self.layer3(x, training=training)\n",
        "        x = self.layer4(x, training=training)\n",
        "        x = self.avgpool(x)\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq1ydYSCBPoq",
        "outputId": "9508889e-be08-4ecb-84b6-4d6fd67c6c5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "def resnet_18():\n",
        "    return ResNetTypeI(layer_params=[2, 2, 2, 2])\n",
        "\n",
        "def preact_resnet_18():\n",
        "    return PreActResNet(layer_params=[2, 2, 2, 2])\n",
        "\n",
        "def get_model():\n",
        "    model = preact_resnet_18()\n",
        "    model.build(input_shape=(None, image_height, image_width, channels))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = get_model()\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"pre_act_res_net_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_61 (Conv2D)           multiple                  9472      \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc multiple                  256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
            "_________________________________________________________________\n",
            "sequential_22 (Sequential)   (None, 8, 8, 64)          148736    \n",
            "_________________________________________________________________\n",
            "sequential_23 (Sequential)   (None, 4, 4, 128)         526720    \n",
            "_________________________________________________________________\n",
            "sequential_25 (Sequential)   (None, 2, 2, 256)         2102016   \n",
            "_________________________________________________________________\n",
            "sequential_27 (Sequential)   (None, 1, 1, 512)         8398336   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  5130      \n",
            "=================================================================\n",
            "Total params: 11,190,666\n",
            "Trainable params: 11,183,754\n",
            "Non-trainable params: 6,912\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfLEC5M4CYMy",
        "outputId": "443efea9-48ec-4a97-c885-dd51191ed56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\n",
        "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images/255, train_labels)).batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adadelta()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "for epoch in range(3):\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        # valid_loss.reset_states()\n",
        "        # valid_accuracy.reset_states()\n",
        "        step = 0\n",
        "        for images, labels in train_dataset:            \n",
        "            step += 1\n",
        "            train_step(images, labels)\n",
        "            print(\"Epoch: {}/{}, step: {}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
        "                                                                                     10,\n",
        "                                                                                     step,\n",
        "                                                                                    #  math.ceil(train_count / config.BATCH_SIZE),\n",
        "                                                                                     train_loss.result(),\n",
        "                                                                                     train_accuracy.result()))\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \".format(epoch + 1,\n",
        "                                                                  10,\n",
        "                                                                  train_loss.result(),\n",
        "                                                                  train_accuracy.result()\n",
        "                                                                 ))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 1/10, step: 1116, loss: 2.64442, accuracy: 0.11940\n",
            "Epoch: 1/10, step: 1117, loss: 2.64455, accuracy: 0.11929\n",
            "Epoch: 1/10, step: 1118, loss: 2.64493, accuracy: 0.11930\n",
            "Epoch: 1/10, step: 1119, loss: 2.64490, accuracy: 0.11919\n",
            "Epoch: 1/10, step: 1120, loss: 2.64508, accuracy: 0.11920\n",
            "Epoch: 1/10, step: 1121, loss: 2.64489, accuracy: 0.11920\n",
            "Epoch: 1/10, step: 1122, loss: 2.64465, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1123, loss: 2.64488, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1124, loss: 2.64461, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1125, loss: 2.64463, accuracy: 0.11900\n",
            "Epoch: 1/10, step: 1126, loss: 2.64475, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1127, loss: 2.64497, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1128, loss: 2.64545, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1129, loss: 2.64585, accuracy: 0.11891\n",
            "Epoch: 1/10, step: 1130, loss: 2.64567, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1131, loss: 2.64590, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1132, loss: 2.64568, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1133, loss: 2.64573, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1134, loss: 2.64601, accuracy: 0.11883\n",
            "Epoch: 1/10, step: 1135, loss: 2.64588, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1136, loss: 2.64585, accuracy: 0.11906\n",
            "Epoch: 1/10, step: 1137, loss: 2.64564, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1138, loss: 2.64516, accuracy: 0.11907\n",
            "Epoch: 1/10, step: 1139, loss: 2.64504, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1140, loss: 2.64495, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1141, loss: 2.64497, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1142, loss: 2.64487, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1143, loss: 2.64491, accuracy: 0.11866\n",
            "Epoch: 1/10, step: 1144, loss: 2.64466, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1145, loss: 2.64453, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1146, loss: 2.64448, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1147, loss: 2.64501, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1148, loss: 2.64484, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1149, loss: 2.64449, accuracy: 0.11880\n",
            "Epoch: 1/10, step: 1150, loss: 2.64485, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1151, loss: 2.64470, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1152, loss: 2.64421, accuracy: 0.11882\n",
            "Epoch: 1/10, step: 1153, loss: 2.64432, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1154, loss: 2.64411, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1155, loss: 2.64440, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1156, loss: 2.64429, accuracy: 0.11884\n",
            "Epoch: 1/10, step: 1157, loss: 2.64455, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1158, loss: 2.64455, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1159, loss: 2.64442, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1160, loss: 2.64425, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1161, loss: 2.64445, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1162, loss: 2.64449, accuracy: 0.11887\n",
            "Epoch: 1/10, step: 1163, loss: 2.64423, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1164, loss: 2.64383, accuracy: 0.11909\n",
            "Epoch: 1/10, step: 1165, loss: 2.64381, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1166, loss: 2.64384, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1167, loss: 2.64382, accuracy: 0.11922\n",
            "Epoch: 1/10, step: 1168, loss: 2.64380, accuracy: 0.11933\n",
            "Epoch: 1/10, step: 1169, loss: 2.64413, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1170, loss: 2.64414, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1171, loss: 2.64482, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1172, loss: 2.64489, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1173, loss: 2.64462, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1174, loss: 2.64441, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1175, loss: 2.64427, accuracy: 0.11926\n",
            "Epoch: 1/10, step: 1176, loss: 2.64390, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1177, loss: 2.64368, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1178, loss: 2.64425, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1179, loss: 2.64379, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1180, loss: 2.64371, accuracy: 0.11939\n",
            "Epoch: 1/10, step: 1181, loss: 2.64417, accuracy: 0.11928\n",
            "Epoch: 1/10, step: 1182, loss: 2.64459, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1183, loss: 2.64422, accuracy: 0.11929\n",
            "Epoch: 1/10, step: 1184, loss: 2.64415, accuracy: 0.11919\n",
            "Epoch: 1/10, step: 1185, loss: 2.64390, accuracy: 0.11909\n",
            "Epoch: 1/10, step: 1186, loss: 2.64413, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1187, loss: 2.64402, accuracy: 0.11900\n",
            "Epoch: 1/10, step: 1188, loss: 2.64404, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1189, loss: 2.64404, accuracy: 0.11901\n",
            "Epoch: 1/10, step: 1190, loss: 2.64381, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1191, loss: 2.64388, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1192, loss: 2.64499, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1193, loss: 2.64496, accuracy: 0.11934\n",
            "Epoch: 1/10, step: 1194, loss: 2.64526, accuracy: 0.11924\n",
            "Epoch: 1/10, step: 1195, loss: 2.64574, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1196, loss: 2.64604, accuracy: 0.11915\n",
            "Epoch: 1/10, step: 1197, loss: 2.64593, accuracy: 0.11905\n",
            "Epoch: 1/10, step: 1198, loss: 2.64565, accuracy: 0.11916\n",
            "Epoch: 1/10, step: 1199, loss: 2.64553, accuracy: 0.11916\n",
            "Epoch: 1/10, step: 1200, loss: 2.64559, accuracy: 0.11917\n",
            "Epoch: 1/10, step: 1201, loss: 2.64549, accuracy: 0.11917\n",
            "Epoch: 1/10, step: 1202, loss: 2.64568, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1203, loss: 2.64539, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1204, loss: 2.64506, accuracy: 0.11919\n",
            "Epoch: 1/10, step: 1205, loss: 2.64546, accuracy: 0.11909\n",
            "Epoch: 1/10, step: 1206, loss: 2.64551, accuracy: 0.11909\n",
            "Epoch: 1/10, step: 1207, loss: 2.64547, accuracy: 0.11910\n",
            "Epoch: 1/10, step: 1208, loss: 2.64548, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1209, loss: 2.64530, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1210, loss: 2.64570, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1211, loss: 2.64618, accuracy: 0.11901\n",
            "Epoch: 1/10, step: 1212, loss: 2.64578, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1213, loss: 2.64582, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1214, loss: 2.64531, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1215, loss: 2.64524, accuracy: 0.11914\n",
            "Epoch: 1/10, step: 1216, loss: 2.64545, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1217, loss: 2.64560, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1218, loss: 2.64562, accuracy: 0.11884\n",
            "Epoch: 1/10, step: 1219, loss: 2.64589, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1220, loss: 2.64568, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1221, loss: 2.64599, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1222, loss: 2.64670, accuracy: 0.11866\n",
            "Epoch: 1/10, step: 1223, loss: 2.64689, accuracy: 0.11856\n",
            "Epoch: 1/10, step: 1224, loss: 2.64714, accuracy: 0.11857\n",
            "Epoch: 1/10, step: 1225, loss: 2.64684, accuracy: 0.11867\n",
            "Epoch: 1/10, step: 1226, loss: 2.64669, accuracy: 0.11868\n",
            "Epoch: 1/10, step: 1227, loss: 2.64639, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1228, loss: 2.64593, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1229, loss: 2.64593, accuracy: 0.11910\n",
            "Epoch: 1/10, step: 1230, loss: 2.64565, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1231, loss: 2.64578, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1232, loss: 2.64605, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1233, loss: 2.64594, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1234, loss: 2.64587, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1235, loss: 2.64622, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1236, loss: 2.64588, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1237, loss: 2.64605, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1238, loss: 2.64614, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1239, loss: 2.64634, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1240, loss: 2.64656, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1241, loss: 2.64648, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1242, loss: 2.64620, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1243, loss: 2.64570, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1244, loss: 2.64553, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1245, loss: 2.64525, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1246, loss: 2.64581, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1247, loss: 2.64599, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1248, loss: 2.64602, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1249, loss: 2.64620, accuracy: 0.11859\n",
            "Epoch: 1/10, step: 1250, loss: 2.64572, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1251, loss: 2.64567, accuracy: 0.11871\n",
            "Epoch: 1/10, step: 1252, loss: 2.64590, accuracy: 0.11861\n",
            "Epoch: 1/10, step: 1253, loss: 2.64542, accuracy: 0.11862\n",
            "Epoch: 1/10, step: 1254, loss: 2.64611, accuracy: 0.11852\n",
            "Epoch: 1/10, step: 1255, loss: 2.64599, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1256, loss: 2.64607, accuracy: 0.11853\n",
            "Epoch: 1/10, step: 1257, loss: 2.64596, accuracy: 0.11864\n",
            "Epoch: 1/10, step: 1258, loss: 2.64582, accuracy: 0.11874\n",
            "Epoch: 1/10, step: 1259, loss: 2.64555, accuracy: 0.11884\n",
            "Epoch: 1/10, step: 1260, loss: 2.64548, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1261, loss: 2.64591, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1262, loss: 2.64572, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1263, loss: 2.64560, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1264, loss: 2.64575, accuracy: 0.11867\n",
            "Epoch: 1/10, step: 1265, loss: 2.64577, accuracy: 0.11858\n",
            "Epoch: 1/10, step: 1266, loss: 2.64534, accuracy: 0.11858\n",
            "Epoch: 1/10, step: 1267, loss: 2.64515, accuracy: 0.11859\n",
            "Epoch: 1/10, step: 1268, loss: 2.64493, accuracy: 0.11859\n",
            "Epoch: 1/10, step: 1269, loss: 2.64459, accuracy: 0.11860\n",
            "Epoch: 1/10, step: 1270, loss: 2.64455, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1271, loss: 2.64501, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1272, loss: 2.64513, accuracy: 0.11832\n",
            "Epoch: 1/10, step: 1273, loss: 2.64489, accuracy: 0.11852\n",
            "Epoch: 1/10, step: 1274, loss: 2.64464, accuracy: 0.11852\n",
            "Epoch: 1/10, step: 1275, loss: 2.64443, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1276, loss: 2.64484, accuracy: 0.11853\n",
            "Epoch: 1/10, step: 1277, loss: 2.64507, accuracy: 0.11864\n",
            "Epoch: 1/10, step: 1278, loss: 2.64512, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1279, loss: 2.64524, accuracy: 0.11845\n",
            "Epoch: 1/10, step: 1280, loss: 2.64512, accuracy: 0.11836\n",
            "Epoch: 1/10, step: 1281, loss: 2.64494, accuracy: 0.11836\n",
            "Epoch: 1/10, step: 1282, loss: 2.64528, accuracy: 0.11837\n",
            "Epoch: 1/10, step: 1283, loss: 2.64498, accuracy: 0.11828\n",
            "Epoch: 1/10, step: 1284, loss: 2.64492, accuracy: 0.11828\n",
            "Epoch: 1/10, step: 1285, loss: 2.64475, accuracy: 0.11829\n",
            "Epoch: 1/10, step: 1286, loss: 2.64437, accuracy: 0.11839\n",
            "Epoch: 1/10, step: 1287, loss: 2.64428, accuracy: 0.11830\n",
            "Epoch: 1/10, step: 1288, loss: 2.64395, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1289, loss: 2.64374, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1290, loss: 2.64336, accuracy: 0.11851\n",
            "Epoch: 1/10, step: 1291, loss: 2.64356, accuracy: 0.11851\n",
            "Epoch: 1/10, step: 1292, loss: 2.64379, accuracy: 0.11842\n",
            "Epoch: 1/10, step: 1293, loss: 2.64366, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1294, loss: 2.64341, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1295, loss: 2.64324, accuracy: 0.11853\n",
            "Epoch: 1/10, step: 1296, loss: 2.64257, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1297, loss: 2.64271, accuracy: 0.11864\n",
            "Epoch: 1/10, step: 1298, loss: 2.64260, accuracy: 0.11864\n",
            "Epoch: 1/10, step: 1299, loss: 2.64224, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1300, loss: 2.64215, accuracy: 0.11865\n",
            "Epoch: 1/10, step: 1301, loss: 2.64207, accuracy: 0.11856\n",
            "Epoch: 1/10, step: 1302, loss: 2.64202, accuracy: 0.11847\n",
            "Epoch: 1/10, step: 1303, loss: 2.64215, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1304, loss: 2.64162, accuracy: 0.11867\n",
            "Epoch: 1/10, step: 1305, loss: 2.64159, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1306, loss: 2.64152, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1307, loss: 2.64169, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1308, loss: 2.64175, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1309, loss: 2.64158, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1310, loss: 2.64122, accuracy: 0.11889\n",
            "Epoch: 1/10, step: 1311, loss: 2.64144, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1312, loss: 2.64171, accuracy: 0.11881\n",
            "Epoch: 1/10, step: 1313, loss: 2.64184, accuracy: 0.11881\n",
            "Epoch: 1/10, step: 1314, loss: 2.64146, accuracy: 0.11891\n",
            "Epoch: 1/10, step: 1315, loss: 2.64182, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1316, loss: 2.64159, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1317, loss: 2.64116, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1318, loss: 2.64129, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1319, loss: 2.64136, accuracy: 0.11903\n",
            "Epoch: 1/10, step: 1320, loss: 2.64167, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1321, loss: 2.64198, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1322, loss: 2.64170, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1323, loss: 2.64177, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1324, loss: 2.64156, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1325, loss: 2.64155, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1326, loss: 2.64179, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1327, loss: 2.64220, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1328, loss: 2.64209, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1329, loss: 2.64208, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1330, loss: 2.64206, accuracy: 0.11880\n",
            "Epoch: 1/10, step: 1331, loss: 2.64204, accuracy: 0.11880\n",
            "Epoch: 1/10, step: 1332, loss: 2.64202, accuracy: 0.11871\n",
            "Epoch: 1/10, step: 1333, loss: 2.64165, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1334, loss: 2.64160, accuracy: 0.11891\n",
            "Epoch: 1/10, step: 1335, loss: 2.64172, accuracy: 0.11901\n",
            "Epoch: 1/10, step: 1336, loss: 2.64208, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1337, loss: 2.64198, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1338, loss: 2.64181, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1339, loss: 2.64155, accuracy: 0.11884\n",
            "Epoch: 1/10, step: 1340, loss: 2.64160, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1341, loss: 2.64137, accuracy: 0.11866\n",
            "Epoch: 1/10, step: 1342, loss: 2.64141, accuracy: 0.11867\n",
            "Epoch: 1/10, step: 1343, loss: 2.64174, accuracy: 0.11858\n",
            "Epoch: 1/10, step: 1344, loss: 2.64174, accuracy: 0.11849\n",
            "Epoch: 1/10, step: 1345, loss: 2.64168, accuracy: 0.11849\n",
            "Epoch: 1/10, step: 1346, loss: 2.64137, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1347, loss: 2.64087, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1348, loss: 2.64104, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1349, loss: 2.64078, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1350, loss: 2.64060, accuracy: 0.11861\n",
            "Epoch: 1/10, step: 1351, loss: 2.64083, accuracy: 0.11852\n",
            "Epoch: 1/10, step: 1352, loss: 2.64056, accuracy: 0.11862\n",
            "Epoch: 1/10, step: 1353, loss: 2.64014, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1354, loss: 2.64010, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1355, loss: 2.64025, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1356, loss: 2.64056, accuracy: 0.11846\n",
            "Epoch: 1/10, step: 1357, loss: 2.64060, accuracy: 0.11837\n",
            "Epoch: 1/10, step: 1358, loss: 2.64033, accuracy: 0.11846\n",
            "Epoch: 1/10, step: 1359, loss: 2.64025, accuracy: 0.11847\n",
            "Epoch: 1/10, step: 1360, loss: 2.64030, accuracy: 0.11838\n",
            "Epoch: 1/10, step: 1361, loss: 2.64020, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1362, loss: 2.64031, accuracy: 0.11839\n",
            "Epoch: 1/10, step: 1363, loss: 2.64003, accuracy: 0.11840\n",
            "Epoch: 1/10, step: 1364, loss: 2.63968, accuracy: 0.11840\n",
            "Epoch: 1/10, step: 1365, loss: 2.63959, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1366, loss: 2.63981, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1367, loss: 2.63972, accuracy: 0.11832\n",
            "Epoch: 1/10, step: 1368, loss: 2.63987, accuracy: 0.11824\n",
            "Epoch: 1/10, step: 1369, loss: 2.63990, accuracy: 0.11815\n",
            "Epoch: 1/10, step: 1370, loss: 2.63979, accuracy: 0.11825\n",
            "Epoch: 1/10, step: 1371, loss: 2.63991, accuracy: 0.11816\n",
            "Epoch: 1/10, step: 1372, loss: 2.64009, accuracy: 0.11808\n",
            "Epoch: 1/10, step: 1373, loss: 2.63999, accuracy: 0.11799\n",
            "Epoch: 1/10, step: 1374, loss: 2.63995, accuracy: 0.11790\n",
            "Epoch: 1/10, step: 1375, loss: 2.63951, accuracy: 0.11800\n",
            "Epoch: 1/10, step: 1376, loss: 2.63948, accuracy: 0.11801\n",
            "Epoch: 1/10, step: 1377, loss: 2.63952, accuracy: 0.11810\n",
            "Epoch: 1/10, step: 1378, loss: 2.63918, accuracy: 0.11820\n",
            "Epoch: 1/10, step: 1379, loss: 2.63906, accuracy: 0.11820\n",
            "Epoch: 1/10, step: 1380, loss: 2.63937, accuracy: 0.11812\n",
            "Epoch: 1/10, step: 1381, loss: 2.63934, accuracy: 0.11812\n",
            "Epoch: 1/10, step: 1382, loss: 2.63906, accuracy: 0.11804\n",
            "Epoch: 1/10, step: 1383, loss: 2.63892, accuracy: 0.11813\n",
            "Epoch: 1/10, step: 1384, loss: 2.63946, accuracy: 0.11805\n",
            "Epoch: 1/10, step: 1385, loss: 2.63979, accuracy: 0.11796\n",
            "Epoch: 1/10, step: 1386, loss: 2.63966, accuracy: 0.11788\n",
            "Epoch: 1/10, step: 1387, loss: 2.63969, accuracy: 0.11788\n",
            "Epoch: 1/10, step: 1388, loss: 2.63946, accuracy: 0.11789\n",
            "Epoch: 1/10, step: 1389, loss: 2.63920, accuracy: 0.11807\n",
            "Epoch: 1/10, step: 1390, loss: 2.63921, accuracy: 0.11799\n",
            "Epoch: 1/10, step: 1391, loss: 2.63914, accuracy: 0.11790\n",
            "Epoch: 1/10, step: 1392, loss: 2.63900, accuracy: 0.11791\n",
            "Epoch: 1/10, step: 1393, loss: 2.63893, accuracy: 0.11782\n",
            "Epoch: 1/10, step: 1394, loss: 2.63855, accuracy: 0.11792\n",
            "Epoch: 1/10, step: 1395, loss: 2.63850, accuracy: 0.11801\n",
            "Epoch: 1/10, step: 1396, loss: 2.63831, accuracy: 0.11802\n",
            "Epoch: 1/10, step: 1397, loss: 2.63815, accuracy: 0.11802\n",
            "Epoch: 1/10, step: 1398, loss: 2.63791, accuracy: 0.11803\n",
            "Epoch: 1/10, step: 1399, loss: 2.63762, accuracy: 0.11812\n",
            "Epoch: 1/10, step: 1400, loss: 2.63761, accuracy: 0.11812\n",
            "Epoch: 1/10, step: 1401, loss: 2.63765, accuracy: 0.11822\n",
            "Epoch: 1/10, step: 1402, loss: 2.63753, accuracy: 0.11831\n",
            "Epoch: 1/10, step: 1403, loss: 2.63724, accuracy: 0.11832\n",
            "Epoch: 1/10, step: 1404, loss: 2.63699, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1405, loss: 2.63708, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1406, loss: 2.63673, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1407, loss: 2.63681, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1408, loss: 2.63683, accuracy: 0.11834\n",
            "Epoch: 1/10, step: 1409, loss: 2.63651, accuracy: 0.11844\n",
            "Epoch: 1/10, step: 1410, loss: 2.63636, accuracy: 0.11853\n",
            "Epoch: 1/10, step: 1411, loss: 2.63657, accuracy: 0.11844\n",
            "Epoch: 1/10, step: 1412, loss: 2.63681, accuracy: 0.11836\n",
            "Epoch: 1/10, step: 1413, loss: 2.63639, accuracy: 0.11845\n",
            "Epoch: 1/10, step: 1414, loss: 2.63647, accuracy: 0.11837\n",
            "Epoch: 1/10, step: 1415, loss: 2.63671, accuracy: 0.11837\n",
            "Epoch: 1/10, step: 1416, loss: 2.63667, accuracy: 0.11829\n",
            "Epoch: 1/10, step: 1417, loss: 2.63717, accuracy: 0.11821\n",
            "Epoch: 1/10, step: 1418, loss: 2.63733, accuracy: 0.11812\n",
            "Epoch: 1/10, step: 1419, loss: 2.63728, accuracy: 0.11804\n",
            "Epoch: 1/10, step: 1420, loss: 2.63717, accuracy: 0.11813\n",
            "Epoch: 1/10, step: 1421, loss: 2.63682, accuracy: 0.11823\n",
            "Epoch: 1/10, step: 1422, loss: 2.63641, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1423, loss: 2.63628, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1424, loss: 2.63618, accuracy: 0.11842\n",
            "Epoch: 1/10, step: 1425, loss: 2.63633, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1426, loss: 2.63623, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1427, loss: 2.63666, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1428, loss: 2.63723, accuracy: 0.11835\n",
            "Epoch: 1/10, step: 1429, loss: 2.63710, accuracy: 0.11835\n",
            "Epoch: 1/10, step: 1430, loss: 2.63714, accuracy: 0.11844\n",
            "Epoch: 1/10, step: 1431, loss: 2.63723, accuracy: 0.11845\n",
            "Epoch: 1/10, step: 1432, loss: 2.63697, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1433, loss: 2.63672, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1434, loss: 2.63670, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1435, loss: 2.63661, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1436, loss: 2.63639, accuracy: 0.11856\n",
            "Epoch: 1/10, step: 1437, loss: 2.63640, accuracy: 0.11865\n",
            "Epoch: 1/10, step: 1438, loss: 2.63666, accuracy: 0.11865\n",
            "Epoch: 1/10, step: 1439, loss: 2.63680, accuracy: 0.11866\n",
            "Epoch: 1/10, step: 1440, loss: 2.63648, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1441, loss: 2.63654, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1442, loss: 2.63584, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1443, loss: 2.63576, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1444, loss: 2.63615, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1445, loss: 2.63615, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1446, loss: 2.63622, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1447, loss: 2.63587, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1448, loss: 2.63591, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1449, loss: 2.63597, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1450, loss: 2.63575, accuracy: 0.11862\n",
            "Epoch: 1/10, step: 1451, loss: 2.63562, accuracy: 0.11863\n",
            "Epoch: 1/10, step: 1452, loss: 2.63587, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1453, loss: 2.63569, accuracy: 0.11846\n",
            "Epoch: 1/10, step: 1454, loss: 2.63596, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1455, loss: 2.63601, accuracy: 0.11847\n",
            "Epoch: 1/10, step: 1456, loss: 2.63586, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1457, loss: 2.63611, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1458, loss: 2.63603, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1459, loss: 2.63601, accuracy: 0.11857\n",
            "Epoch: 1/10, step: 1460, loss: 2.63574, accuracy: 0.11858\n",
            "Epoch: 1/10, step: 1461, loss: 2.63598, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1462, loss: 2.63599, accuracy: 0.11850\n",
            "Epoch: 1/10, step: 1463, loss: 2.63587, accuracy: 0.11851\n",
            "Epoch: 1/10, step: 1464, loss: 2.63596, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1465, loss: 2.63608, accuracy: 0.11852\n",
            "Epoch: 1/10, step: 1466, loss: 2.63590, accuracy: 0.11861\n",
            "Epoch: 1/10, step: 1467, loss: 2.63561, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1468, loss: 2.63579, accuracy: 0.11870\n",
            "Epoch: 1/10, step: 1469, loss: 2.63570, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1470, loss: 2.63539, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1471, loss: 2.63533, accuracy: 0.11880\n",
            "Epoch: 1/10, step: 1472, loss: 2.63507, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1473, loss: 2.63517, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1474, loss: 2.63553, accuracy: 0.11889\n",
            "Epoch: 1/10, step: 1475, loss: 2.63542, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1476, loss: 2.63553, accuracy: 0.11882\n",
            "Epoch: 1/10, step: 1477, loss: 2.63553, accuracy: 0.11882\n",
            "Epoch: 1/10, step: 1478, loss: 2.63526, accuracy: 0.11883\n",
            "Epoch: 1/10, step: 1479, loss: 2.63495, accuracy: 0.11883\n",
            "Epoch: 1/10, step: 1480, loss: 2.63487, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1481, loss: 2.63529, accuracy: 0.11867\n",
            "Epoch: 1/10, step: 1482, loss: 2.63545, accuracy: 0.11859\n",
            "Epoch: 1/10, step: 1483, loss: 2.63483, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1484, loss: 2.63520, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1485, loss: 2.63504, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1486, loss: 2.63529, accuracy: 0.11869\n",
            "Epoch: 1/10, step: 1487, loss: 2.63507, accuracy: 0.11861\n",
            "Epoch: 1/10, step: 1488, loss: 2.63495, accuracy: 0.11853\n",
            "Epoch: 1/10, step: 1489, loss: 2.63497, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1490, loss: 2.63452, accuracy: 0.11862\n",
            "Epoch: 1/10, step: 1491, loss: 2.63529, accuracy: 0.11854\n",
            "Epoch: 1/10, step: 1492, loss: 2.63498, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1493, loss: 2.63499, accuracy: 0.11855\n",
            "Epoch: 1/10, step: 1494, loss: 2.63519, accuracy: 0.11847\n",
            "Epoch: 1/10, step: 1495, loss: 2.63521, accuracy: 0.11848\n",
            "Epoch: 1/10, step: 1496, loss: 2.63519, accuracy: 0.11840\n",
            "Epoch: 1/10, step: 1497, loss: 2.63511, accuracy: 0.11832\n",
            "Epoch: 1/10, step: 1498, loss: 2.63501, accuracy: 0.11841\n",
            "Epoch: 1/10, step: 1499, loss: 2.63510, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1500, loss: 2.63497, accuracy: 0.11833\n",
            "Epoch: 1/10, step: 1501, loss: 2.63474, accuracy: 0.11842\n",
            "Epoch: 1/10, step: 1502, loss: 2.63479, accuracy: 0.11843\n",
            "Epoch: 1/10, step: 1503, loss: 2.63437, accuracy: 0.11851\n",
            "Epoch: 1/10, step: 1504, loss: 2.63403, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1505, loss: 2.63395, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1506, loss: 2.63372, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1507, loss: 2.63374, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1508, loss: 2.63392, accuracy: 0.11887\n",
            "Epoch: 1/10, step: 1509, loss: 2.63381, accuracy: 0.11887\n",
            "Epoch: 1/10, step: 1510, loss: 2.63392, accuracy: 0.11879\n",
            "Epoch: 1/10, step: 1511, loss: 2.63366, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1512, loss: 2.63350, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1513, loss: 2.63329, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1514, loss: 2.63335, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1515, loss: 2.63321, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1516, loss: 2.63322, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1517, loss: 2.63291, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1518, loss: 2.63312, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1519, loss: 2.63356, accuracy: 0.11891\n",
            "Epoch: 1/10, step: 1520, loss: 2.63340, accuracy: 0.11900\n",
            "Epoch: 1/10, step: 1521, loss: 2.63332, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1522, loss: 2.63346, accuracy: 0.11892\n",
            "Epoch: 1/10, step: 1523, loss: 2.63379, accuracy: 0.11884\n",
            "Epoch: 1/10, step: 1524, loss: 2.63350, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1525, loss: 2.63323, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1526, loss: 2.63331, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1527, loss: 2.63337, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1528, loss: 2.63362, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1529, loss: 2.63355, accuracy: 0.11887\n",
            "Epoch: 1/10, step: 1530, loss: 2.63355, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1531, loss: 2.63340, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1532, loss: 2.63313, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1533, loss: 2.63296, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1534, loss: 2.63308, accuracy: 0.11930\n",
            "Epoch: 1/10, step: 1535, loss: 2.63272, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1536, loss: 2.63250, accuracy: 0.11947\n",
            "Epoch: 1/10, step: 1537, loss: 2.63238, accuracy: 0.11939\n",
            "Epoch: 1/10, step: 1538, loss: 2.63260, accuracy: 0.11931\n",
            "Epoch: 1/10, step: 1539, loss: 2.63243, accuracy: 0.11931\n",
            "Epoch: 1/10, step: 1540, loss: 2.63225, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1541, loss: 2.63215, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1542, loss: 2.63230, accuracy: 0.11924\n",
            "Epoch: 1/10, step: 1543, loss: 2.63271, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1544, loss: 2.63274, accuracy: 0.11917\n",
            "Epoch: 1/10, step: 1545, loss: 2.63311, accuracy: 0.11909\n",
            "Epoch: 1/10, step: 1546, loss: 2.63367, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1547, loss: 2.63368, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1548, loss: 2.63370, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1549, loss: 2.63323, accuracy: 0.11927\n",
            "Epoch: 1/10, step: 1550, loss: 2.63317, accuracy: 0.11927\n",
            "Epoch: 1/10, step: 1551, loss: 2.63316, accuracy: 0.11928\n",
            "Epoch: 1/10, step: 1552, loss: 2.63355, accuracy: 0.11920\n",
            "Epoch: 1/10, step: 1553, loss: 2.63397, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1554, loss: 2.63371, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1555, loss: 2.63357, accuracy: 0.11921\n",
            "Epoch: 1/10, step: 1556, loss: 2.63317, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1557, loss: 2.63282, accuracy: 0.11946\n",
            "Epoch: 1/10, step: 1558, loss: 2.63287, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1559, loss: 2.63247, accuracy: 0.11939\n",
            "Epoch: 1/10, step: 1560, loss: 2.63248, accuracy: 0.11939\n",
            "Epoch: 1/10, step: 1561, loss: 2.63228, accuracy: 0.11947\n",
            "Epoch: 1/10, step: 1562, loss: 2.63264, accuracy: 0.11940\n",
            "Epoch: 1/10, step: 1563, loss: 2.63259, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1564, loss: 2.63295, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1565, loss: 2.63257, accuracy: 0.11933\n",
            "Epoch: 1/10, step: 1566, loss: 2.63209, accuracy: 0.11941\n",
            "Epoch: 1/10, step: 1567, loss: 2.63222, accuracy: 0.11934\n",
            "Epoch: 1/10, step: 1568, loss: 2.63211, accuracy: 0.11926\n",
            "Epoch: 1/10, step: 1569, loss: 2.63197, accuracy: 0.11926\n",
            "Epoch: 1/10, step: 1570, loss: 2.63189, accuracy: 0.11927\n",
            "Epoch: 1/10, step: 1571, loss: 2.63174, accuracy: 0.11927\n",
            "Epoch: 1/10, step: 1572, loss: 2.63134, accuracy: 0.11935\n",
            "Epoch: 1/10, step: 1573, loss: 2.63110, accuracy: 0.11936\n",
            "Epoch: 1/10, step: 1574, loss: 2.63086, accuracy: 0.11936\n",
            "Epoch: 1/10, step: 1575, loss: 2.63051, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1576, loss: 2.63088, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1577, loss: 2.63097, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1578, loss: 2.63100, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1579, loss: 2.63048, accuracy: 0.11954\n",
            "Epoch: 1/10, step: 1580, loss: 2.63040, accuracy: 0.11954\n",
            "Epoch: 1/10, step: 1581, loss: 2.63043, accuracy: 0.11947\n",
            "Epoch: 1/10, step: 1582, loss: 2.63025, accuracy: 0.11939\n",
            "Epoch: 1/10, step: 1583, loss: 2.63021, accuracy: 0.11931\n",
            "Epoch: 1/10, step: 1584, loss: 2.63002, accuracy: 0.11924\n",
            "Epoch: 1/10, step: 1585, loss: 2.63011, accuracy: 0.11924\n",
            "Epoch: 1/10, step: 1586, loss: 2.62975, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1587, loss: 2.62965, accuracy: 0.11917\n",
            "Epoch: 1/10, step: 1588, loss: 2.62983, accuracy: 0.11910\n",
            "Epoch: 1/10, step: 1589, loss: 2.62961, accuracy: 0.11910\n",
            "Epoch: 1/10, step: 1590, loss: 2.62944, accuracy: 0.11910\n",
            "Epoch: 1/10, step: 1591, loss: 2.62943, accuracy: 0.11903\n",
            "Epoch: 1/10, step: 1592, loss: 2.62973, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1593, loss: 2.62961, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1594, loss: 2.62980, accuracy: 0.11880\n",
            "Epoch: 1/10, step: 1595, loss: 2.62965, accuracy: 0.11881\n",
            "Epoch: 1/10, step: 1596, loss: 2.62980, accuracy: 0.11873\n",
            "Epoch: 1/10, step: 1597, loss: 2.62940, accuracy: 0.11882\n",
            "Epoch: 1/10, step: 1598, loss: 2.62950, accuracy: 0.11874\n",
            "Epoch: 1/10, step: 1599, loss: 2.62953, accuracy: 0.11882\n",
            "Epoch: 1/10, step: 1600, loss: 2.62981, accuracy: 0.11883\n",
            "Epoch: 1/10, step: 1601, loss: 2.63033, accuracy: 0.11875\n",
            "Epoch: 1/10, step: 1602, loss: 2.63064, accuracy: 0.11868\n",
            "Epoch: 1/10, step: 1603, loss: 2.63038, accuracy: 0.11876\n",
            "Epoch: 1/10, step: 1604, loss: 2.63030, accuracy: 0.11877\n",
            "Epoch: 1/10, step: 1605, loss: 2.62997, accuracy: 0.11893\n",
            "Epoch: 1/10, step: 1606, loss: 2.63001, accuracy: 0.11885\n",
            "Epoch: 1/10, step: 1607, loss: 2.63035, accuracy: 0.11878\n",
            "Epoch: 1/10, step: 1608, loss: 2.63012, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1609, loss: 2.62979, accuracy: 0.11886\n",
            "Epoch: 1/10, step: 1610, loss: 2.62930, accuracy: 0.11887\n",
            "Epoch: 1/10, step: 1611, loss: 2.62900, accuracy: 0.11903\n",
            "Epoch: 1/10, step: 1612, loss: 2.62892, accuracy: 0.11903\n",
            "Epoch: 1/10, step: 1613, loss: 2.62932, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1614, loss: 2.62951, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1615, loss: 2.62948, accuracy: 0.11889\n",
            "Epoch: 1/10, step: 1616, loss: 2.62953, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1617, loss: 2.62951, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1618, loss: 2.62952, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1619, loss: 2.62925, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1620, loss: 2.62961, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1621, loss: 2.62935, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1622, loss: 2.62948, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1623, loss: 2.62921, accuracy: 0.11899\n",
            "Epoch: 1/10, step: 1624, loss: 2.62888, accuracy: 0.11915\n",
            "Epoch: 1/10, step: 1625, loss: 2.62904, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1626, loss: 2.62895, accuracy: 0.11916\n",
            "Epoch: 1/10, step: 1627, loss: 2.62916, accuracy: 0.11908\n",
            "Epoch: 1/10, step: 1628, loss: 2.62875, accuracy: 0.11924\n",
            "Epoch: 1/10, step: 1629, loss: 2.62900, accuracy: 0.11917\n",
            "Epoch: 1/10, step: 1630, loss: 2.62911, accuracy: 0.11925\n",
            "Epoch: 1/10, step: 1631, loss: 2.62897, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1632, loss: 2.62895, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1633, loss: 2.62869, accuracy: 0.11918\n",
            "Epoch: 1/10, step: 1634, loss: 2.62865, accuracy: 0.11911\n",
            "Epoch: 1/10, step: 1635, loss: 2.62865, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1636, loss: 2.62879, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1637, loss: 2.62859, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1638, loss: 2.62851, accuracy: 0.11905\n",
            "Epoch: 1/10, step: 1639, loss: 2.62846, accuracy: 0.11905\n",
            "Epoch: 1/10, step: 1640, loss: 2.62822, accuracy: 0.11913\n",
            "Epoch: 1/10, step: 1641, loss: 2.62839, accuracy: 0.11906\n",
            "Epoch: 1/10, step: 1642, loss: 2.62816, accuracy: 0.11914\n",
            "Epoch: 1/10, step: 1643, loss: 2.62866, accuracy: 0.11907\n",
            "Epoch: 1/10, step: 1644, loss: 2.62882, accuracy: 0.11907\n",
            "Epoch: 1/10, step: 1645, loss: 2.62883, accuracy: 0.11907\n",
            "Epoch: 1/10, step: 1646, loss: 2.62873, accuracy: 0.11923\n",
            "Epoch: 1/10, step: 1647, loss: 2.62887, accuracy: 0.11916\n",
            "Epoch: 1/10, step: 1648, loss: 2.62888, accuracy: 0.11908\n",
            "Epoch: 1/10, step: 1649, loss: 2.62904, accuracy: 0.11901\n",
            "Epoch: 1/10, step: 1650, loss: 2.62908, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1651, loss: 2.62891, accuracy: 0.11894\n",
            "Epoch: 1/10, step: 1652, loss: 2.62869, accuracy: 0.11902\n",
            "Epoch: 1/10, step: 1653, loss: 2.62875, accuracy: 0.11895\n",
            "Epoch: 1/10, step: 1654, loss: 2.62890, accuracy: 0.11888\n",
            "Epoch: 1/10, step: 1655, loss: 2.62879, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1656, loss: 2.62849, accuracy: 0.11904\n",
            "Epoch: 1/10, step: 1657, loss: 2.62862, accuracy: 0.11896\n",
            "Epoch: 1/10, step: 1658, loss: 2.62857, accuracy: 0.11897\n",
            "Epoch: 1/10, step: 1659, loss: 2.62832, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1660, loss: 2.62832, accuracy: 0.11883\n",
            "Epoch: 1/10, step: 1661, loss: 2.62820, accuracy: 0.11890\n",
            "Epoch: 1/10, step: 1662, loss: 2.62804, accuracy: 0.11898\n",
            "Epoch: 1/10, step: 1663, loss: 2.62806, accuracy: 0.11906\n",
            "Epoch: 1/10, step: 1664, loss: 2.62771, accuracy: 0.11929\n",
            "Epoch: 1/10, step: 1665, loss: 2.62760, accuracy: 0.11944\n",
            "Epoch: 1/10, step: 1666, loss: 2.62739, accuracy: 0.11945\n",
            "Epoch: 1/10, step: 1667, loss: 2.62770, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1668, loss: 2.62744, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1669, loss: 2.62768, accuracy: 0.11938\n",
            "Epoch: 1/10, step: 1670, loss: 2.62757, accuracy: 0.11931\n",
            "Epoch: 1/10, step: 1671, loss: 2.62783, accuracy: 0.11931\n",
            "Epoch: 1/10, step: 1672, loss: 2.62771, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1673, loss: 2.62775, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1674, loss: 2.62790, accuracy: 0.11932\n",
            "Epoch: 1/10, step: 1675, loss: 2.62785, accuracy: 0.11933\n",
            "Epoch: 1/10, step: 1676, loss: 2.62792, accuracy: 0.11926\n",
            "Epoch: 1/10, step: 1677, loss: 2.62805, accuracy: 0.11919\n",
            "Epoch: 1/10, step: 1678, loss: 2.62819, accuracy: 0.11912\n",
            "Epoch: 1/10, step: 1679, loss: 2.62796, accuracy: 0.11919\n",
            "Epoch: 1/10, step: 1680, loss: 2.62796, accuracy: 0.11920\n",
            "Epoch: 1/10, step: 1681, loss: 2.62788, accuracy: 0.11935\n",
            "Epoch: 1/10, step: 1682, loss: 2.62780, accuracy: 0.11943\n",
            "Epoch: 1/10, step: 1683, loss: 2.62755, accuracy: 0.11950\n",
            "Epoch: 1/10, step: 1684, loss: 2.62754, accuracy: 0.11951\n",
            "Epoch: 1/10, step: 1685, loss: 2.62753, accuracy: 0.11944\n",
            "Epoch: 1/10, step: 1686, loss: 2.62746, accuracy: 0.11951\n",
            "Epoch: 1/10, step: 1687, loss: 2.62748, accuracy: 0.11944\n",
            "Epoch: 1/10, step: 1688, loss: 2.62759, accuracy: 0.11952\n",
            "Epoch: 1/10, step: 1689, loss: 2.62736, accuracy: 0.11967\n",
            "Epoch: 1/10, step: 1690, loss: 2.62716, accuracy: 0.11967\n",
            "Epoch: 1/10, step: 1691, loss: 2.62741, accuracy: 0.11960\n",
            "Epoch: 1/10, step: 1692, loss: 2.62700, accuracy: 0.11968\n",
            "Epoch: 1/10, step: 1693, loss: 2.62713, accuracy: 0.11961\n",
            "Epoch: 1/10, step: 1694, loss: 2.62691, accuracy: 0.11976\n",
            "Epoch: 1/10, step: 1695, loss: 2.62677, accuracy: 0.11969\n",
            "Epoch: 1/10, step: 1696, loss: 2.62664, accuracy: 0.11962\n",
            "Epoch: 1/10, step: 1697, loss: 2.62653, accuracy: 0.11970\n",
            "Epoch: 1/10, step: 1698, loss: 2.62685, accuracy: 0.11970\n",
            "Epoch: 1/10, step: 1699, loss: 2.62723, accuracy: 0.11963\n",
            "Epoch: 1/10, step: 1700, loss: 2.62766, accuracy: 0.11956\n",
            "Epoch: 1/10, step: 1701, loss: 2.62751, accuracy: 0.11964\n",
            "Epoch: 1/10, step: 1702, loss: 2.62743, accuracy: 0.11957\n",
            "Epoch: 1/10, step: 1703, loss: 2.62751, accuracy: 0.11950\n",
            "Epoch: 1/10, step: 1704, loss: 2.62717, accuracy: 0.11950\n",
            "Epoch: 1/10, step: 1705, loss: 2.62745, accuracy: 0.11943\n",
            "Epoch: 1/10, step: 1706, loss: 2.62731, accuracy: 0.11936\n",
            "Epoch: 1/10, step: 1707, loss: 2.62713, accuracy: 0.11936\n",
            "Epoch: 1/10, step: 1708, loss: 2.62731, accuracy: 0.11929\n",
            "Epoch: 1/10, step: 1709, loss: 2.62710, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1710, loss: 2.62711, accuracy: 0.11930\n",
            "Epoch: 1/10, step: 1711, loss: 2.62695, accuracy: 0.11937\n",
            "Epoch: 1/10, step: 1712, loss: 2.62652, accuracy: 0.11945\n",
            "Epoch: 1/10, step: 1713, loss: 2.62654, accuracy: 0.11945\n",
            "Epoch: 1/10, step: 1714, loss: 2.62669, accuracy: 0.11946\n",
            "Epoch: 1/10, step: 1715, loss: 2.62665, accuracy: 0.11953\n",
            "Epoch: 1/10, step: 1716, loss: 2.62675, accuracy: 0.11946\n",
            "Epoch: 1/10, step: 1717, loss: 2.62652, accuracy: 0.11954\n",
            "Epoch: 1/10, step: 1718, loss: 2.62637, accuracy: 0.11947\n",
            "Epoch: 1/10, step: 1719, loss: 2.62625, accuracy: 0.11947\n",
            "Epoch: 1/10, step: 1720, loss: 2.62628, accuracy: 0.11955\n",
            "Epoch: 1/10, step: 1721, loss: 2.62620, accuracy: 0.11963\n",
            "Epoch: 1/10, step: 1722, loss: 2.62622, accuracy: 0.11963\n",
            "Epoch: 1/10, step: 1723, loss: 2.62606, accuracy: 0.11978\n",
            "Epoch: 1/10, step: 1724, loss: 2.62622, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1725, loss: 2.62590, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1726, loss: 2.62628, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1727, loss: 2.62588, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1728, loss: 2.62565, accuracy: 0.12008\n",
            "Epoch: 1/10, step: 1729, loss: 2.62532, accuracy: 0.12023\n",
            "Epoch: 1/10, step: 1730, loss: 2.62536, accuracy: 0.12016\n",
            "Epoch: 1/10, step: 1731, loss: 2.62529, accuracy: 0.12016\n",
            "Epoch: 1/10, step: 1732, loss: 2.62528, accuracy: 0.12016\n",
            "Epoch: 1/10, step: 1733, loss: 2.62518, accuracy: 0.12017\n",
            "Epoch: 1/10, step: 1734, loss: 2.62473, accuracy: 0.12024\n",
            "Epoch: 1/10, step: 1735, loss: 2.62482, accuracy: 0.12017\n",
            "Epoch: 1/10, step: 1736, loss: 2.62477, accuracy: 0.12010\n",
            "Epoch: 1/10, step: 1737, loss: 2.62509, accuracy: 0.12003\n",
            "Epoch: 1/10, step: 1738, loss: 2.62552, accuracy: 0.12004\n",
            "Epoch: 1/10, step: 1739, loss: 2.62534, accuracy: 0.12004\n",
            "Epoch: 1/10, step: 1740, loss: 2.62534, accuracy: 0.12004\n",
            "Epoch: 1/10, step: 1741, loss: 2.62569, accuracy: 0.11997\n",
            "Epoch: 1/10, step: 1742, loss: 2.62593, accuracy: 0.11998\n",
            "Epoch: 1/10, step: 1743, loss: 2.62586, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1744, loss: 2.62583, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1745, loss: 2.62591, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1746, loss: 2.62587, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1747, loss: 2.62547, accuracy: 0.11999\n",
            "Epoch: 1/10, step: 1748, loss: 2.62514, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1749, loss: 2.62543, accuracy: 0.12000\n",
            "Epoch: 1/10, step: 1750, loss: 2.62540, accuracy: 0.12000\n",
            "Epoch: 1/10, step: 1751, loss: 2.62559, accuracy: 0.11993\n",
            "Epoch: 1/10, step: 1752, loss: 2.62573, accuracy: 0.11993\n",
            "Epoch: 1/10, step: 1753, loss: 2.62575, accuracy: 0.11987\n",
            "Epoch: 1/10, step: 1754, loss: 2.62583, accuracy: 0.11980\n",
            "Epoch: 1/10, step: 1755, loss: 2.62591, accuracy: 0.11973\n",
            "Epoch: 1/10, step: 1756, loss: 2.62606, accuracy: 0.11973\n",
            "Epoch: 1/10, step: 1757, loss: 2.62608, accuracy: 0.11966\n",
            "Epoch: 1/10, step: 1758, loss: 2.62580, accuracy: 0.11988\n",
            "Epoch: 1/10, step: 1759, loss: 2.62568, accuracy: 0.11988\n",
            "Epoch: 1/10, step: 1760, loss: 2.62571, accuracy: 0.11982\n",
            "Epoch: 1/10, step: 1761, loss: 2.62585, accuracy: 0.11975\n",
            "Epoch: 1/10, step: 1762, loss: 2.62584, accuracy: 0.11975\n",
            "Epoch: 1/10, step: 1763, loss: 2.62550, accuracy: 0.11990\n",
            "Epoch: 1/10, step: 1764, loss: 2.62551, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1765, loss: 2.62551, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1766, loss: 2.62522, accuracy: 0.11990\n",
            "Epoch: 1/10, step: 1767, loss: 2.62539, accuracy: 0.11984\n",
            "Epoch: 1/10, step: 1768, loss: 2.62541, accuracy: 0.11984\n",
            "Epoch: 1/10, step: 1769, loss: 2.62563, accuracy: 0.11977\n",
            "Epoch: 1/10, step: 1770, loss: 2.62558, accuracy: 0.11977\n",
            "Epoch: 1/10, step: 1771, loss: 2.62557, accuracy: 0.11978\n",
            "Epoch: 1/10, step: 1772, loss: 2.62522, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1773, loss: 2.62525, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1774, loss: 2.62530, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1775, loss: 2.62509, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1776, loss: 2.62490, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1777, loss: 2.62483, accuracy: 0.11979\n",
            "Epoch: 1/10, step: 1778, loss: 2.62476, accuracy: 0.11987\n",
            "Epoch: 1/10, step: 1779, loss: 2.62493, accuracy: 0.11987\n",
            "Epoch: 1/10, step: 1780, loss: 2.62491, accuracy: 0.11980\n",
            "Epoch: 1/10, step: 1781, loss: 2.62487, accuracy: 0.11974\n",
            "Epoch: 1/10, step: 1782, loss: 2.62511, accuracy: 0.11974\n",
            "Epoch: 1/10, step: 1783, loss: 2.62540, accuracy: 0.11967\n",
            "Epoch: 1/10, step: 1784, loss: 2.62512, accuracy: 0.11982\n",
            "Epoch: 1/10, step: 1785, loss: 2.62519, accuracy: 0.11975\n",
            "Epoch: 1/10, step: 1786, loss: 2.62503, accuracy: 0.11975\n",
            "Epoch: 1/10, step: 1787, loss: 2.62511, accuracy: 0.11982\n",
            "Epoch: 1/10, step: 1788, loss: 2.62516, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1789, loss: 2.62527, accuracy: 0.11976\n",
            "Epoch: 1/10, step: 1790, loss: 2.62531, accuracy: 0.11976\n",
            "Epoch: 1/10, step: 1791, loss: 2.62547, accuracy: 0.11977\n",
            "Epoch: 1/10, step: 1792, loss: 2.62567, accuracy: 0.11984\n",
            "Epoch: 1/10, step: 1793, loss: 2.62548, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1794, loss: 2.62546, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1795, loss: 2.62545, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1796, loss: 2.62550, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1797, loss: 2.62546, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1798, loss: 2.62513, accuracy: 0.11999\n",
            "Epoch: 1/10, step: 1799, loss: 2.62499, accuracy: 0.12000\n",
            "Epoch: 1/10, step: 1800, loss: 2.62473, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1801, loss: 2.62489, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1802, loss: 2.62500, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1803, loss: 2.62477, accuracy: 0.12008\n",
            "Epoch: 1/10, step: 1804, loss: 2.62442, accuracy: 0.12008\n",
            "Epoch: 1/10, step: 1805, loss: 2.62455, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1806, loss: 2.62447, accuracy: 0.11995\n",
            "Epoch: 1/10, step: 1807, loss: 2.62454, accuracy: 0.11988\n",
            "Epoch: 1/10, step: 1808, loss: 2.62469, accuracy: 0.11981\n",
            "Epoch: 1/10, step: 1809, loss: 2.62474, accuracy: 0.11989\n",
            "Epoch: 1/10, step: 1810, loss: 2.62531, accuracy: 0.11982\n",
            "Epoch: 1/10, step: 1811, loss: 2.62537, accuracy: 0.11982\n",
            "Epoch: 1/10, step: 1812, loss: 2.62506, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1813, loss: 2.62498, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1814, loss: 2.62483, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1815, loss: 2.62513, accuracy: 0.11983\n",
            "Epoch: 1/10, step: 1816, loss: 2.62503, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1817, loss: 2.62507, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1818, loss: 2.62491, accuracy: 0.11984\n",
            "Epoch: 1/10, step: 1819, loss: 2.62494, accuracy: 0.11978\n",
            "Epoch: 1/10, step: 1820, loss: 2.62499, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1821, loss: 2.62476, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1822, loss: 2.62481, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1823, loss: 2.62472, accuracy: 0.11999\n",
            "Epoch: 1/10, step: 1824, loss: 2.62448, accuracy: 0.12013\n",
            "Epoch: 1/10, step: 1825, loss: 2.62438, accuracy: 0.12021\n",
            "Epoch: 1/10, step: 1826, loss: 2.62437, accuracy: 0.12021\n",
            "Epoch: 1/10, step: 1827, loss: 2.62428, accuracy: 0.12028\n",
            "Epoch: 1/10, step: 1828, loss: 2.62422, accuracy: 0.12028\n",
            "Epoch: 1/10, step: 1829, loss: 2.62410, accuracy: 0.12035\n",
            "Epoch: 1/10, step: 1830, loss: 2.62414, accuracy: 0.12036\n",
            "Epoch: 1/10, step: 1831, loss: 2.62434, accuracy: 0.12029\n",
            "Epoch: 1/10, step: 1832, loss: 2.62421, accuracy: 0.12022\n",
            "Epoch: 1/10, step: 1833, loss: 2.62415, accuracy: 0.12023\n",
            "Epoch: 1/10, step: 1834, loss: 2.62377, accuracy: 0.12030\n",
            "Epoch: 1/10, step: 1835, loss: 2.62385, accuracy: 0.12030\n",
            "Epoch: 1/10, step: 1836, loss: 2.62390, accuracy: 0.12030\n",
            "Epoch: 1/10, step: 1837, loss: 2.62386, accuracy: 0.12030\n",
            "Epoch: 1/10, step: 1838, loss: 2.62414, accuracy: 0.12024\n",
            "Epoch: 1/10, step: 1839, loss: 2.62430, accuracy: 0.12017\n",
            "Epoch: 1/10, step: 1840, loss: 2.62432, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1841, loss: 2.62439, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1842, loss: 2.62406, accuracy: 0.12039\n",
            "Epoch: 1/10, step: 1843, loss: 2.62416, accuracy: 0.12032\n",
            "Epoch: 1/10, step: 1844, loss: 2.62443, accuracy: 0.12032\n",
            "Epoch: 1/10, step: 1845, loss: 2.62438, accuracy: 0.12039\n",
            "Epoch: 1/10, step: 1846, loss: 2.62422, accuracy: 0.12040\n",
            "Epoch: 1/10, step: 1847, loss: 2.62414, accuracy: 0.12047\n",
            "Epoch: 1/10, step: 1848, loss: 2.62417, accuracy: 0.12047\n",
            "Epoch: 1/10, step: 1849, loss: 2.62425, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1850, loss: 2.62419, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1851, loss: 2.62412, accuracy: 0.12061\n",
            "Epoch: 1/10, step: 1852, loss: 2.62418, accuracy: 0.12061\n",
            "Epoch: 1/10, step: 1853, loss: 2.62419, accuracy: 0.12075\n",
            "Epoch: 1/10, step: 1854, loss: 2.62441, accuracy: 0.12069\n",
            "Epoch: 1/10, step: 1855, loss: 2.62432, accuracy: 0.12069\n",
            "Epoch: 1/10, step: 1856, loss: 2.62454, accuracy: 0.12062\n",
            "Epoch: 1/10, step: 1857, loss: 2.62474, accuracy: 0.12062\n",
            "Epoch: 1/10, step: 1858, loss: 2.62466, accuracy: 0.12069\n",
            "Epoch: 1/10, step: 1859, loss: 2.62439, accuracy: 0.12070\n",
            "Epoch: 1/10, step: 1860, loss: 2.62448, accuracy: 0.12070\n",
            "Epoch: 1/10, step: 1861, loss: 2.62457, accuracy: 0.12063\n",
            "Epoch: 1/10, step: 1862, loss: 2.62452, accuracy: 0.12057\n",
            "Epoch: 1/10, step: 1863, loss: 2.62431, accuracy: 0.12071\n",
            "Epoch: 1/10, step: 1864, loss: 2.62426, accuracy: 0.12084\n",
            "Epoch: 1/10, step: 1865, loss: 2.62419, accuracy: 0.12078\n",
            "Epoch: 1/10, step: 1866, loss: 2.62422, accuracy: 0.12078\n",
            "Epoch: 1/10, step: 1867, loss: 2.62429, accuracy: 0.12078\n",
            "Epoch: 1/10, step: 1868, loss: 2.62433, accuracy: 0.12072\n",
            "Epoch: 1/10, step: 1869, loss: 2.62448, accuracy: 0.12065\n",
            "Epoch: 1/10, step: 1870, loss: 2.62431, accuracy: 0.12066\n",
            "Epoch: 1/10, step: 1871, loss: 2.62415, accuracy: 0.12066\n",
            "Epoch: 1/10, step: 1872, loss: 2.62420, accuracy: 0.12066\n",
            "Epoch: 1/10, step: 1873, loss: 2.62434, accuracy: 0.12066\n",
            "Epoch: 1/10, step: 1874, loss: 2.62445, accuracy: 0.12060\n",
            "Epoch: 1/10, step: 1875, loss: 2.62443, accuracy: 0.12053\n",
            "Epoch: 1/10, step: 1876, loss: 2.62449, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1877, loss: 2.62427, accuracy: 0.12060\n",
            "Epoch: 1/10, step: 1878, loss: 2.62419, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1879, loss: 2.62439, accuracy: 0.12048\n",
            "Epoch: 1/10, step: 1880, loss: 2.62449, accuracy: 0.12048\n",
            "Epoch: 1/10, step: 1881, loss: 2.62460, accuracy: 0.12041\n",
            "Epoch: 1/10, step: 1882, loss: 2.62475, accuracy: 0.12042\n",
            "Epoch: 1/10, step: 1883, loss: 2.62445, accuracy: 0.12042\n",
            "Epoch: 1/10, step: 1884, loss: 2.62427, accuracy: 0.12036\n",
            "Epoch: 1/10, step: 1885, loss: 2.62442, accuracy: 0.12029\n",
            "Epoch: 1/10, step: 1886, loss: 2.62465, accuracy: 0.12029\n",
            "Epoch: 1/10, step: 1887, loss: 2.62460, accuracy: 0.12023\n",
            "Epoch: 1/10, step: 1888, loss: 2.62454, accuracy: 0.12023\n",
            "Epoch: 1/10, step: 1889, loss: 2.62458, accuracy: 0.12024\n",
            "Epoch: 1/10, step: 1890, loss: 2.62468, accuracy: 0.12017\n",
            "Epoch: 1/10, step: 1891, loss: 2.62511, accuracy: 0.12011\n",
            "Epoch: 1/10, step: 1892, loss: 2.62483, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1893, loss: 2.62480, accuracy: 0.12011\n",
            "Epoch: 1/10, step: 1894, loss: 2.62500, accuracy: 0.12012\n",
            "Epoch: 1/10, step: 1895, loss: 2.62479, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1896, loss: 2.62467, accuracy: 0.12019\n",
            "Epoch: 1/10, step: 1897, loss: 2.62455, accuracy: 0.12019\n",
            "Epoch: 1/10, step: 1898, loss: 2.62464, accuracy: 0.12019\n",
            "Epoch: 1/10, step: 1899, loss: 2.62472, accuracy: 0.12019\n",
            "Epoch: 1/10, step: 1900, loss: 2.62466, accuracy: 0.12013\n",
            "Epoch: 1/10, step: 1901, loss: 2.62454, accuracy: 0.12020\n",
            "Epoch: 1/10, step: 1902, loss: 2.62451, accuracy: 0.12027\n",
            "Epoch: 1/10, step: 1903, loss: 2.62462, accuracy: 0.12027\n",
            "Epoch: 1/10, step: 1904, loss: 2.62484, accuracy: 0.12021\n",
            "Epoch: 1/10, step: 1905, loss: 2.62467, accuracy: 0.12021\n",
            "Epoch: 1/10, step: 1906, loss: 2.62444, accuracy: 0.12028\n",
            "Epoch: 1/10, step: 1907, loss: 2.62431, accuracy: 0.12021\n",
            "Epoch: 1/10, step: 1908, loss: 2.62433, accuracy: 0.12015\n",
            "Epoch: 1/10, step: 1909, loss: 2.62436, accuracy: 0.12015\n",
            "Epoch: 1/10, step: 1910, loss: 2.62408, accuracy: 0.12022\n",
            "Epoch: 1/10, step: 1911, loss: 2.62382, accuracy: 0.12029\n",
            "Epoch: 1/10, step: 1912, loss: 2.62357, accuracy: 0.12029\n",
            "Epoch: 1/10, step: 1913, loss: 2.62335, accuracy: 0.12030\n",
            "Epoch: 1/10, step: 1914, loss: 2.62319, accuracy: 0.12043\n",
            "Epoch: 1/10, step: 1915, loss: 2.62305, accuracy: 0.12043\n",
            "Epoch: 1/10, step: 1916, loss: 2.62318, accuracy: 0.12037\n",
            "Epoch: 1/10, step: 1917, loss: 2.62294, accuracy: 0.12044\n",
            "Epoch: 1/10, step: 1918, loss: 2.62308, accuracy: 0.12037\n",
            "Epoch: 1/10, step: 1919, loss: 2.62295, accuracy: 0.12038\n",
            "Epoch: 1/10, step: 1920, loss: 2.62317, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 1921, loss: 2.62321, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 1922, loss: 2.62315, accuracy: 0.12032\n",
            "Epoch: 1/10, step: 1923, loss: 2.62309, accuracy: 0.12025\n",
            "Epoch: 1/10, step: 1924, loss: 2.62284, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 1925, loss: 2.62261, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 1926, loss: 2.62267, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 1927, loss: 2.62261, accuracy: 0.12020\n",
            "Epoch: 1/10, step: 1928, loss: 2.62280, accuracy: 0.12014\n",
            "Epoch: 1/10, step: 1929, loss: 2.62310, accuracy: 0.12008\n",
            "Epoch: 1/10, step: 1930, loss: 2.62311, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1931, loss: 2.62344, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1932, loss: 2.62335, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1933, loss: 2.62322, accuracy: 0.11996\n",
            "Epoch: 1/10, step: 1934, loss: 2.62300, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1935, loss: 2.62352, accuracy: 0.12009\n",
            "Epoch: 1/10, step: 1936, loss: 2.62325, accuracy: 0.12016\n",
            "Epoch: 1/10, step: 1937, loss: 2.62339, accuracy: 0.12010\n",
            "Epoch: 1/10, step: 1938, loss: 2.62326, accuracy: 0.12003\n",
            "Epoch: 1/10, step: 1939, loss: 2.62321, accuracy: 0.11997\n",
            "Epoch: 1/10, step: 1940, loss: 2.62311, accuracy: 0.12004\n",
            "Epoch: 1/10, step: 1941, loss: 2.62311, accuracy: 0.11998\n",
            "Epoch: 1/10, step: 1942, loss: 2.62300, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1943, loss: 2.62285, accuracy: 0.11985\n",
            "Epoch: 1/10, step: 1944, loss: 2.62274, accuracy: 0.11986\n",
            "Epoch: 1/10, step: 1945, loss: 2.62272, accuracy: 0.11979\n",
            "Epoch: 1/10, step: 1946, loss: 2.62272, accuracy: 0.11973\n",
            "Epoch: 1/10, step: 1947, loss: 2.62268, accuracy: 0.11980\n",
            "Epoch: 1/10, step: 1948, loss: 2.62236, accuracy: 0.11993\n",
            "Epoch: 1/10, step: 1949, loss: 2.62219, accuracy: 0.12006\n",
            "Epoch: 1/10, step: 1950, loss: 2.62185, accuracy: 0.12013\n",
            "Epoch: 1/10, step: 1951, loss: 2.62197, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1952, loss: 2.62186, accuracy: 0.12007\n",
            "Epoch: 1/10, step: 1953, loss: 2.62210, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1954, loss: 2.62198, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1955, loss: 2.62170, accuracy: 0.12001\n",
            "Epoch: 1/10, step: 1956, loss: 2.62161, accuracy: 0.12008\n",
            "Epoch: 1/10, step: 1957, loss: 2.62166, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1958, loss: 2.62189, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1959, loss: 2.62165, accuracy: 0.12002\n",
            "Epoch: 1/10, step: 1960, loss: 2.62145, accuracy: 0.12003\n",
            "Epoch: 1/10, step: 1961, loss: 2.62111, accuracy: 0.12009\n",
            "Epoch: 1/10, step: 1962, loss: 2.62133, accuracy: 0.12003\n",
            "Epoch: 1/10, step: 1963, loss: 2.62137, accuracy: 0.11997\n",
            "Epoch: 1/10, step: 1964, loss: 2.62111, accuracy: 0.11997\n",
            "Epoch: 1/10, step: 1965, loss: 2.62109, accuracy: 0.11991\n",
            "Epoch: 1/10, step: 1966, loss: 2.62083, accuracy: 0.11998\n",
            "Epoch: 1/10, step: 1967, loss: 2.62078, accuracy: 0.11992\n",
            "Epoch: 1/10, step: 1968, loss: 2.62063, accuracy: 0.12005\n",
            "Epoch: 1/10, step: 1969, loss: 2.62064, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1970, loss: 2.62058, accuracy: 0.12018\n",
            "Epoch: 1/10, step: 1971, loss: 2.62036, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 1972, loss: 2.62031, accuracy: 0.12037\n",
            "Epoch: 1/10, step: 1973, loss: 2.62027, accuracy: 0.12038\n",
            "Epoch: 1/10, step: 1974, loss: 2.62045, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 1975, loss: 2.62035, accuracy: 0.12032\n",
            "Epoch: 1/10, step: 1976, loss: 2.62034, accuracy: 0.12038\n",
            "Epoch: 1/10, step: 1977, loss: 2.62039, accuracy: 0.12038\n",
            "Epoch: 1/10, step: 1978, loss: 2.62043, accuracy: 0.12039\n",
            "Epoch: 1/10, step: 1979, loss: 2.62024, accuracy: 0.12039\n",
            "Epoch: 1/10, step: 1980, loss: 2.61993, accuracy: 0.12045\n",
            "Epoch: 1/10, step: 1981, loss: 2.61986, accuracy: 0.12039\n",
            "Epoch: 1/10, step: 1982, loss: 2.61971, accuracy: 0.12040\n",
            "Epoch: 1/10, step: 1983, loss: 2.61974, accuracy: 0.12046\n",
            "Epoch: 1/10, step: 1984, loss: 2.61954, accuracy: 0.12046\n",
            "Epoch: 1/10, step: 1985, loss: 2.61951, accuracy: 0.12040\n",
            "Epoch: 1/10, step: 1986, loss: 2.61928, accuracy: 0.12059\n",
            "Epoch: 1/10, step: 1987, loss: 2.61921, accuracy: 0.12060\n",
            "Epoch: 1/10, step: 1988, loss: 2.61923, accuracy: 0.12060\n",
            "Epoch: 1/10, step: 1989, loss: 2.61935, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1990, loss: 2.61935, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 1991, loss: 2.61946, accuracy: 0.12048\n",
            "Epoch: 1/10, step: 1992, loss: 2.61933, accuracy: 0.12042\n",
            "Epoch: 1/10, step: 1993, loss: 2.61931, accuracy: 0.12036\n",
            "Epoch: 1/10, step: 1994, loss: 2.61937, accuracy: 0.12042\n",
            "Epoch: 1/10, step: 1995, loss: 2.61923, accuracy: 0.12043\n",
            "Epoch: 1/10, step: 1996, loss: 2.61915, accuracy: 0.12037\n",
            "Epoch: 1/10, step: 1997, loss: 2.61942, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 1998, loss: 2.61953, accuracy: 0.12025\n",
            "Epoch: 1/10, step: 1999, loss: 2.61957, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 2000, loss: 2.61952, accuracy: 0.12031\n",
            "Epoch: 1/10, step: 2001, loss: 2.61959, accuracy: 0.12025\n",
            "Epoch: 1/10, step: 2002, loss: 2.61972, accuracy: 0.12025\n",
            "Epoch: 1/10, step: 2003, loss: 2.61964, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 2004, loss: 2.61971, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 2005, loss: 2.62001, accuracy: 0.12020\n",
            "Epoch: 1/10, step: 2006, loss: 2.61995, accuracy: 0.12026\n",
            "Epoch: 1/10, step: 2007, loss: 2.61970, accuracy: 0.12033\n",
            "Epoch: 1/10, step: 2008, loss: 2.61965, accuracy: 0.12033\n",
            "Epoch: 1/10, step: 2009, loss: 2.61962, accuracy: 0.12033\n",
            "Epoch: 1/10, step: 2010, loss: 2.61956, accuracy: 0.12034\n",
            "Epoch: 1/10, step: 2011, loss: 2.61944, accuracy: 0.12034\n",
            "Epoch: 1/10, step: 2012, loss: 2.61937, accuracy: 0.12034\n",
            "Epoch: 1/10, step: 2013, loss: 2.61960, accuracy: 0.12034\n",
            "Epoch: 1/10, step: 2014, loss: 2.61973, accuracy: 0.12047\n",
            "Epoch: 1/10, step: 2015, loss: 2.61977, accuracy: 0.12047\n",
            "Epoch: 1/10, step: 2016, loss: 2.61969, accuracy: 0.12047\n",
            "Epoch: 1/10, step: 2017, loss: 2.61949, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 2018, loss: 2.61963, accuracy: 0.12054\n",
            "Epoch: 1/10, step: 2019, loss: 2.61944, accuracy: 0.12060\n",
            "Epoch: 1/10, step: 2020, loss: 2.61930, accuracy: 0.12073\n",
            "Epoch: 1/10, step: 2021, loss: 2.61925, accuracy: 0.12073\n",
            "Epoch: 1/10, step: 2022, loss: 2.61908, accuracy: 0.12073\n",
            "Epoch: 1/10, step: 2023, loss: 2.61914, accuracy: 0.12074\n",
            "Epoch: 1/10, step: 2024, loss: 2.61920, accuracy: 0.12068\n",
            "Epoch: 1/10, step: 2025, loss: 2.61932, accuracy: 0.12068\n",
            "Epoch: 1/10, step: 2026, loss: 2.61919, accuracy: 0.12068\n",
            "Epoch: 1/10, step: 2027, loss: 2.61897, accuracy: 0.12068\n",
            "Epoch: 1/10, step: 2028, loss: 2.61922, accuracy: 0.12062\n",
            "Epoch: 1/10, step: 2029, loss: 2.61917, accuracy: 0.12063\n",
            "Epoch: 1/10, step: 2030, loss: 2.61912, accuracy: 0.12063\n",
            "Epoch: 1/10, step: 2031, loss: 2.61936, accuracy: 0.12063\n",
            "Epoch: 1/10, step: 2032, loss: 2.61934, accuracy: 0.12063\n",
            "Epoch: 1/10, step: 2033, loss: 2.61909, accuracy: 0.12076\n",
            "Epoch: 1/10, step: 2034, loss: 2.61891, accuracy: 0.12082\n",
            "Epoch: 1/10, step: 2035, loss: 2.61877, accuracy: 0.12088\n",
            "Epoch: 1/10, step: 2036, loss: 2.61899, accuracy: 0.12089\n",
            "Epoch: 1/10, step: 2037, loss: 2.61880, accuracy: 0.12089\n",
            "Epoch: 1/10, step: 2038, loss: 2.61875, accuracy: 0.12095\n",
            "Epoch: 1/10, step: 2039, loss: 2.61870, accuracy: 0.12095\n",
            "Epoch: 1/10, step: 2040, loss: 2.61844, accuracy: 0.12102\n",
            "Epoch: 1/10, step: 2041, loss: 2.61825, accuracy: 0.12108\n",
            "Epoch: 1/10, step: 2042, loss: 2.61817, accuracy: 0.12108\n",
            "Epoch: 1/10, step: 2043, loss: 2.61804, accuracy: 0.12115\n",
            "Epoch: 1/10, step: 2044, loss: 2.61815, accuracy: 0.12115\n",
            "Epoch: 1/10, step: 2045, loss: 2.61798, accuracy: 0.12121\n",
            "Epoch: 1/10, step: 2046, loss: 2.61793, accuracy: 0.12115\n",
            "Epoch: 1/10, step: 2047, loss: 2.61798, accuracy: 0.12109\n",
            "Epoch: 1/10, step: 2048, loss: 2.61796, accuracy: 0.12122\n",
            "Epoch: 1/10, step: 2049, loss: 2.61821, accuracy: 0.12116\n",
            "Epoch: 1/10, step: 2050, loss: 2.61850, accuracy: 0.12110\n",
            "Epoch: 1/10, step: 2051, loss: 2.61861, accuracy: 0.12116\n",
            "Epoch: 1/10, step: 2052, loss: 2.61821, accuracy: 0.12128\n",
            "Epoch: 1/10, step: 2053, loss: 2.61797, accuracy: 0.12141\n",
            "Epoch: 1/10, step: 2054, loss: 2.61799, accuracy: 0.12141\n",
            "Epoch: 1/10, step: 2055, loss: 2.61828, accuracy: 0.12141\n",
            "Epoch: 1/10, step: 2056, loss: 2.61840, accuracy: 0.12135\n",
            "Epoch: 1/10, step: 2057, loss: 2.61876, accuracy: 0.12129\n",
            "Epoch: 1/10, step: 2058, loss: 2.61857, accuracy: 0.12129\n",
            "Epoch: 1/10, step: 2059, loss: 2.61829, accuracy: 0.12136\n",
            "Epoch: 1/10, step: 2060, loss: 2.61825, accuracy: 0.12136\n",
            "Epoch: 1/10, step: 2061, loss: 2.61827, accuracy: 0.12142\n",
            "Epoch: 1/10, step: 2062, loss: 2.61820, accuracy: 0.12142\n",
            "Epoch: 1/10, step: 2063, loss: 2.61816, accuracy: 0.12149\n",
            "Epoch: 1/10, step: 2064, loss: 2.61803, accuracy: 0.12149\n",
            "Epoch: 1/10, step: 2065, loss: 2.61778, accuracy: 0.12161\n",
            "Epoch: 1/10, step: 2066, loss: 2.61763, accuracy: 0.12155\n",
            "Epoch: 1/10, step: 2067, loss: 2.61762, accuracy: 0.12155\n",
            "Epoch: 1/10, step: 2068, loss: 2.61775, accuracy: 0.12149\n",
            "Epoch: 1/10, step: 2069, loss: 2.61804, accuracy: 0.12144\n",
            "Epoch: 1/10, step: 2070, loss: 2.61838, accuracy: 0.12138\n",
            "Epoch: 1/10, step: 2071, loss: 2.61851, accuracy: 0.12132\n",
            "Epoch: 1/10, step: 2072, loss: 2.61822, accuracy: 0.12132\n",
            "Epoch: 1/10, step: 2073, loss: 2.61834, accuracy: 0.12126\n",
            "Epoch: 1/10, step: 2074, loss: 2.61807, accuracy: 0.12126\n",
            "Epoch: 1/10, step: 2075, loss: 2.61788, accuracy: 0.12133\n",
            "Epoch: 1/10, step: 2076, loss: 2.61809, accuracy: 0.12127\n",
            "Epoch: 1/10, step: 2077, loss: 2.61819, accuracy: 0.12127\n",
            "Epoch: 1/10, step: 2078, loss: 2.61803, accuracy: 0.12127\n",
            "Epoch: 1/10, step: 2079, loss: 2.61775, accuracy: 0.12133\n",
            "Epoch: 1/10, step: 2080, loss: 2.61775, accuracy: 0.12127\n",
            "Epoch: 1/10, step: 2081, loss: 2.61753, accuracy: 0.12134\n",
            "Epoch: 1/10, step: 2082, loss: 2.61708, accuracy: 0.12146\n",
            "Epoch: 1/10, step: 2083, loss: 2.61708, accuracy: 0.12140\n",
            "Epoch: 1/10, step: 2084, loss: 2.61713, accuracy: 0.12140\n",
            "Epoch: 1/10, step: 2085, loss: 2.61691, accuracy: 0.12134\n",
            "Epoch: 1/10, step: 2086, loss: 2.61679, accuracy: 0.12140\n",
            "Epoch: 1/10, step: 2087, loss: 2.61676, accuracy: 0.12135\n",
            "Epoch: 1/10, step: 2088, loss: 2.61688, accuracy: 0.12129\n",
            "Epoch: 1/10, step: 2089, loss: 2.61663, accuracy: 0.12147\n",
            "Epoch: 1/10, step: 2090, loss: 2.61659, accuracy: 0.12147\n",
            "Epoch: 1/10, step: 2091, loss: 2.61637, accuracy: 0.12159\n",
            "Epoch: 1/10, step: 2092, loss: 2.61630, accuracy: 0.12165\n",
            "Epoch: 1/10, step: 2093, loss: 2.61600, accuracy: 0.12172\n",
            "Epoch: 1/10, step: 2094, loss: 2.61586, accuracy: 0.12172\n",
            "Epoch: 1/10, step: 2095, loss: 2.61575, accuracy: 0.12172\n",
            "Epoch: 1/10, step: 2096, loss: 2.61557, accuracy: 0.12172\n",
            "Epoch: 1/10, step: 2097, loss: 2.61545, accuracy: 0.12178\n",
            "Epoch: 1/10, step: 2098, loss: 2.61568, accuracy: 0.12172\n",
            "Epoch: 1/10, step: 2099, loss: 2.61592, accuracy: 0.12167\n",
            "Epoch: 1/10, step: 2100, loss: 2.61597, accuracy: 0.12167\n",
            "Epoch: 1/10, step: 2101, loss: 2.61641, accuracy: 0.12161\n",
            "Epoch: 1/10, step: 2102, loss: 2.61641, accuracy: 0.12161\n",
            "Epoch: 1/10, step: 2103, loss: 2.61645, accuracy: 0.12161\n",
            "Epoch: 1/10, step: 2104, loss: 2.61639, accuracy: 0.12161\n",
            "Epoch: 1/10, step: 2105, loss: 2.61612, accuracy: 0.12167\n",
            "Epoch: 1/10, step: 2106, loss: 2.61658, accuracy: 0.12162\n",
            "Epoch: 1/10, step: 2107, loss: 2.61641, accuracy: 0.12162\n",
            "Epoch: 1/10, step: 2108, loss: 2.61662, accuracy: 0.12162\n",
            "Epoch: 1/10, step: 2109, loss: 2.61626, accuracy: 0.12174\n",
            "Epoch: 1/10, step: 2110, loss: 2.61650, accuracy: 0.12174\n",
            "Epoch: 1/10, step: 2111, loss: 2.61657, accuracy: 0.12168\n",
            "Epoch: 1/10, step: 2112, loss: 2.61663, accuracy: 0.12174\n",
            "Epoch: 1/10, step: 2113, loss: 2.61664, accuracy: 0.12169\n",
            "Epoch: 1/10, step: 2114, loss: 2.61634, accuracy: 0.12175\n",
            "Epoch: 1/10, step: 2115, loss: 2.61619, accuracy: 0.12187\n",
            "Epoch: 1/10, step: 2116, loss: 2.61635, accuracy: 0.12187\n",
            "Epoch: 1/10, step: 2117, loss: 2.61627, accuracy: 0.12181\n",
            "Epoch: 1/10, step: 2118, loss: 2.61658, accuracy: 0.12175\n",
            "Epoch: 1/10, step: 2119, loss: 2.61670, accuracy: 0.12176\n",
            "Epoch: 1/10, step: 2120, loss: 2.61646, accuracy: 0.12170\n",
            "Epoch: 1/10, step: 2121, loss: 2.61645, accuracy: 0.12176\n",
            "Epoch: 1/10, step: 2122, loss: 2.61661, accuracy: 0.12176\n",
            "Epoch: 1/10, step: 2123, loss: 2.61644, accuracy: 0.12182\n",
            "Epoch: 1/10, step: 2124, loss: 2.61627, accuracy: 0.12182\n",
            "Epoch: 1/10, step: 2125, loss: 2.61640, accuracy: 0.12182\n",
            "Epoch: 1/10, step: 2126, loss: 2.61621, accuracy: 0.12194\n",
            "Epoch: 1/10, step: 2127, loss: 2.61633, accuracy: 0.12189\n",
            "Epoch: 1/10, step: 2128, loss: 2.61625, accuracy: 0.12200\n",
            "Epoch: 1/10, step: 2129, loss: 2.61596, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2130, loss: 2.61595, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2131, loss: 2.61581, accuracy: 0.12218\n",
            "Epoch: 1/10, step: 2132, loss: 2.61570, accuracy: 0.12219\n",
            "Epoch: 1/10, step: 2133, loss: 2.61538, accuracy: 0.12230\n",
            "Epoch: 1/10, step: 2134, loss: 2.61516, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2135, loss: 2.61517, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2136, loss: 2.61516, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2137, loss: 2.61519, accuracy: 0.12237\n",
            "Epoch: 1/10, step: 2138, loss: 2.61523, accuracy: 0.12231\n",
            "Epoch: 1/10, step: 2139, loss: 2.61524, accuracy: 0.12225\n",
            "Epoch: 1/10, step: 2140, loss: 2.61547, accuracy: 0.12220\n",
            "Epoch: 1/10, step: 2141, loss: 2.61549, accuracy: 0.12226\n",
            "Epoch: 1/10, step: 2142, loss: 2.61527, accuracy: 0.12220\n",
            "Epoch: 1/10, step: 2143, loss: 2.61534, accuracy: 0.12214\n",
            "Epoch: 1/10, step: 2144, loss: 2.61549, accuracy: 0.12214\n",
            "Epoch: 1/10, step: 2145, loss: 2.61555, accuracy: 0.12209\n",
            "Epoch: 1/10, step: 2146, loss: 2.61564, accuracy: 0.12209\n",
            "Epoch: 1/10, step: 2147, loss: 2.61562, accuracy: 0.12215\n",
            "Epoch: 1/10, step: 2148, loss: 2.61557, accuracy: 0.12215\n",
            "Epoch: 1/10, step: 2149, loss: 2.61573, accuracy: 0.12209\n",
            "Epoch: 1/10, step: 2150, loss: 2.61608, accuracy: 0.12203\n",
            "Epoch: 1/10, step: 2151, loss: 2.61612, accuracy: 0.12198\n",
            "Epoch: 1/10, step: 2152, loss: 2.61599, accuracy: 0.12198\n",
            "Epoch: 1/10, step: 2153, loss: 2.61570, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2154, loss: 2.61538, accuracy: 0.12210\n",
            "Epoch: 1/10, step: 2155, loss: 2.61558, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2156, loss: 2.61545, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2157, loss: 2.61549, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2158, loss: 2.61546, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2159, loss: 2.61545, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2160, loss: 2.61512, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2161, loss: 2.61492, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2162, loss: 2.61512, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2163, loss: 2.61505, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2164, loss: 2.61520, accuracy: 0.12200\n",
            "Epoch: 1/10, step: 2165, loss: 2.61527, accuracy: 0.12194\n",
            "Epoch: 1/10, step: 2166, loss: 2.61525, accuracy: 0.12194\n",
            "Epoch: 1/10, step: 2167, loss: 2.61519, accuracy: 0.12194\n",
            "Epoch: 1/10, step: 2168, loss: 2.61534, accuracy: 0.12189\n",
            "Epoch: 1/10, step: 2169, loss: 2.61522, accuracy: 0.12195\n",
            "Epoch: 1/10, step: 2170, loss: 2.61526, accuracy: 0.12195\n",
            "Epoch: 1/10, step: 2171, loss: 2.61514, accuracy: 0.12195\n",
            "Epoch: 1/10, step: 2172, loss: 2.61512, accuracy: 0.12201\n",
            "Epoch: 1/10, step: 2173, loss: 2.61500, accuracy: 0.12207\n",
            "Epoch: 1/10, step: 2174, loss: 2.61539, accuracy: 0.12201\n",
            "Epoch: 1/10, step: 2175, loss: 2.61522, accuracy: 0.12195\n",
            "Epoch: 1/10, step: 2176, loss: 2.61530, accuracy: 0.12190\n",
            "Epoch: 1/10, step: 2177, loss: 2.61519, accuracy: 0.12190\n",
            "Epoch: 1/10, step: 2178, loss: 2.61524, accuracy: 0.12184\n",
            "Epoch: 1/10, step: 2179, loss: 2.61517, accuracy: 0.12184\n",
            "Epoch: 1/10, step: 2180, loss: 2.61530, accuracy: 0.12185\n",
            "Epoch: 1/10, step: 2181, loss: 2.61537, accuracy: 0.12185\n",
            "Epoch: 1/10, step: 2182, loss: 2.61547, accuracy: 0.12185\n",
            "Epoch: 1/10, step: 2183, loss: 2.61543, accuracy: 0.12185\n",
            "Epoch: 1/10, step: 2184, loss: 2.61539, accuracy: 0.12179\n",
            "Epoch: 1/10, step: 2185, loss: 2.61564, accuracy: 0.12180\n",
            "Epoch: 1/10, step: 2186, loss: 2.61573, accuracy: 0.12180\n",
            "Epoch: 1/10, step: 2187, loss: 2.61559, accuracy: 0.12191\n",
            "Epoch: 1/10, step: 2188, loss: 2.61548, accuracy: 0.12197\n",
            "Epoch: 1/10, step: 2189, loss: 2.61547, accuracy: 0.12197\n",
            "Epoch: 1/10, step: 2190, loss: 2.61551, accuracy: 0.12192\n",
            "Epoch: 1/10, step: 2191, loss: 2.61547, accuracy: 0.12198\n",
            "Epoch: 1/10, step: 2192, loss: 2.61537, accuracy: 0.12209\n",
            "Epoch: 1/10, step: 2193, loss: 2.61544, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2194, loss: 2.61529, accuracy: 0.12209\n",
            "Epoch: 1/10, step: 2195, loss: 2.61555, accuracy: 0.12210\n",
            "Epoch: 1/10, step: 2196, loss: 2.61553, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2197, loss: 2.61537, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2198, loss: 2.61560, accuracy: 0.12204\n",
            "Epoch: 1/10, step: 2199, loss: 2.61537, accuracy: 0.12210\n",
            "Epoch: 1/10, step: 2200, loss: 2.61510, accuracy: 0.12216\n",
            "Epoch: 1/10, step: 2201, loss: 2.61501, accuracy: 0.12210\n",
            "Epoch: 1/10, step: 2202, loss: 2.61503, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2203, loss: 2.61491, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2204, loss: 2.61486, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2205, loss: 2.61492, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2206, loss: 2.61526, accuracy: 0.12205\n",
            "Epoch: 1/10, step: 2207, loss: 2.61544, accuracy: 0.12211\n",
            "Epoch: 1/10, step: 2208, loss: 2.61562, accuracy: 0.12206\n",
            "Epoch: 1/10, step: 2209, loss: 2.61568, accuracy: 0.12206\n",
            "Epoch: 1/10, step: 2210, loss: 2.61558, accuracy: 0.12206\n",
            "Epoch: 1/10, step: 2211, loss: 2.61545, accuracy: 0.12200\n",
            "Epoch: 1/10, step: 2212, loss: 2.61553, accuracy: 0.12195\n",
            "Epoch: 1/10, step: 2213, loss: 2.61522, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2214, loss: 2.61501, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2215, loss: 2.61513, accuracy: 0.12218\n",
            "Epoch: 1/10, step: 2216, loss: 2.61519, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2217, loss: 2.61515, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2218, loss: 2.61517, accuracy: 0.12213\n",
            "Epoch: 1/10, step: 2219, loss: 2.61486, accuracy: 0.12218\n",
            "Epoch: 1/10, step: 2220, loss: 2.61463, accuracy: 0.12235\n",
            "Epoch: 1/10, step: 2221, loss: 2.61449, accuracy: 0.12241\n",
            "Epoch: 1/10, step: 2222, loss: 2.61476, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2223, loss: 2.61457, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2224, loss: 2.61464, accuracy: 0.12241\n",
            "Epoch: 1/10, step: 2225, loss: 2.61467, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2226, loss: 2.61440, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2227, loss: 2.61452, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2228, loss: 2.61459, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2229, loss: 2.61453, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2230, loss: 2.61458, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2231, loss: 2.61451, accuracy: 0.12259\n",
            "Epoch: 1/10, step: 2232, loss: 2.61468, accuracy: 0.12254\n",
            "Epoch: 1/10, step: 2233, loss: 2.61471, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2234, loss: 2.61476, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2235, loss: 2.61474, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2236, loss: 2.61491, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2237, loss: 2.61499, accuracy: 0.12249\n",
            "Epoch: 1/10, step: 2238, loss: 2.61487, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2239, loss: 2.61471, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2240, loss: 2.61450, accuracy: 0.12249\n",
            "Epoch: 1/10, step: 2241, loss: 2.61443, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2242, loss: 2.61438, accuracy: 0.12238\n",
            "Epoch: 1/10, step: 2243, loss: 2.61442, accuracy: 0.12255\n",
            "Epoch: 1/10, step: 2244, loss: 2.61435, accuracy: 0.12249\n",
            "Epoch: 1/10, step: 2245, loss: 2.61427, accuracy: 0.12255\n",
            "Epoch: 1/10, step: 2246, loss: 2.61409, accuracy: 0.12250\n",
            "Epoch: 1/10, step: 2247, loss: 2.61391, accuracy: 0.12255\n",
            "Epoch: 1/10, step: 2248, loss: 2.61393, accuracy: 0.12255\n",
            "Epoch: 1/10, step: 2249, loss: 2.61389, accuracy: 0.12255\n",
            "Epoch: 1/10, step: 2250, loss: 2.61417, accuracy: 0.12256\n",
            "Epoch: 1/10, step: 2251, loss: 2.61401, accuracy: 0.12261\n",
            "Epoch: 1/10, step: 2252, loss: 2.61410, accuracy: 0.12261\n",
            "Epoch: 1/10, step: 2253, loss: 2.61402, accuracy: 0.12256\n",
            "Epoch: 1/10, step: 2254, loss: 2.61419, accuracy: 0.12250\n",
            "Epoch: 1/10, step: 2255, loss: 2.61429, accuracy: 0.12245\n",
            "Epoch: 1/10, step: 2256, loss: 2.61453, accuracy: 0.12245\n",
            "Epoch: 1/10, step: 2257, loss: 2.61458, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2258, loss: 2.61462, accuracy: 0.12234\n",
            "Epoch: 1/10, step: 2259, loss: 2.61438, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2260, loss: 2.61438, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2261, loss: 2.61440, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2262, loss: 2.61457, accuracy: 0.12246\n",
            "Epoch: 1/10, step: 2263, loss: 2.61466, accuracy: 0.12246\n",
            "Epoch: 1/10, step: 2264, loss: 2.61448, accuracy: 0.12252\n",
            "Epoch: 1/10, step: 2265, loss: 2.61433, accuracy: 0.12257\n",
            "Epoch: 1/10, step: 2266, loss: 2.61422, accuracy: 0.12257\n",
            "Epoch: 1/10, step: 2267, loss: 2.61441, accuracy: 0.12252\n",
            "Epoch: 1/10, step: 2268, loss: 2.61444, accuracy: 0.12246\n",
            "Epoch: 1/10, step: 2269, loss: 2.61447, accuracy: 0.12241\n",
            "Epoch: 1/10, step: 2270, loss: 2.61426, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2271, loss: 2.61425, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2272, loss: 2.61421, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2273, loss: 2.61394, accuracy: 0.12253\n",
            "Epoch: 1/10, step: 2274, loss: 2.61391, accuracy: 0.12258\n",
            "Epoch: 1/10, step: 2275, loss: 2.61389, accuracy: 0.12253\n",
            "Epoch: 1/10, step: 2276, loss: 2.61383, accuracy: 0.12253\n",
            "Epoch: 1/10, step: 2277, loss: 2.61400, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2278, loss: 2.61408, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2279, loss: 2.61401, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2280, loss: 2.61399, accuracy: 0.12248\n",
            "Epoch: 1/10, step: 2281, loss: 2.61414, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2282, loss: 2.61448, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2283, loss: 2.61464, accuracy: 0.12237\n",
            "Epoch: 1/10, step: 2284, loss: 2.61476, accuracy: 0.12232\n",
            "Epoch: 1/10, step: 2285, loss: 2.61460, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2286, loss: 2.61466, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2287, loss: 2.61459, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2288, loss: 2.61445, accuracy: 0.12249\n",
            "Epoch: 1/10, step: 2289, loss: 2.61441, accuracy: 0.12249\n",
            "Epoch: 1/10, step: 2290, loss: 2.61439, accuracy: 0.12243\n",
            "Epoch: 1/10, step: 2291, loss: 2.61438, accuracy: 0.12238\n",
            "Epoch: 1/10, step: 2292, loss: 2.61424, accuracy: 0.12233\n",
            "Epoch: 1/10, step: 2293, loss: 2.61444, accuracy: 0.12227\n",
            "Epoch: 1/10, step: 2294, loss: 2.61440, accuracy: 0.12228\n",
            "Epoch: 1/10, step: 2295, loss: 2.61433, accuracy: 0.12222\n",
            "Epoch: 1/10, step: 2296, loss: 2.61448, accuracy: 0.12217\n",
            "Epoch: 1/10, step: 2297, loss: 2.61455, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2298, loss: 2.61452, accuracy: 0.12217\n",
            "Epoch: 1/10, step: 2299, loss: 2.61490, accuracy: 0.12212\n",
            "Epoch: 1/10, step: 2300, loss: 2.61470, accuracy: 0.12217\n",
            "Epoch: 1/10, step: 2301, loss: 2.61459, accuracy: 0.12223\n",
            "Epoch: 1/10, step: 2302, loss: 2.61449, accuracy: 0.12228\n",
            "Epoch: 1/10, step: 2303, loss: 2.61438, accuracy: 0.12223\n",
            "Epoch: 1/10, step: 2304, loss: 2.61423, accuracy: 0.12229\n",
            "Epoch: 1/10, step: 2305, loss: 2.61422, accuracy: 0.12229\n",
            "Epoch: 1/10, step: 2306, loss: 2.61390, accuracy: 0.12234\n",
            "Epoch: 1/10, step: 2307, loss: 2.61375, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2308, loss: 2.61383, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2309, loss: 2.61369, accuracy: 0.12251\n",
            "Epoch: 1/10, step: 2310, loss: 2.61403, accuracy: 0.12246\n",
            "Epoch: 1/10, step: 2311, loss: 2.61414, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2312, loss: 2.61422, accuracy: 0.12240\n",
            "Epoch: 1/10, step: 2313, loss: 2.61432, accuracy: 0.12235\n",
            "Epoch: 1/10, step: 2314, loss: 2.61439, accuracy: 0.12235\n",
            "Epoch: 1/10, step: 2315, loss: 2.61448, accuracy: 0.12230\n",
            "Epoch: 1/10, step: 2316, loss: 2.61444, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2317, loss: 2.61422, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2318, loss: 2.61387, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2319, loss: 2.61384, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2320, loss: 2.61378, accuracy: 0.12236\n",
            "Epoch: 1/10, step: 2321, loss: 2.61370, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2322, loss: 2.61369, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2323, loss: 2.61347, accuracy: 0.12247\n",
            "Epoch: 1/10, step: 2324, loss: 2.61352, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2325, loss: 2.61338, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2326, loss: 2.61332, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2327, loss: 2.61321, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2328, loss: 2.61308, accuracy: 0.12237\n",
            "Epoch: 1/10, step: 2329, loss: 2.61303, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2330, loss: 2.61299, accuracy: 0.12242\n",
            "Epoch: 1/10, step: 2331, loss: 2.61280, accuracy: 0.12253\n",
            "Epoch: 1/10, step: 2332, loss: 2.61250, accuracy: 0.12259\n",
            "Epoch: 1/10, step: 2333, loss: 2.61246, accuracy: 0.12259\n",
            "Epoch: 1/10, step: 2334, loss: 2.61238, accuracy: 0.12270\n",
            "Epoch: 1/10, step: 2335, loss: 2.61253, accuracy: 0.12264\n",
            "Epoch: 1/10, step: 2336, loss: 2.61227, accuracy: 0.12275\n",
            "Epoch: 1/10, step: 2337, loss: 2.61220, accuracy: 0.12275\n",
            "Epoch: 1/10, step: 2338, loss: 2.61202, accuracy: 0.12275\n",
            "Epoch: 1/10, step: 2339, loss: 2.61215, accuracy: 0.12270\n",
            "Epoch: 1/10, step: 2340, loss: 2.61210, accuracy: 0.12265\n",
            "Epoch: 1/10, step: 2341, loss: 2.61207, accuracy: 0.12265\n",
            "Epoch: 1/10, step: 2342, loss: 2.61193, accuracy: 0.12276\n",
            "Epoch: 1/10, step: 2343, loss: 2.61214, accuracy: 0.12276\n",
            "Epoch: 1/10, step: 2344, loss: 2.61223, accuracy: 0.12271\n",
            "Epoch: 1/10, step: 2345, loss: 2.61228, accuracy: 0.12276\n",
            "Epoch: 1/10, step: 2346, loss: 2.61213, accuracy: 0.12282\n",
            "Epoch: 1/10, step: 2347, loss: 2.61213, accuracy: 0.12282\n",
            "Epoch: 1/10, step: 2348, loss: 2.61201, accuracy: 0.12287\n",
            "Epoch: 1/10, step: 2349, loss: 2.61206, accuracy: 0.12282\n",
            "Epoch: 1/10, step: 2350, loss: 2.61193, accuracy: 0.12287\n",
            "Epoch: 1/10, step: 2351, loss: 2.61191, accuracy: 0.12293\n",
            "Epoch: 1/10, step: 2352, loss: 2.61221, accuracy: 0.12287\n",
            "Epoch: 1/10, step: 2353, loss: 2.61233, accuracy: 0.12282\n",
            "Epoch: 1/10, step: 2354, loss: 2.61222, accuracy: 0.12277\n",
            "Epoch: 1/10, step: 2355, loss: 2.61232, accuracy: 0.12272\n",
            "Epoch: 1/10, step: 2356, loss: 2.61223, accuracy: 0.12277\n",
            "Epoch: 1/10, step: 2357, loss: 2.61217, accuracy: 0.12272\n",
            "Epoch: 1/10, step: 2358, loss: 2.61202, accuracy: 0.12277\n",
            "Epoch: 1/10, step: 2359, loss: 2.61189, accuracy: 0.12272\n",
            "Epoch: 1/10, step: 2360, loss: 2.61174, accuracy: 0.12267\n",
            "Epoch: 1/10, step: 2361, loss: 2.61168, accuracy: 0.12272\n",
            "Epoch: 1/10, step: 2362, loss: 2.61166, accuracy: 0.12267\n",
            "Epoch: 1/10, step: 2363, loss: 2.61163, accuracy: 0.12267\n",
            "Epoch: 1/10, step: 2364, loss: 2.61162, accuracy: 0.12273\n",
            "Epoch: 1/10, step: 2365, loss: 2.61182, accuracy: 0.12267\n",
            "Epoch: 1/10, step: 2366, loss: 2.61163, accuracy: 0.12273\n",
            "Epoch: 1/10, step: 2367, loss: 2.61171, accuracy: 0.12268\n",
            "Epoch: 1/10, step: 2368, loss: 2.61174, accuracy: 0.12262\n",
            "Epoch: 1/10, step: 2369, loss: 2.61185, accuracy: 0.12263\n",
            "Epoch: 1/10, step: 2370, loss: 2.61204, accuracy: 0.12257\n",
            "Epoch: 1/10, step: 2371, loss: 2.61204, accuracy: 0.12257\n",
            "Epoch: 1/10, step: 2372, loss: 2.61198, accuracy: 0.12258\n",
            "Epoch: 1/10, step: 2373, loss: 2.61234, accuracy: 0.12252\n",
            "Epoch: 1/10, step: 2374, loss: 2.61229, accuracy: 0.12253\n",
            "Epoch: 1/10, step: 2375, loss: 2.61231, accuracy: 0.12263\n",
            "Epoch: 1/10, step: 2376, loss: 2.61204, accuracy: 0.12279\n",
            "Epoch: 1/10, step: 2377, loss: 2.61207, accuracy: 0.12279\n",
            "Epoch: 1/10, step: 2378, loss: 2.61217, accuracy: 0.12279\n",
            "Epoch: 1/10, step: 2379, loss: 2.61205, accuracy: 0.12279\n",
            "Epoch: 1/10, step: 2380, loss: 2.61216, accuracy: 0.12279\n",
            "Epoch: 1/10, step: 2381, loss: 2.61210, accuracy: 0.12280\n",
            "Epoch: 1/10, step: 2382, loss: 2.61199, accuracy: 0.12285\n",
            "Epoch: 1/10, step: 2383, loss: 2.61194, accuracy: 0.12290\n",
            "Epoch: 1/10, step: 2384, loss: 2.61191, accuracy: 0.12290\n",
            "Epoch: 1/10, step: 2385, loss: 2.61197, accuracy: 0.12290\n",
            "Epoch: 1/10, step: 2386, loss: 2.61208, accuracy: 0.12285\n",
            "Epoch: 1/10, step: 2387, loss: 2.61212, accuracy: 0.12285\n",
            "Epoch: 1/10, step: 2388, loss: 2.61196, accuracy: 0.12296\n",
            "Epoch: 1/10, step: 2389, loss: 2.61194, accuracy: 0.12306\n",
            "Epoch: 1/10, step: 2390, loss: 2.61181, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2391, loss: 2.61183, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2392, loss: 2.61196, accuracy: 0.12307\n",
            "Epoch: 1/10, step: 2393, loss: 2.61192, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2394, loss: 2.61201, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2395, loss: 2.61196, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2396, loss: 2.61198, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2397, loss: 2.61185, accuracy: 0.12312\n",
            "Epoch: 1/10, step: 2398, loss: 2.61193, accuracy: 0.12307\n",
            "Epoch: 1/10, step: 2399, loss: 2.61200, accuracy: 0.12302\n",
            "Epoch: 1/10, step: 2400, loss: 2.61173, accuracy: 0.12307\n",
            "Epoch: 1/10, step: 2401, loss: 2.61169, accuracy: 0.12313\n",
            "Epoch: 1/10, step: 2402, loss: 2.61192, accuracy: 0.12313\n",
            "Epoch: 1/10, step: 2403, loss: 2.61203, accuracy: 0.12313\n",
            "Epoch: 1/10, step: 2404, loss: 2.61205, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2405, loss: 2.61187, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2406, loss: 2.61187, accuracy: 0.12303\n",
            "Epoch: 1/10, step: 2407, loss: 2.61190, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2408, loss: 2.61188, accuracy: 0.12313\n",
            "Epoch: 1/10, step: 2409, loss: 2.61159, accuracy: 0.12318\n",
            "Epoch: 1/10, step: 2410, loss: 2.61190, accuracy: 0.12313\n",
            "Epoch: 1/10, step: 2411, loss: 2.61183, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2412, loss: 2.61158, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2413, loss: 2.61130, accuracy: 0.12314\n",
            "Epoch: 1/10, step: 2414, loss: 2.61121, accuracy: 0.12314\n",
            "Epoch: 1/10, step: 2415, loss: 2.61128, accuracy: 0.12308\n",
            "Epoch: 1/10, step: 2416, loss: 2.61117, accuracy: 0.12319\n",
            "Epoch: 1/10, step: 2417, loss: 2.61124, accuracy: 0.12314\n",
            "Epoch: 1/10, step: 2418, loss: 2.61127, accuracy: 0.12314\n",
            "Epoch: 1/10, step: 2419, loss: 2.61136, accuracy: 0.12309\n",
            "Epoch: 1/10, step: 2420, loss: 2.61147, accuracy: 0.12304\n",
            "Epoch: 1/10, step: 2421, loss: 2.61140, accuracy: 0.12304\n",
            "Epoch: 1/10, step: 2422, loss: 2.61136, accuracy: 0.12299\n",
            "Epoch: 1/10, step: 2423, loss: 2.61112, accuracy: 0.12314\n",
            "Epoch: 1/10, step: 2424, loss: 2.61125, accuracy: 0.12309\n",
            "Epoch: 1/10, step: 2425, loss: 2.61128, accuracy: 0.12304\n",
            "Epoch: 1/10, step: 2426, loss: 2.61121, accuracy: 0.12299\n",
            "Epoch: 1/10, step: 2427, loss: 2.61108, accuracy: 0.12299\n",
            "Epoch: 1/10, step: 2428, loss: 2.61104, accuracy: 0.12299\n",
            "Epoch: 1/10, step: 2429, loss: 2.61107, accuracy: 0.12294\n",
            "Epoch: 1/10, step: 2430, loss: 2.61083, accuracy: 0.12305\n",
            "Epoch: 1/10, step: 2431, loss: 2.61079, accuracy: 0.12305\n",
            "Epoch: 1/10, step: 2432, loss: 2.61060, accuracy: 0.12305\n",
            "Epoch: 1/10, step: 2433, loss: 2.61060, accuracy: 0.12310\n",
            "Epoch: 1/10, step: 2434, loss: 2.61046, accuracy: 0.12310\n",
            "Epoch: 1/10, step: 2435, loss: 2.61059, accuracy: 0.12310\n",
            "Epoch: 1/10, step: 2436, loss: 2.61088, accuracy: 0.12305\n",
            "Epoch: 1/10, step: 2437, loss: 2.61085, accuracy: 0.12310\n",
            "Epoch: 1/10, step: 2438, loss: 2.61101, accuracy: 0.12310\n",
            "Epoch: 1/10, step: 2439, loss: 2.61072, accuracy: 0.12315\n",
            "Epoch: 1/10, step: 2440, loss: 2.61064, accuracy: 0.12316\n",
            "Epoch: 1/10, step: 2441, loss: 2.61056, accuracy: 0.12311\n",
            "Epoch: 1/10, step: 2442, loss: 2.61052, accuracy: 0.12311\n",
            "Epoch: 1/10, step: 2443, loss: 2.61030, accuracy: 0.12316\n",
            "Epoch: 1/10, step: 2444, loss: 2.61020, accuracy: 0.12321\n",
            "Epoch: 1/10, step: 2445, loss: 2.61009, accuracy: 0.12316\n",
            "Epoch: 1/10, step: 2446, loss: 2.61002, accuracy: 0.12321\n",
            "Epoch: 1/10, step: 2447, loss: 2.61020, accuracy: 0.12326\n",
            "Epoch: 1/10, step: 2448, loss: 2.61040, accuracy: 0.12321\n",
            "Epoch: 1/10, step: 2449, loss: 2.61032, accuracy: 0.12321\n",
            "Epoch: 1/10, step: 2450, loss: 2.61018, accuracy: 0.12327\n",
            "Epoch: 1/10, step: 2451, loss: 2.61018, accuracy: 0.12327\n",
            "Epoch: 1/10, step: 2452, loss: 2.61034, accuracy: 0.12322\n",
            "Epoch: 1/10, step: 2453, loss: 2.61025, accuracy: 0.12317\n",
            "Epoch: 1/10, step: 2454, loss: 2.61029, accuracy: 0.12317\n",
            "Epoch: 1/10, step: 2455, loss: 2.61032, accuracy: 0.12317\n",
            "Epoch: 1/10, step: 2456, loss: 2.61030, accuracy: 0.12322\n",
            "Epoch: 1/10, step: 2457, loss: 2.61005, accuracy: 0.12327\n",
            "Epoch: 1/10, step: 2458, loss: 2.60995, accuracy: 0.12327\n",
            "Epoch: 1/10, step: 2459, loss: 2.60996, accuracy: 0.12322\n",
            "Epoch: 1/10, step: 2460, loss: 2.60991, accuracy: 0.12322\n",
            "Epoch: 1/10, step: 2461, loss: 2.60973, accuracy: 0.12327\n",
            "Epoch: 1/10, step: 2462, loss: 2.60964, accuracy: 0.12332\n",
            "Epoch: 1/10, step: 2463, loss: 2.60951, accuracy: 0.12338\n",
            "Epoch: 1/10, step: 2464, loss: 2.60931, accuracy: 0.12338\n",
            "Epoch: 1/10, step: 2465, loss: 2.60916, accuracy: 0.12338\n",
            "Epoch: 1/10, step: 2466, loss: 2.60897, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2467, loss: 2.60885, accuracy: 0.12348\n",
            "Epoch: 1/10, step: 2468, loss: 2.60890, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2469, loss: 2.60878, accuracy: 0.12338\n",
            "Epoch: 1/10, step: 2470, loss: 2.60865, accuracy: 0.12348\n",
            "Epoch: 1/10, step: 2471, loss: 2.60868, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2472, loss: 2.60865, accuracy: 0.12338\n",
            "Epoch: 1/10, step: 2473, loss: 2.60843, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2474, loss: 2.60844, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2475, loss: 2.60851, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2476, loss: 2.60874, accuracy: 0.12343\n",
            "Epoch: 1/10, step: 2477, loss: 2.60876, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2478, loss: 2.60877, accuracy: 0.12339\n",
            "Epoch: 1/10, step: 2479, loss: 2.60864, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2480, loss: 2.60854, accuracy: 0.12349\n",
            "Epoch: 1/10, step: 2481, loss: 2.60869, accuracy: 0.12349\n",
            "Epoch: 1/10, step: 2482, loss: 2.60900, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2483, loss: 2.60905, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2484, loss: 2.60908, accuracy: 0.12339\n",
            "Epoch: 1/10, step: 2485, loss: 2.60924, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2486, loss: 2.60900, accuracy: 0.12344\n",
            "Epoch: 1/10, step: 2487, loss: 2.60880, accuracy: 0.12354\n",
            "Epoch: 1/10, step: 2488, loss: 2.60853, accuracy: 0.12359\n",
            "Epoch: 1/10, step: 2489, loss: 2.60853, accuracy: 0.12359\n",
            "Epoch: 1/10, step: 2490, loss: 2.60853, accuracy: 0.12364\n",
            "Epoch: 1/10, step: 2491, loss: 2.60868, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2492, loss: 2.60857, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2493, loss: 2.60859, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2494, loss: 2.60877, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2495, loss: 2.60874, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2496, loss: 2.60877, accuracy: 0.12360\n",
            "Epoch: 1/10, step: 2497, loss: 2.60882, accuracy: 0.12360\n",
            "Epoch: 1/10, step: 2498, loss: 2.60880, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2499, loss: 2.60875, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2500, loss: 2.60868, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2501, loss: 2.60873, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2502, loss: 2.60853, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2503, loss: 2.60858, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2504, loss: 2.60844, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2505, loss: 2.60858, accuracy: 0.12365\n",
            "Epoch: 1/10, step: 2506, loss: 2.60846, accuracy: 0.12375\n",
            "Epoch: 1/10, step: 2507, loss: 2.60840, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2508, loss: 2.60822, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2509, loss: 2.60812, accuracy: 0.12370\n",
            "Epoch: 1/10, step: 2510, loss: 2.60786, accuracy: 0.12375\n",
            "Epoch: 1/10, step: 2511, loss: 2.60774, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2512, loss: 2.60755, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2513, loss: 2.60725, accuracy: 0.12396\n",
            "Epoch: 1/10, step: 2514, loss: 2.60724, accuracy: 0.12391\n",
            "Epoch: 1/10, step: 2515, loss: 2.60717, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2516, loss: 2.60712, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2517, loss: 2.60711, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2518, loss: 2.60708, accuracy: 0.12391\n",
            "Epoch: 1/10, step: 2519, loss: 2.60717, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2520, loss: 2.60731, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2521, loss: 2.60724, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2522, loss: 2.60730, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2523, loss: 2.60716, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2524, loss: 2.60720, accuracy: 0.12391\n",
            "Epoch: 1/10, step: 2525, loss: 2.60727, accuracy: 0.12391\n",
            "Epoch: 1/10, step: 2526, loss: 2.60721, accuracy: 0.12386\n",
            "Epoch: 1/10, step: 2527, loss: 2.60731, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2528, loss: 2.60729, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2529, loss: 2.60722, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2530, loss: 2.60728, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2531, loss: 2.60727, accuracy: 0.12381\n",
            "Epoch: 1/10, step: 2532, loss: 2.60730, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2533, loss: 2.60727, accuracy: 0.12372\n",
            "Epoch: 1/10, step: 2534, loss: 2.60715, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2535, loss: 2.60715, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2536, loss: 2.60709, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2537, loss: 2.60691, accuracy: 0.12372\n",
            "Epoch: 1/10, step: 2538, loss: 2.60683, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2539, loss: 2.60684, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2540, loss: 2.60671, accuracy: 0.12382\n",
            "Epoch: 1/10, step: 2541, loss: 2.60680, accuracy: 0.12382\n",
            "Epoch: 1/10, step: 2542, loss: 2.60679, accuracy: 0.12377\n",
            "Epoch: 1/10, step: 2543, loss: 2.60670, accuracy: 0.12382\n",
            "Epoch: 1/10, step: 2544, loss: 2.60664, accuracy: 0.12382\n",
            "Epoch: 1/10, step: 2545, loss: 2.60654, accuracy: 0.12387\n",
            "Epoch: 1/10, step: 2546, loss: 2.60624, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2547, loss: 2.60624, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2548, loss: 2.60605, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2549, loss: 2.60608, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2550, loss: 2.60610, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2551, loss: 2.60625, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2552, loss: 2.60638, accuracy: 0.12392\n",
            "Epoch: 1/10, step: 2553, loss: 2.60617, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2554, loss: 2.60626, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2555, loss: 2.60628, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2556, loss: 2.60629, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2557, loss: 2.60631, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2558, loss: 2.60640, accuracy: 0.12392\n",
            "Epoch: 1/10, step: 2559, loss: 2.60647, accuracy: 0.12388\n",
            "Epoch: 1/10, step: 2560, loss: 2.60628, accuracy: 0.12393\n",
            "Epoch: 1/10, step: 2561, loss: 2.60620, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2562, loss: 2.60602, accuracy: 0.12407\n",
            "Epoch: 1/10, step: 2563, loss: 2.60596, accuracy: 0.12412\n",
            "Epoch: 1/10, step: 2564, loss: 2.60570, accuracy: 0.12422\n",
            "Epoch: 1/10, step: 2565, loss: 2.60578, accuracy: 0.12422\n",
            "Epoch: 1/10, step: 2566, loss: 2.60561, accuracy: 0.12417\n",
            "Epoch: 1/10, step: 2567, loss: 2.60550, accuracy: 0.12412\n",
            "Epoch: 1/10, step: 2568, loss: 2.60566, accuracy: 0.12408\n",
            "Epoch: 1/10, step: 2569, loss: 2.60563, accuracy: 0.12408\n",
            "Epoch: 1/10, step: 2570, loss: 2.60560, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2571, loss: 2.60547, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2572, loss: 2.60536, accuracy: 0.12413\n",
            "Epoch: 1/10, step: 2573, loss: 2.60532, accuracy: 0.12408\n",
            "Epoch: 1/10, step: 2574, loss: 2.60501, accuracy: 0.12422\n",
            "Epoch: 1/10, step: 2575, loss: 2.60525, accuracy: 0.12417\n",
            "Epoch: 1/10, step: 2576, loss: 2.60495, accuracy: 0.12418\n",
            "Epoch: 1/10, step: 2577, loss: 2.60502, accuracy: 0.12413\n",
            "Epoch: 1/10, step: 2578, loss: 2.60498, accuracy: 0.12408\n",
            "Epoch: 1/10, step: 2579, loss: 2.60493, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2580, loss: 2.60487, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2581, loss: 2.60490, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2582, loss: 2.60482, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2583, loss: 2.60465, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2584, loss: 2.60442, accuracy: 0.12408\n",
            "Epoch: 1/10, step: 2585, loss: 2.60448, accuracy: 0.12403\n",
            "Epoch: 1/10, step: 2586, loss: 2.60458, accuracy: 0.12398\n",
            "Epoch: 1/10, step: 2587, loss: 2.60448, accuracy: 0.12399\n",
            "Epoch: 1/10, step: 2588, loss: 2.60446, accuracy: 0.12394\n",
            "Epoch: 1/10, step: 2589, loss: 2.60447, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2590, loss: 2.60443, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2591, loss: 2.60446, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2592, loss: 2.60454, accuracy: 0.12384\n",
            "Epoch: 1/10, step: 2593, loss: 2.60447, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2594, loss: 2.60454, accuracy: 0.12384\n",
            "Epoch: 1/10, step: 2595, loss: 2.60445, accuracy: 0.12384\n",
            "Epoch: 1/10, step: 2596, loss: 2.60452, accuracy: 0.12380\n",
            "Epoch: 1/10, step: 2597, loss: 2.60446, accuracy: 0.12380\n",
            "Epoch: 1/10, step: 2598, loss: 2.60436, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2599, loss: 2.60429, accuracy: 0.12389\n",
            "Epoch: 1/10, step: 2600, loss: 2.60428, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2601, loss: 2.60419, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2602, loss: 2.60417, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2603, loss: 2.60410, accuracy: 0.12380\n",
            "Epoch: 1/10, step: 2604, loss: 2.60390, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2605, loss: 2.60376, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2606, loss: 2.60376, accuracy: 0.12385\n",
            "Epoch: 1/10, step: 2607, loss: 2.60347, accuracy: 0.12390\n",
            "Epoch: 1/10, step: 2608, loss: 2.60329, accuracy: 0.12395\n",
            "Epoch: 1/10, step: 2609, loss: 2.60322, accuracy: 0.12399\n",
            "Epoch: 1/10, step: 2610, loss: 2.60312, accuracy: 0.12399\n",
            "Epoch: 1/10, step: 2611, loss: 2.60298, accuracy: 0.12399\n",
            "Epoch: 1/10, step: 2612, loss: 2.60273, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2613, loss: 2.60276, accuracy: 0.12409\n",
            "Epoch: 1/10, step: 2614, loss: 2.60264, accuracy: 0.12409\n",
            "Epoch: 1/10, step: 2615, loss: 2.60250, accuracy: 0.12409\n",
            "Epoch: 1/10, step: 2616, loss: 2.60238, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2617, loss: 2.60224, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2618, loss: 2.60204, accuracy: 0.12419\n",
            "Epoch: 1/10, step: 2619, loss: 2.60201, accuracy: 0.12419\n",
            "Epoch: 1/10, step: 2620, loss: 2.60206, accuracy: 0.12424\n",
            "Epoch: 1/10, step: 2621, loss: 2.60216, accuracy: 0.12419\n",
            "Epoch: 1/10, step: 2622, loss: 2.60194, accuracy: 0.12424\n",
            "Epoch: 1/10, step: 2623, loss: 2.60205, accuracy: 0.12419\n",
            "Epoch: 1/10, step: 2624, loss: 2.60216, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2625, loss: 2.60207, accuracy: 0.12419\n",
            "Epoch: 1/10, step: 2626, loss: 2.60213, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2627, loss: 2.60204, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2628, loss: 2.60212, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2629, loss: 2.60209, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2630, loss: 2.60208, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2631, loss: 2.60179, accuracy: 0.12414\n",
            "Epoch: 1/10, step: 2632, loss: 2.60180, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2633, loss: 2.60173, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2634, loss: 2.60159, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2635, loss: 2.60152, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2636, loss: 2.60162, accuracy: 0.12405\n",
            "Epoch: 1/10, step: 2637, loss: 2.60149, accuracy: 0.12400\n",
            "Epoch: 1/10, step: 2638, loss: 2.60155, accuracy: 0.12405\n",
            "Epoch: 1/10, step: 2639, loss: 2.60151, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2640, loss: 2.60165, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2641, loss: 2.60158, accuracy: 0.12405\n",
            "Epoch: 1/10, step: 2642, loss: 2.60134, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2643, loss: 2.60118, accuracy: 0.12420\n",
            "Epoch: 1/10, step: 2644, loss: 2.60116, accuracy: 0.12420\n",
            "Epoch: 1/10, step: 2645, loss: 2.60121, accuracy: 0.12415\n",
            "Epoch: 1/10, step: 2646, loss: 2.60123, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2647, loss: 2.60109, accuracy: 0.12410\n",
            "Epoch: 1/10, step: 2648, loss: 2.60098, accuracy: 0.12406\n",
            "Epoch: 1/10, step: 2649, loss: 2.60081, accuracy: 0.12406\n",
            "Epoch: 1/10, step: 2650, loss: 2.60078, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2651, loss: 2.60077, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2652, loss: 2.60066, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2653, loss: 2.60049, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2654, loss: 2.60044, accuracy: 0.12396\n",
            "Epoch: 1/10, step: 2655, loss: 2.60029, accuracy: 0.12396\n",
            "Epoch: 1/10, step: 2656, loss: 2.60038, accuracy: 0.12396\n",
            "Epoch: 1/10, step: 2657, loss: 2.60033, accuracy: 0.12396\n",
            "Epoch: 1/10, step: 2658, loss: 2.60029, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2659, loss: 2.60014, accuracy: 0.12406\n",
            "Epoch: 1/10, step: 2660, loss: 2.60012, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2661, loss: 2.60009, accuracy: 0.12401\n",
            "Epoch: 1/10, step: 2662, loss: 2.59982, accuracy: 0.12411\n",
            "Epoch: 1/10, step: 2663, loss: 2.59980, accuracy: 0.12411\n",
            "Epoch: 1/10, step: 2664, loss: 2.59966, accuracy: 0.12411\n",
            "Epoch: 1/10, step: 2665, loss: 2.59959, accuracy: 0.12411\n",
            "Epoch: 1/10, step: 2666, loss: 2.59957, accuracy: 0.12406\n",
            "Epoch: 1/10, step: 2667, loss: 2.59970, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2668, loss: 2.59978, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2669, loss: 2.59977, accuracy: 0.12392\n",
            "Epoch: 1/10, step: 2670, loss: 2.59961, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2671, loss: 2.59990, accuracy: 0.12392\n",
            "Epoch: 1/10, step: 2672, loss: 2.59990, accuracy: 0.12388\n",
            "Epoch: 1/10, step: 2673, loss: 2.60016, accuracy: 0.12383\n",
            "Epoch: 1/10, step: 2674, loss: 2.59991, accuracy: 0.12388\n",
            "Epoch: 1/10, step: 2675, loss: 2.59983, accuracy: 0.12393\n",
            "Epoch: 1/10, step: 2676, loss: 2.59969, accuracy: 0.12393\n",
            "Epoch: 1/10, step: 2677, loss: 2.59956, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2678, loss: 2.59950, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2679, loss: 2.59941, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2680, loss: 2.59956, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2681, loss: 2.59938, accuracy: 0.12397\n",
            "Epoch: 1/10, step: 2682, loss: 2.59925, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2683, loss: 2.59937, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2684, loss: 2.59953, accuracy: 0.12398\n",
            "Epoch: 1/10, step: 2685, loss: 2.59945, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2686, loss: 2.59958, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2687, loss: 2.59970, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2688, loss: 2.59961, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2689, loss: 2.59980, accuracy: 0.12402\n",
            "Epoch: 1/10, step: 2690, loss: 2.59952, accuracy: 0.12416\n",
            "Epoch: 1/10, step: 2691, loss: 2.59948, accuracy: 0.12416\n",
            "Epoch: 1/10, step: 2692, loss: 2.59943, accuracy: 0.12416\n",
            "Epoch: 1/10, step: 2693, loss: 2.59919, accuracy: 0.12421\n",
            "Epoch: 1/10, step: 2694, loss: 2.59917, accuracy: 0.12421\n",
            "Epoch: 1/10, step: 2695, loss: 2.59915, accuracy: 0.12417\n",
            "Epoch: 1/10, step: 2696, loss: 2.59898, accuracy: 0.12421\n",
            "Epoch: 1/10, step: 2697, loss: 2.59876, accuracy: 0.12430\n",
            "Epoch: 1/10, step: 2698, loss: 2.59891, accuracy: 0.12426\n",
            "Epoch: 1/10, step: 2699, loss: 2.59905, accuracy: 0.12421\n",
            "Epoch: 1/10, step: 2700, loss: 2.59928, accuracy: 0.12417\n",
            "Epoch: 1/10, step: 2701, loss: 2.59905, accuracy: 0.12426\n",
            "Epoch: 1/10, step: 2702, loss: 2.59896, accuracy: 0.12431\n",
            "Epoch: 1/10, step: 2703, loss: 2.59884, accuracy: 0.12435\n",
            "Epoch: 1/10, step: 2704, loss: 2.59893, accuracy: 0.12435\n",
            "Epoch: 1/10, step: 2705, loss: 2.59894, accuracy: 0.12431\n",
            "Epoch: 1/10, step: 2706, loss: 2.59900, accuracy: 0.12431\n",
            "Epoch: 1/10, step: 2707, loss: 2.59875, accuracy: 0.12449\n",
            "Epoch: 1/10, step: 2708, loss: 2.59877, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2709, loss: 2.59883, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2710, loss: 2.59862, accuracy: 0.12449\n",
            "Epoch: 1/10, step: 2711, loss: 2.59841, accuracy: 0.12449\n",
            "Epoch: 1/10, step: 2712, loss: 2.59841, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2713, loss: 2.59829, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2714, loss: 2.59825, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2715, loss: 2.59806, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2716, loss: 2.59810, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2717, loss: 2.59810, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2718, loss: 2.59804, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2719, loss: 2.59796, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2720, loss: 2.59798, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2721, loss: 2.59812, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2722, loss: 2.59809, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2723, loss: 2.59815, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2724, loss: 2.59812, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2725, loss: 2.59810, accuracy: 0.12440\n",
            "Epoch: 1/10, step: 2726, loss: 2.59799, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2727, loss: 2.59782, accuracy: 0.12454\n",
            "Epoch: 1/10, step: 2728, loss: 2.59769, accuracy: 0.12454\n",
            "Epoch: 1/10, step: 2729, loss: 2.59768, accuracy: 0.12459\n",
            "Epoch: 1/10, step: 2730, loss: 2.59774, accuracy: 0.12454\n",
            "Epoch: 1/10, step: 2731, loss: 2.59772, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2732, loss: 2.59788, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2733, loss: 2.59787, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2734, loss: 2.59776, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2735, loss: 2.59779, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2736, loss: 2.59785, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2737, loss: 2.59788, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2738, loss: 2.59788, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2739, loss: 2.59767, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2740, loss: 2.59776, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2741, loss: 2.59790, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2742, loss: 2.59788, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2743, loss: 2.59772, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2744, loss: 2.59761, accuracy: 0.12454\n",
            "Epoch: 1/10, step: 2745, loss: 2.59779, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2746, loss: 2.59800, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2747, loss: 2.59799, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2748, loss: 2.59791, accuracy: 0.12445\n",
            "Epoch: 1/10, step: 2749, loss: 2.59809, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2750, loss: 2.59809, accuracy: 0.12436\n",
            "Epoch: 1/10, step: 2751, loss: 2.59820, accuracy: 0.12432\n",
            "Epoch: 1/10, step: 2752, loss: 2.59822, accuracy: 0.12432\n",
            "Epoch: 1/10, step: 2753, loss: 2.59823, accuracy: 0.12432\n",
            "Epoch: 1/10, step: 2754, loss: 2.59802, accuracy: 0.12436\n",
            "Epoch: 1/10, step: 2755, loss: 2.59790, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2756, loss: 2.59779, accuracy: 0.12446\n",
            "Epoch: 1/10, step: 2757, loss: 2.59784, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2758, loss: 2.59789, accuracy: 0.12437\n",
            "Epoch: 1/10, step: 2759, loss: 2.59789, accuracy: 0.12432\n",
            "Epoch: 1/10, step: 2760, loss: 2.59779, accuracy: 0.12437\n",
            "Epoch: 1/10, step: 2761, loss: 2.59790, accuracy: 0.12432\n",
            "Epoch: 1/10, step: 2762, loss: 2.59792, accuracy: 0.12437\n",
            "Epoch: 1/10, step: 2763, loss: 2.59770, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2764, loss: 2.59775, accuracy: 0.12437\n",
            "Epoch: 1/10, step: 2765, loss: 2.59770, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2766, loss: 2.59766, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2767, loss: 2.59761, accuracy: 0.12441\n",
            "Epoch: 1/10, step: 2768, loss: 2.59739, accuracy: 0.12450\n",
            "Epoch: 1/10, step: 2769, loss: 2.59727, accuracy: 0.12455\n",
            "Epoch: 1/10, step: 2770, loss: 2.59705, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2771, loss: 2.59700, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2772, loss: 2.59704, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2773, loss: 2.59699, accuracy: 0.12459\n",
            "Epoch: 1/10, step: 2774, loss: 2.59679, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2775, loss: 2.59683, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2776, loss: 2.59688, accuracy: 0.12459\n",
            "Epoch: 1/10, step: 2777, loss: 2.59682, accuracy: 0.12468\n",
            "Epoch: 1/10, step: 2778, loss: 2.59674, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2779, loss: 2.59668, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2780, loss: 2.59665, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2781, loss: 2.59677, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2782, loss: 2.59706, accuracy: 0.12460\n",
            "Epoch: 1/10, step: 2783, loss: 2.59712, accuracy: 0.12455\n",
            "Epoch: 1/10, step: 2784, loss: 2.59722, accuracy: 0.12455\n",
            "Epoch: 1/10, step: 2785, loss: 2.59751, accuracy: 0.12451\n",
            "Epoch: 1/10, step: 2786, loss: 2.59748, accuracy: 0.12451\n",
            "Epoch: 1/10, step: 2787, loss: 2.59765, accuracy: 0.12455\n",
            "Epoch: 1/10, step: 2788, loss: 2.59752, accuracy: 0.12455\n",
            "Epoch: 1/10, step: 2789, loss: 2.59720, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2790, loss: 2.59734, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2791, loss: 2.59735, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2792, loss: 2.59713, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2793, loss: 2.59713, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2794, loss: 2.59709, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2795, loss: 2.59741, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2796, loss: 2.59745, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2797, loss: 2.59758, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2798, loss: 2.59782, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2799, loss: 2.59765, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2800, loss: 2.59744, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2801, loss: 2.59728, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2802, loss: 2.59712, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2803, loss: 2.59709, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2804, loss: 2.59715, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2805, loss: 2.59731, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2806, loss: 2.59730, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2807, loss: 2.59736, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2808, loss: 2.59739, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2809, loss: 2.59752, accuracy: 0.12469\n",
            "Epoch: 1/10, step: 2810, loss: 2.59753, accuracy: 0.12464\n",
            "Epoch: 1/10, step: 2811, loss: 2.59730, accuracy: 0.12473\n",
            "Epoch: 1/10, step: 2812, loss: 2.59745, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2813, loss: 2.59749, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2814, loss: 2.59742, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2815, loss: 2.59739, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2816, loss: 2.59757, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2817, loss: 2.59779, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2818, loss: 2.59788, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2819, loss: 2.59772, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2820, loss: 2.59760, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2821, loss: 2.59762, accuracy: 0.12491\n",
            "Epoch: 1/10, step: 2822, loss: 2.59750, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2823, loss: 2.59755, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2824, loss: 2.59747, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2825, loss: 2.59729, accuracy: 0.12496\n",
            "Epoch: 1/10, step: 2826, loss: 2.59733, accuracy: 0.12491\n",
            "Epoch: 1/10, step: 2827, loss: 2.59733, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2828, loss: 2.59741, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2829, loss: 2.59750, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2830, loss: 2.59754, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2831, loss: 2.59742, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2832, loss: 2.59744, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2833, loss: 2.59733, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2834, loss: 2.59721, accuracy: 0.12478\n",
            "Epoch: 1/10, step: 2835, loss: 2.59738, accuracy: 0.12474\n",
            "Epoch: 1/10, step: 2836, loss: 2.59730, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2837, loss: 2.59737, accuracy: 0.12482\n",
            "Epoch: 1/10, step: 2838, loss: 2.59708, accuracy: 0.12487\n",
            "Epoch: 1/10, step: 2839, loss: 2.59700, accuracy: 0.12496\n",
            "Epoch: 1/10, step: 2840, loss: 2.59708, accuracy: 0.12491\n",
            "Epoch: 1/10, step: 2841, loss: 2.59714, accuracy: 0.12496\n",
            "Epoch: 1/10, step: 2842, loss: 2.59706, accuracy: 0.12504\n",
            "Epoch: 1/10, step: 2843, loss: 2.59695, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2844, loss: 2.59716, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2845, loss: 2.59713, accuracy: 0.12509\n",
            "Epoch: 1/10, step: 2846, loss: 2.59686, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2847, loss: 2.59682, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2848, loss: 2.59673, accuracy: 0.12509\n",
            "Epoch: 1/10, step: 2849, loss: 2.59676, accuracy: 0.12504\n",
            "Epoch: 1/10, step: 2850, loss: 2.59673, accuracy: 0.12500\n",
            "Epoch: 1/10, step: 2851, loss: 2.59682, accuracy: 0.12500\n",
            "Epoch: 1/10, step: 2852, loss: 2.59683, accuracy: 0.12500\n",
            "Epoch: 1/10, step: 2853, loss: 2.59674, accuracy: 0.12496\n",
            "Epoch: 1/10, step: 2854, loss: 2.59675, accuracy: 0.12491\n",
            "Epoch: 1/10, step: 2855, loss: 2.59700, accuracy: 0.12496\n",
            "Epoch: 1/10, step: 2856, loss: 2.59701, accuracy: 0.12500\n",
            "Epoch: 1/10, step: 2857, loss: 2.59684, accuracy: 0.12504\n",
            "Epoch: 1/10, step: 2858, loss: 2.59655, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2859, loss: 2.59654, accuracy: 0.12513\n",
            "Epoch: 1/10, step: 2860, loss: 2.59656, accuracy: 0.12517\n",
            "Epoch: 1/10, step: 2861, loss: 2.59649, accuracy: 0.12522\n",
            "Epoch: 1/10, step: 2862, loss: 2.59649, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2863, loss: 2.59655, accuracy: 0.12522\n",
            "Epoch: 1/10, step: 2864, loss: 2.59646, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2865, loss: 2.59647, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2866, loss: 2.59642, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2867, loss: 2.59631, accuracy: 0.12531\n",
            "Epoch: 1/10, step: 2868, loss: 2.59626, accuracy: 0.12531\n",
            "Epoch: 1/10, step: 2869, loss: 2.59647, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2870, loss: 2.59645, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2871, loss: 2.59637, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2872, loss: 2.59627, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2873, loss: 2.59639, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2874, loss: 2.59664, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2875, loss: 2.59659, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2876, loss: 2.59689, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2877, loss: 2.59705, accuracy: 0.12522\n",
            "Epoch: 1/10, step: 2878, loss: 2.59690, accuracy: 0.12522\n",
            "Epoch: 1/10, step: 2879, loss: 2.59693, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2880, loss: 2.59692, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2881, loss: 2.59678, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2882, loss: 2.59677, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2883, loss: 2.59680, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2884, loss: 2.59677, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2885, loss: 2.59702, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2886, loss: 2.59724, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2887, loss: 2.59721, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2888, loss: 2.59744, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2889, loss: 2.59727, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2890, loss: 2.59724, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2891, loss: 2.59723, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2892, loss: 2.59730, accuracy: 0.12526\n",
            "Epoch: 1/10, step: 2893, loss: 2.59710, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2894, loss: 2.59705, accuracy: 0.12530\n",
            "Epoch: 1/10, step: 2895, loss: 2.59698, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2896, loss: 2.59695, accuracy: 0.12539\n",
            "Epoch: 1/10, step: 2897, loss: 2.59710, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2898, loss: 2.59711, accuracy: 0.12535\n",
            "Epoch: 1/10, step: 2899, loss: 2.59705, accuracy: 0.12539\n",
            "Epoch: 1/10, step: 2900, loss: 2.59701, accuracy: 0.12539\n",
            "Epoch: 1/10, step: 2901, loss: 2.59699, accuracy: 0.12539\n",
            "Epoch: 1/10, step: 2902, loss: 2.59687, accuracy: 0.12543\n",
            "Epoch: 1/10, step: 2903, loss: 2.59697, accuracy: 0.12543\n",
            "Epoch: 1/10, step: 2904, loss: 2.59685, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2905, loss: 2.59682, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2906, loss: 2.59665, accuracy: 0.12556\n",
            "Epoch: 1/10, step: 2907, loss: 2.59645, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 2908, loss: 2.59649, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2909, loss: 2.59629, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2910, loss: 2.59603, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 2911, loss: 2.59593, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2912, loss: 2.59582, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2913, loss: 2.59578, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2914, loss: 2.59553, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2915, loss: 2.59545, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2916, loss: 2.59544, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2917, loss: 2.59536, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2918, loss: 2.59513, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2919, loss: 2.59513, accuracy: 0.12556\n",
            "Epoch: 1/10, step: 2920, loss: 2.59522, accuracy: 0.12551\n",
            "Epoch: 1/10, step: 2921, loss: 2.59534, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2922, loss: 2.59547, accuracy: 0.12543\n",
            "Epoch: 1/10, step: 2923, loss: 2.59550, accuracy: 0.12538\n",
            "Epoch: 1/10, step: 2924, loss: 2.59568, accuracy: 0.12538\n",
            "Epoch: 1/10, step: 2925, loss: 2.59547, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2926, loss: 2.59555, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2927, loss: 2.59547, accuracy: 0.12547\n",
            "Epoch: 1/10, step: 2928, loss: 2.59532, accuracy: 0.12551\n",
            "Epoch: 1/10, step: 2929, loss: 2.59526, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2930, loss: 2.59507, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2931, loss: 2.59496, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2932, loss: 2.59514, accuracy: 0.12560\n",
            "Epoch: 1/10, step: 2933, loss: 2.59529, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2934, loss: 2.59513, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2935, loss: 2.59508, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2936, loss: 2.59495, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2937, loss: 2.59510, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2938, loss: 2.59491, accuracy: 0.12568\n",
            "Epoch: 1/10, step: 2939, loss: 2.59496, accuracy: 0.12572\n",
            "Epoch: 1/10, step: 2940, loss: 2.59505, accuracy: 0.12568\n",
            "Epoch: 1/10, step: 2941, loss: 2.59519, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2942, loss: 2.59530, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2943, loss: 2.59549, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2944, loss: 2.59535, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2945, loss: 2.59530, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2946, loss: 2.59532, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2947, loss: 2.59524, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2948, loss: 2.59515, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2949, loss: 2.59527, accuracy: 0.12564\n",
            "Epoch: 1/10, step: 2950, loss: 2.59516, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2951, loss: 2.59505, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2952, loss: 2.59503, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2953, loss: 2.59509, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2954, loss: 2.59502, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2955, loss: 2.59498, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2956, loss: 2.59494, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2957, loss: 2.59486, accuracy: 0.12563\n",
            "Epoch: 1/10, step: 2958, loss: 2.59509, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2959, loss: 2.59520, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2960, loss: 2.59522, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2961, loss: 2.59511, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2962, loss: 2.59508, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2963, loss: 2.59510, accuracy: 0.12551\n",
            "Epoch: 1/10, step: 2964, loss: 2.59522, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2965, loss: 2.59517, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2966, loss: 2.59508, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2967, loss: 2.59517, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2968, loss: 2.59497, accuracy: 0.12567\n",
            "Epoch: 1/10, step: 2969, loss: 2.59490, accuracy: 0.12567\n",
            "Epoch: 1/10, step: 2970, loss: 2.59485, accuracy: 0.12563\n",
            "Epoch: 1/10, step: 2971, loss: 2.59473, accuracy: 0.12563\n",
            "Epoch: 1/10, step: 2972, loss: 2.59484, accuracy: 0.12559\n",
            "Epoch: 1/10, step: 2973, loss: 2.59492, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2974, loss: 2.59496, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2975, loss: 2.59488, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2976, loss: 2.59482, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2977, loss: 2.59461, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2978, loss: 2.59458, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2979, loss: 2.59435, accuracy: 0.12555\n",
            "Epoch: 1/10, step: 2980, loss: 2.59429, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2981, loss: 2.59414, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2982, loss: 2.59408, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2983, loss: 2.59417, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2984, loss: 2.59420, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2985, loss: 2.59409, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2986, loss: 2.59402, accuracy: 0.12554\n",
            "Epoch: 1/10, step: 2987, loss: 2.59403, accuracy: 0.12554\n",
            "Epoch: 1/10, step: 2988, loss: 2.59397, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2989, loss: 2.59380, accuracy: 0.12554\n",
            "Epoch: 1/10, step: 2990, loss: 2.59383, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 2991, loss: 2.59401, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2992, loss: 2.59401, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2993, loss: 2.59420, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 2994, loss: 2.59430, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 2995, loss: 2.59425, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 2996, loss: 2.59419, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 2997, loss: 2.59442, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 2998, loss: 2.59461, accuracy: 0.12538\n",
            "Epoch: 1/10, step: 2999, loss: 2.59458, accuracy: 0.12538\n",
            "Epoch: 1/10, step: 3000, loss: 2.59457, accuracy: 0.12538\n",
            "Epoch: 1/10, step: 3001, loss: 2.59454, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3002, loss: 2.59453, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 3003, loss: 2.59466, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3004, loss: 2.59444, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3005, loss: 2.59457, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3006, loss: 2.59447, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 3007, loss: 2.59443, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3008, loss: 2.59454, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3009, loss: 2.59437, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 3010, loss: 2.59427, accuracy: 0.12542\n",
            "Epoch: 1/10, step: 3011, loss: 2.59421, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3012, loss: 2.59437, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3013, loss: 2.59432, accuracy: 0.12537\n",
            "Epoch: 1/10, step: 3014, loss: 2.59425, accuracy: 0.12541\n",
            "Epoch: 1/10, step: 3015, loss: 2.59408, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 3016, loss: 2.59401, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 3017, loss: 2.59395, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3018, loss: 2.59392, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3019, loss: 2.59386, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3020, loss: 2.59376, accuracy: 0.12546\n",
            "Epoch: 1/10, step: 3021, loss: 2.59373, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 3022, loss: 2.59379, accuracy: 0.12550\n",
            "Epoch: 1/10, step: 3023, loss: 2.59377, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3024, loss: 2.59373, accuracy: 0.12541\n",
            "Epoch: 1/10, step: 3025, loss: 2.59372, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3026, loss: 2.59385, accuracy: 0.12541\n",
            "Epoch: 1/10, step: 3027, loss: 2.59391, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3028, loss: 2.59381, accuracy: 0.12554\n",
            "Epoch: 1/10, step: 3029, loss: 2.59360, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3030, loss: 2.59363, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3031, loss: 2.59353, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3032, loss: 2.59346, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3033, loss: 2.59359, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3034, loss: 2.59365, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3035, loss: 2.59340, accuracy: 0.12578\n",
            "Epoch: 1/10, step: 3036, loss: 2.59341, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3037, loss: 2.59338, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3038, loss: 2.59335, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3039, loss: 2.59350, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3040, loss: 2.59345, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3041, loss: 2.59335, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3042, loss: 2.59335, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3043, loss: 2.59329, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3044, loss: 2.59316, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3045, loss: 2.59323, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3046, loss: 2.59316, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3047, loss: 2.59329, accuracy: 0.12562\n",
            "Epoch: 1/10, step: 3048, loss: 2.59316, accuracy: 0.12566\n",
            "Epoch: 1/10, step: 3049, loss: 2.59298, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3050, loss: 2.59311, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3051, loss: 2.59308, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3052, loss: 2.59311, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3053, loss: 2.59322, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3054, loss: 2.59304, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3055, loss: 2.59299, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3056, loss: 2.59317, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3057, loss: 2.59312, accuracy: 0.12570\n",
            "Epoch: 1/10, step: 3058, loss: 2.59335, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3059, loss: 2.59349, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3060, loss: 2.59350, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3061, loss: 2.59345, accuracy: 0.12574\n",
            "Epoch: 1/10, step: 3062, loss: 2.59345, accuracy: 0.12573\n",
            "Epoch: 1/10, step: 3063, loss: 2.59358, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3064, loss: 2.59364, accuracy: 0.12565\n",
            "Epoch: 1/10, step: 3065, loss: 2.59349, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3066, loss: 2.59332, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3067, loss: 2.59323, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3068, loss: 2.59320, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3069, loss: 2.59317, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3070, loss: 2.59327, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3071, loss: 2.59309, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3072, loss: 2.59305, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3073, loss: 2.59290, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3074, loss: 2.59287, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3075, loss: 2.59296, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3076, loss: 2.59285, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3077, loss: 2.59300, accuracy: 0.12557\n",
            "Epoch: 1/10, step: 3078, loss: 2.59302, accuracy: 0.12553\n",
            "Epoch: 1/10, step: 3079, loss: 2.59302, accuracy: 0.12549\n",
            "Epoch: 1/10, step: 3080, loss: 2.59294, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3081, loss: 2.59282, accuracy: 0.12541\n",
            "Epoch: 1/10, step: 3082, loss: 2.59270, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3083, loss: 2.59266, accuracy: 0.12549\n",
            "Epoch: 1/10, step: 3084, loss: 2.59260, accuracy: 0.12549\n",
            "Epoch: 1/10, step: 3085, loss: 2.59274, accuracy: 0.12545\n",
            "Epoch: 1/10, step: 3086, loss: 2.59259, accuracy: 0.12549\n",
            "Epoch: 1/10, step: 3087, loss: 2.59247, accuracy: 0.12549\n",
            "Epoch: 1/10, step: 3088, loss: 2.59220, accuracy: 0.12561\n",
            "Epoch: 1/10, step: 3089, loss: 2.59208, accuracy: 0.12565\n",
            "Epoch: 1/10, step: 3090, loss: 2.59197, accuracy: 0.12573\n",
            "Epoch: 1/10, step: 3091, loss: 2.59189, accuracy: 0.12573\n",
            "Epoch: 1/10, step: 3092, loss: 2.59176, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3093, loss: 2.59171, accuracy: 0.12581\n",
            "Epoch: 1/10, step: 3094, loss: 2.59175, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3095, loss: 2.59176, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3096, loss: 2.59178, accuracy: 0.12573\n",
            "Epoch: 1/10, step: 3097, loss: 2.59180, accuracy: 0.12573\n",
            "Epoch: 1/10, step: 3098, loss: 2.59204, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3099, loss: 2.59199, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3100, loss: 2.59195, accuracy: 0.12569\n",
            "Epoch: 1/10, step: 3101, loss: 2.59177, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3102, loss: 2.59177, accuracy: 0.12581\n",
            "Epoch: 1/10, step: 3103, loss: 2.59178, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3104, loss: 2.59182, accuracy: 0.12577\n",
            "Epoch: 1/10, step: 3105, loss: 2.59176, accuracy: 0.12581\n",
            "Epoch: 1/10, step: 3106, loss: 2.59178, accuracy: 0.12576\n",
            "Epoch: 1/10, step: 3107, loss: 2.59178, accuracy: 0.12576\n",
            "Epoch: 1/10, step: 3108, loss: 2.59174, accuracy: 0.12576\n",
            "Epoch: 1/10, step: 3109, loss: 2.59180, accuracy: 0.12572\n",
            "Epoch: 1/10, step: 3110, loss: 2.59167, accuracy: 0.12576\n",
            "Epoch: 1/10, step: 3111, loss: 2.59156, accuracy: 0.12572\n",
            "Epoch: 1/10, step: 3112, loss: 2.59162, accuracy: 0.12572\n",
            "Epoch: 1/10, step: 3113, loss: 2.59146, accuracy: 0.12572\n",
            "Epoch: 1/10, step: 3114, loss: 2.59131, accuracy: 0.12580\n",
            "Epoch: 1/10, step: 3115, loss: 2.59130, accuracy: 0.12580\n",
            "Epoch: 1/10, step: 3116, loss: 2.59122, accuracy: 0.12584\n",
            "Epoch: 1/10, step: 3117, loss: 2.59120, accuracy: 0.12580\n",
            "Epoch: 1/10, step: 3118, loss: 2.59099, accuracy: 0.12588\n",
            "Epoch: 1/10, step: 3119, loss: 2.59104, accuracy: 0.12588\n",
            "Epoch: 1/10, step: 3120, loss: 2.59112, accuracy: 0.12584\n",
            "Epoch: 1/10, step: 3121, loss: 2.59106, accuracy: 0.12588\n",
            "Epoch: 1/10, step: 3122, loss: 2.59093, accuracy: 0.12592\n",
            "Epoch: 1/10, step: 3123, loss: 2.59100, accuracy: 0.12596\n",
            "Epoch: 1/10, step: 3124, loss: 2.59078, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3125, loss: 2.59077, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3126, loss: 2.59090, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3127, loss: 2.59087, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3128, loss: 2.59085, accuracy: 0.12600\n",
            "Epoch: 1/10, step: 3129, loss: 2.59072, accuracy: 0.12608\n",
            "Epoch: 1/10, step: 3130, loss: 2.59100, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3131, loss: 2.59099, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3132, loss: 2.59093, accuracy: 0.12604\n",
            "Epoch: 1/10, step: 3133, loss: 2.59079, accuracy: 0.12612\n",
            "Epoch: 1/10, step: 3134, loss: 2.59083, accuracy: 0.12612\n",
            "Epoch: 1/10, step: 3135, loss: 2.59078, accuracy: 0.12612\n",
            "Epoch: 1/10, step: 3136, loss: 2.59101, accuracy: 0.12612\n",
            "Epoch: 1/10, step: 3137, loss: 2.59108, accuracy: 0.12608\n",
            "Epoch: 1/10, step: 3138, loss: 2.59100, accuracy: 0.12616\n",
            "Epoch: 1/10, step: 3139, loss: 2.59106, accuracy: 0.12612\n",
            "Epoch: 1/10, step: 3140, loss: 2.59090, accuracy: 0.12611\n",
            "Epoch: 1/10, step: 3141, loss: 2.59113, accuracy: 0.12607\n",
            "Epoch: 1/10, step: 3142, loss: 2.59101, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3143, loss: 2.59101, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3144, loss: 2.59104, accuracy: 0.12611\n",
            "Epoch: 1/10, step: 3145, loss: 2.59097, accuracy: 0.12619\n",
            "Epoch: 1/10, step: 3146, loss: 2.59106, accuracy: 0.12615\n",
            "Epoch: 1/10, step: 3147, loss: 2.59099, accuracy: 0.12615\n",
            "Epoch: 1/10, step: 3148, loss: 2.59116, accuracy: 0.12611\n",
            "Epoch: 1/10, step: 3149, loss: 2.59120, accuracy: 0.12607\n",
            "Epoch: 1/10, step: 3150, loss: 2.59124, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3151, loss: 2.59119, accuracy: 0.12599\n",
            "Epoch: 1/10, step: 3152, loss: 2.59120, accuracy: 0.12595\n",
            "Epoch: 1/10, step: 3153, loss: 2.59118, accuracy: 0.12595\n",
            "Epoch: 1/10, step: 3154, loss: 2.59101, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3155, loss: 2.59106, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3156, loss: 2.59111, accuracy: 0.12603\n",
            "Epoch: 1/10, step: 3157, loss: 2.59094, accuracy: 0.12607\n",
            "Epoch: 1/10, step: 3158, loss: 2.59098, accuracy: 0.12607\n",
            "Epoch: 1/10, step: 3159, loss: 2.59115, accuracy: 0.12607\n",
            "Epoch: 1/10, step: 3160, loss: 2.59086, accuracy: 0.12623\n",
            "Epoch: 1/10, step: 3161, loss: 2.59082, accuracy: 0.12627\n",
            "Epoch: 1/10, step: 3162, loss: 2.59074, accuracy: 0.12630\n",
            "Epoch: 1/10, step: 3163, loss: 2.59055, accuracy: 0.12634\n",
            "Epoch: 1/10, step: 3164, loss: 2.59036, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3165, loss: 2.59031, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3166, loss: 2.59014, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3167, loss: 2.59017, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3168, loss: 2.59028, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3169, loss: 2.59039, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3170, loss: 2.59056, accuracy: 0.12638\n",
            "Epoch: 1/10, step: 3171, loss: 2.59042, accuracy: 0.12638\n",
            "Epoch: 1/10, step: 3172, loss: 2.59039, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3173, loss: 2.59043, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3174, loss: 2.59046, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3175, loss: 2.59049, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3176, loss: 2.59039, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3177, loss: 2.59037, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3178, loss: 2.59021, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3179, loss: 2.59031, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3180, loss: 2.58999, accuracy: 0.12657\n",
            "Epoch: 1/10, step: 3181, loss: 2.59011, accuracy: 0.12657\n",
            "Epoch: 1/10, step: 3182, loss: 2.59030, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3183, loss: 2.59054, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3184, loss: 2.59051, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3185, loss: 2.59035, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3186, loss: 2.59033, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3187, loss: 2.59023, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3188, loss: 2.59031, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3189, loss: 2.59027, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3190, loss: 2.59028, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3191, loss: 2.59017, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3192, loss: 2.59016, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3193, loss: 2.59016, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3194, loss: 2.59006, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3195, loss: 2.59001, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3196, loss: 2.59011, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3197, loss: 2.58996, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3198, loss: 2.59009, accuracy: 0.12652\n",
            "Epoch: 1/10, step: 3199, loss: 2.58988, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3200, loss: 2.58999, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3201, loss: 2.59011, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3202, loss: 2.59015, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3203, loss: 2.59003, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3204, loss: 2.58990, accuracy: 0.12668\n",
            "Epoch: 1/10, step: 3205, loss: 2.58992, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3206, loss: 2.58993, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3207, loss: 2.58979, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3208, loss: 2.58970, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3209, loss: 2.58981, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3210, loss: 2.58967, accuracy: 0.12671\n",
            "Epoch: 1/10, step: 3211, loss: 2.58969, accuracy: 0.12671\n",
            "Epoch: 1/10, step: 3212, loss: 2.58965, accuracy: 0.12679\n",
            "Epoch: 1/10, step: 3213, loss: 2.58953, accuracy: 0.12675\n",
            "Epoch: 1/10, step: 3214, loss: 2.58951, accuracy: 0.12671\n",
            "Epoch: 1/10, step: 3215, loss: 2.58975, accuracy: 0.12667\n",
            "Epoch: 1/10, step: 3216, loss: 2.58990, accuracy: 0.12667\n",
            "Epoch: 1/10, step: 3217, loss: 2.58999, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3218, loss: 2.59005, accuracy: 0.12659\n",
            "Epoch: 1/10, step: 3219, loss: 2.58990, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3220, loss: 2.58983, accuracy: 0.12667\n",
            "Epoch: 1/10, step: 3221, loss: 2.58988, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3222, loss: 2.58994, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3223, loss: 2.58997, accuracy: 0.12659\n",
            "Epoch: 1/10, step: 3224, loss: 2.58995, accuracy: 0.12655\n",
            "Epoch: 1/10, step: 3225, loss: 2.59011, accuracy: 0.12651\n",
            "Epoch: 1/10, step: 3226, loss: 2.59013, accuracy: 0.12651\n",
            "Epoch: 1/10, step: 3227, loss: 2.59018, accuracy: 0.12655\n",
            "Epoch: 1/10, step: 3228, loss: 2.59032, accuracy: 0.12655\n",
            "Epoch: 1/10, step: 3229, loss: 2.59045, accuracy: 0.12655\n",
            "Epoch: 1/10, step: 3230, loss: 2.59041, accuracy: 0.12651\n",
            "Epoch: 1/10, step: 3231, loss: 2.59030, accuracy: 0.12659\n",
            "Epoch: 1/10, step: 3232, loss: 2.59035, accuracy: 0.12659\n",
            "Epoch: 1/10, step: 3233, loss: 2.59048, accuracy: 0.12659\n",
            "Epoch: 1/10, step: 3234, loss: 2.59033, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3235, loss: 2.59040, accuracy: 0.12666\n",
            "Epoch: 1/10, step: 3236, loss: 2.59046, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3237, loss: 2.59045, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3238, loss: 2.59062, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3239, loss: 2.59059, accuracy: 0.12658\n",
            "Epoch: 1/10, step: 3240, loss: 2.59062, accuracy: 0.12654\n",
            "Epoch: 1/10, step: 3241, loss: 2.59056, accuracy: 0.12654\n",
            "Epoch: 1/10, step: 3242, loss: 2.59048, accuracy: 0.12650\n",
            "Epoch: 1/10, step: 3243, loss: 2.59027, accuracy: 0.12658\n",
            "Epoch: 1/10, step: 3244, loss: 2.59011, accuracy: 0.12666\n",
            "Epoch: 1/10, step: 3245, loss: 2.59014, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3246, loss: 2.59024, accuracy: 0.12662\n",
            "Epoch: 1/10, step: 3247, loss: 2.59024, accuracy: 0.12658\n",
            "Epoch: 1/10, step: 3248, loss: 2.59021, accuracy: 0.12654\n",
            "Epoch: 1/10, step: 3249, loss: 2.59026, accuracy: 0.12654\n",
            "Epoch: 1/10, step: 3250, loss: 2.59019, accuracy: 0.12654\n",
            "Epoch: 1/10, step: 3251, loss: 2.59016, accuracy: 0.12650\n",
            "Epoch: 1/10, step: 3252, loss: 2.58995, accuracy: 0.12661\n",
            "Epoch: 1/10, step: 3253, loss: 2.59002, accuracy: 0.12661\n",
            "Epoch: 1/10, step: 3254, loss: 2.59014, accuracy: 0.12657\n",
            "Epoch: 1/10, step: 3255, loss: 2.59000, accuracy: 0.12661\n",
            "Epoch: 1/10, step: 3256, loss: 2.59001, accuracy: 0.12665\n",
            "Epoch: 1/10, step: 3257, loss: 2.59007, accuracy: 0.12669\n",
            "Epoch: 1/10, step: 3258, loss: 2.59015, accuracy: 0.12665\n",
            "Epoch: 1/10, step: 3259, loss: 2.59027, accuracy: 0.12661\n",
            "Epoch: 1/10, step: 3260, loss: 2.59011, accuracy: 0.12665\n",
            "Epoch: 1/10, step: 3261, loss: 2.59026, accuracy: 0.12661\n",
            "Epoch: 1/10, step: 3262, loss: 2.59031, accuracy: 0.12657\n",
            "Epoch: 1/10, step: 3263, loss: 2.59018, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3264, loss: 2.59025, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3265, loss: 2.59016, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3266, loss: 2.59021, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3267, loss: 2.59018, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3268, loss: 2.59019, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3269, loss: 2.59008, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3270, loss: 2.58999, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3271, loss: 2.59004, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3272, loss: 2.58996, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3273, loss: 2.58987, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3274, loss: 2.58997, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3275, loss: 2.59015, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3276, loss: 2.59012, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3277, loss: 2.59009, accuracy: 0.12637\n",
            "Epoch: 1/10, step: 3278, loss: 2.58993, accuracy: 0.12637\n",
            "Epoch: 1/10, step: 3279, loss: 2.58992, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3280, loss: 2.58988, accuracy: 0.12641\n",
            "Epoch: 1/10, step: 3281, loss: 2.58996, accuracy: 0.12637\n",
            "Epoch: 1/10, step: 3282, loss: 2.58990, accuracy: 0.12637\n",
            "Epoch: 1/10, step: 3283, loss: 2.58991, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3284, loss: 2.58989, accuracy: 0.12629\n",
            "Epoch: 1/10, step: 3285, loss: 2.59013, accuracy: 0.12626\n",
            "Epoch: 1/10, step: 3286, loss: 2.59006, accuracy: 0.12622\n",
            "Epoch: 1/10, step: 3287, loss: 2.58999, accuracy: 0.12625\n",
            "Epoch: 1/10, step: 3288, loss: 2.58990, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3289, loss: 2.58988, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3290, loss: 2.58975, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3291, loss: 2.58981, accuracy: 0.12629\n",
            "Epoch: 1/10, step: 3292, loss: 2.58999, accuracy: 0.12625\n",
            "Epoch: 1/10, step: 3293, loss: 2.59000, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3294, loss: 2.59003, accuracy: 0.12633\n",
            "Epoch: 1/10, step: 3295, loss: 2.59006, accuracy: 0.12629\n",
            "Epoch: 1/10, step: 3296, loss: 2.59011, accuracy: 0.12625\n",
            "Epoch: 1/10, step: 3297, loss: 2.59023, accuracy: 0.12621\n",
            "Epoch: 1/10, step: 3298, loss: 2.59028, accuracy: 0.12621\n",
            "Epoch: 1/10, step: 3299, loss: 2.59024, accuracy: 0.12617\n",
            "Epoch: 1/10, step: 3300, loss: 2.59009, accuracy: 0.12621\n",
            "Epoch: 1/10, step: 3301, loss: 2.59011, accuracy: 0.12621\n",
            "Epoch: 1/10, step: 3302, loss: 2.59022, accuracy: 0.12617\n",
            "Epoch: 1/10, step: 3303, loss: 2.59024, accuracy: 0.12617\n",
            "Epoch: 1/10, step: 3304, loss: 2.59045, accuracy: 0.12617\n",
            "Epoch: 1/10, step: 3305, loss: 2.59025, accuracy: 0.12625\n",
            "Epoch: 1/10, step: 3306, loss: 2.59018, accuracy: 0.12629\n",
            "Epoch: 1/10, step: 3307, loss: 2.59003, accuracy: 0.12632\n",
            "Epoch: 1/10, step: 3308, loss: 2.59005, accuracy: 0.12632\n",
            "Epoch: 1/10, step: 3309, loss: 2.59001, accuracy: 0.12632\n",
            "Epoch: 1/10, step: 3310, loss: 2.58989, accuracy: 0.12636\n",
            "Epoch: 1/10, step: 3311, loss: 2.58978, accuracy: 0.12643\n",
            "Epoch: 1/10, step: 3312, loss: 2.58986, accuracy: 0.12640\n",
            "Epoch: 1/10, step: 3313, loss: 2.58987, accuracy: 0.12640\n",
            "Epoch: 1/10, step: 3314, loss: 2.59005, accuracy: 0.12636\n",
            "Epoch: 1/10, step: 3315, loss: 2.59001, accuracy: 0.12636\n",
            "Epoch: 1/10, step: 3316, loss: 2.58997, accuracy: 0.12632\n",
            "Epoch: 1/10, step: 3317, loss: 2.58998, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3318, loss: 2.59005, accuracy: 0.12624\n",
            "Epoch: 1/10, step: 3319, loss: 2.59000, accuracy: 0.12632\n",
            "Epoch: 1/10, step: 3320, loss: 2.59002, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3321, loss: 2.59005, accuracy: 0.12624\n",
            "Epoch: 1/10, step: 3322, loss: 2.59000, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3323, loss: 2.59002, accuracy: 0.12624\n",
            "Epoch: 1/10, step: 3324, loss: 2.59001, accuracy: 0.12624\n",
            "Epoch: 1/10, step: 3325, loss: 2.58983, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3326, loss: 2.58981, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3327, loss: 2.58972, accuracy: 0.12631\n",
            "Epoch: 1/10, step: 3328, loss: 2.58967, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3329, loss: 2.58959, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3330, loss: 2.58955, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3331, loss: 2.58945, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3332, loss: 2.58950, accuracy: 0.12631\n",
            "Epoch: 1/10, step: 3333, loss: 2.58961, accuracy: 0.12628\n",
            "Epoch: 1/10, step: 3334, loss: 2.58958, accuracy: 0.12631\n",
            "Epoch: 1/10, step: 3335, loss: 2.58949, accuracy: 0.12631\n",
            "Epoch: 1/10, step: 3336, loss: 2.58935, accuracy: 0.12639\n",
            "Epoch: 1/10, step: 3337, loss: 2.58928, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3338, loss: 2.58918, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3339, loss: 2.58916, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3340, loss: 2.58919, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3341, loss: 2.58931, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3342, loss: 2.58918, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3343, loss: 2.58910, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3344, loss: 2.58922, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3345, loss: 2.58924, accuracy: 0.12642\n",
            "Epoch: 1/10, step: 3346, loss: 2.58926, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3347, loss: 2.58920, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3348, loss: 2.58911, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3349, loss: 2.58902, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3350, loss: 2.58917, accuracy: 0.12646\n",
            "Epoch: 1/10, step: 3351, loss: 2.58908, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3352, loss: 2.58904, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3353, loss: 2.58905, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3354, loss: 2.58902, accuracy: 0.12653\n",
            "Epoch: 1/10, step: 3355, loss: 2.58907, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3356, loss: 2.58915, accuracy: 0.12645\n",
            "Epoch: 1/10, step: 3357, loss: 2.58921, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3358, loss: 2.58910, accuracy: 0.12649\n",
            "Epoch: 1/10, step: 3359, loss: 2.58885, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3360, loss: 2.58886, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3361, loss: 2.58876, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3362, loss: 2.58874, accuracy: 0.12652\n",
            "Epoch: 1/10, step: 3363, loss: 2.58858, accuracy: 0.12664\n",
            "Epoch: 1/10, step: 3364, loss: 2.58852, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3365, loss: 2.58841, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3366, loss: 2.58854, accuracy: 0.12660\n",
            "Epoch: 1/10, step: 3367, loss: 2.58850, accuracy: 0.12656\n",
            "Epoch: 1/10, step: 3368, loss: 2.58834, accuracy: 0.12663\n",
            "Epoch: 1/10, step: 3369, loss: 2.58824, accuracy: 0.12667\n",
            "Epoch: 1/10, step: 3370, loss: 2.58829, accuracy: 0.12671\n",
            "Epoch: 1/10, step: 3371, loss: 2.58827, accuracy: 0.12667\n",
            "Epoch: 1/10, step: 3372, loss: 2.58814, accuracy: 0.12671\n",
            "Epoch: 1/10, step: 3373, loss: 2.58817, accuracy: 0.12670\n",
            "Epoch: 1/10, step: 3374, loss: 2.58802, accuracy: 0.12674\n",
            "Epoch: 1/10, step: 3375, loss: 2.58791, accuracy: 0.12674\n",
            "Epoch: 1/10, step: 3376, loss: 2.58785, accuracy: 0.12674\n",
            "Epoch: 1/10, step: 3377, loss: 2.58785, accuracy: 0.12670\n",
            "Epoch: 1/10, step: 3378, loss: 2.58771, accuracy: 0.12678\n",
            "Epoch: 1/10, step: 3379, loss: 2.58782, accuracy: 0.12678\n",
            "Epoch: 1/10, step: 3380, loss: 2.58796, accuracy: 0.12674\n",
            "Epoch: 1/10, step: 3381, loss: 2.58788, accuracy: 0.12677\n",
            "Epoch: 1/10, step: 3382, loss: 2.58788, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3383, loss: 2.58789, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3384, loss: 2.58777, accuracy: 0.12685\n",
            "Epoch: 1/10, step: 3385, loss: 2.58757, accuracy: 0.12685\n",
            "Epoch: 1/10, step: 3386, loss: 2.58749, accuracy: 0.12685\n",
            "Epoch: 1/10, step: 3387, loss: 2.58744, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3388, loss: 2.58752, accuracy: 0.12677\n",
            "Epoch: 1/10, step: 3389, loss: 2.58744, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3390, loss: 2.58726, accuracy: 0.12684\n",
            "Epoch: 1/10, step: 3391, loss: 2.58728, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3392, loss: 2.58720, accuracy: 0.12684\n",
            "Epoch: 1/10, step: 3393, loss: 2.58721, accuracy: 0.12681\n",
            "Epoch: 1/10, step: 3394, loss: 2.58727, accuracy: 0.12684\n",
            "Epoch: 1/10, step: 3395, loss: 2.58714, accuracy: 0.12688\n",
            "Epoch: 1/10, step: 3396, loss: 2.58714, accuracy: 0.12688\n",
            "Epoch: 1/10, step: 3397, loss: 2.58718, accuracy: 0.12684\n",
            "Epoch: 1/10, step: 3398, loss: 2.58716, accuracy: 0.12684\n",
            "Epoch: 1/10, step: 3399, loss: 2.58721, accuracy: 0.12688\n",
            "Epoch: 1/10, step: 3400, loss: 2.58704, accuracy: 0.12691\n",
            "Epoch: 1/10, step: 3401, loss: 2.58690, accuracy: 0.12695\n",
            "Epoch: 1/10, step: 3402, loss: 2.58678, accuracy: 0.12698\n",
            "Epoch: 1/10, step: 3403, loss: 2.58676, accuracy: 0.12695\n",
            "Epoch: 1/10, step: 3404, loss: 2.58667, accuracy: 0.12695\n",
            "Epoch: 1/10, step: 3405, loss: 2.58670, accuracy: 0.12691\n",
            "Epoch: 1/10, step: 3406, loss: 2.58664, accuracy: 0.12691\n",
            "Epoch: 1/10, step: 3407, loss: 2.58651, accuracy: 0.12691\n",
            "Epoch: 1/10, step: 3408, loss: 2.58653, accuracy: 0.12694\n",
            "Epoch: 1/10, step: 3409, loss: 2.58636, accuracy: 0.12698\n",
            "Epoch: 1/10, step: 3410, loss: 2.58628, accuracy: 0.12702\n",
            "Epoch: 1/10, step: 3411, loss: 2.58633, accuracy: 0.12702\n",
            "Epoch: 1/10, step: 3412, loss: 2.58622, accuracy: 0.12701\n",
            "Epoch: 1/10, step: 3413, loss: 2.58625, accuracy: 0.12701\n",
            "Epoch: 1/10, step: 3414, loss: 2.58631, accuracy: 0.12698\n",
            "Epoch: 1/10, step: 3415, loss: 2.58616, accuracy: 0.12705\n",
            "Epoch: 1/10, step: 3416, loss: 2.58607, accuracy: 0.12709\n",
            "Epoch: 1/10, step: 3417, loss: 2.58630, accuracy: 0.12709\n",
            "Epoch: 1/10, step: 3418, loss: 2.58625, accuracy: 0.12708\n",
            "Epoch: 1/10, step: 3419, loss: 2.58604, accuracy: 0.12712\n",
            "Epoch: 1/10, step: 3420, loss: 2.58585, accuracy: 0.12716\n",
            "Epoch: 1/10, step: 3421, loss: 2.58584, accuracy: 0.12716\n",
            "Epoch: 1/10, step: 3422, loss: 2.58574, accuracy: 0.12719\n",
            "Epoch: 1/10, step: 3423, loss: 2.58540, accuracy: 0.12734\n",
            "Epoch: 1/10, step: 3424, loss: 2.58536, accuracy: 0.12734\n",
            "Epoch: 1/10, step: 3425, loss: 2.58540, accuracy: 0.12734\n",
            "Epoch: 1/10, step: 3426, loss: 2.58533, accuracy: 0.12741\n",
            "Epoch: 1/10, step: 3427, loss: 2.58528, accuracy: 0.12741\n",
            "Epoch: 1/10, step: 3428, loss: 2.58525, accuracy: 0.12748\n",
            "Epoch: 1/10, step: 3429, loss: 2.58535, accuracy: 0.12748\n",
            "Epoch: 1/10, step: 3430, loss: 2.58531, accuracy: 0.12748\n",
            "Epoch: 1/10, step: 3431, loss: 2.58541, accuracy: 0.12744\n",
            "Epoch: 1/10, step: 3432, loss: 2.58535, accuracy: 0.12751\n",
            "Epoch: 1/10, step: 3433, loss: 2.58536, accuracy: 0.12751\n",
            "Epoch: 1/10, step: 3434, loss: 2.58525, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3435, loss: 2.58525, accuracy: 0.12758\n",
            "Epoch: 1/10, step: 3436, loss: 2.58534, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3437, loss: 2.58530, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3438, loss: 2.58522, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3439, loss: 2.58532, accuracy: 0.12754\n",
            "Epoch: 1/10, step: 3440, loss: 2.58538, accuracy: 0.12751\n",
            "Epoch: 1/10, step: 3441, loss: 2.58555, accuracy: 0.12747\n",
            "Epoch: 1/10, step: 3442, loss: 2.58569, accuracy: 0.12743\n",
            "Epoch: 1/10, step: 3443, loss: 2.58566, accuracy: 0.12743\n",
            "Epoch: 1/10, step: 3444, loss: 2.58559, accuracy: 0.12740\n",
            "Epoch: 1/10, step: 3445, loss: 2.58557, accuracy: 0.12743\n",
            "Epoch: 1/10, step: 3446, loss: 2.58562, accuracy: 0.12743\n",
            "Epoch: 1/10, step: 3447, loss: 2.58576, accuracy: 0.12743\n",
            "Epoch: 1/10, step: 3448, loss: 2.58580, accuracy: 0.12739\n",
            "Epoch: 1/10, step: 3449, loss: 2.58584, accuracy: 0.12736\n",
            "Epoch: 1/10, step: 3450, loss: 2.58591, accuracy: 0.12736\n",
            "Epoch: 1/10, step: 3451, loss: 2.58582, accuracy: 0.12735\n",
            "Epoch: 1/10, step: 3452, loss: 2.58585, accuracy: 0.12735\n",
            "Epoch: 1/10, step: 3453, loss: 2.58587, accuracy: 0.12732\n",
            "Epoch: 1/10, step: 3454, loss: 2.58576, accuracy: 0.12732\n",
            "Epoch: 1/10, step: 3455, loss: 2.58584, accuracy: 0.12728\n",
            "Epoch: 1/10, step: 3456, loss: 2.58591, accuracy: 0.12724\n",
            "Epoch: 1/10, step: 3457, loss: 2.58577, accuracy: 0.12731\n",
            "Epoch: 1/10, step: 3458, loss: 2.58560, accuracy: 0.12742\n",
            "Epoch: 1/10, step: 3459, loss: 2.58548, accuracy: 0.12746\n",
            "Epoch: 1/10, step: 3460, loss: 2.58543, accuracy: 0.12746\n",
            "Epoch: 1/10, step: 3461, loss: 2.58553, accuracy: 0.12742\n",
            "Epoch: 1/10, step: 3462, loss: 2.58531, accuracy: 0.12753\n",
            "Epoch: 1/10, step: 3463, loss: 2.58533, accuracy: 0.12749\n",
            "Epoch: 1/10, step: 3464, loss: 2.58542, accuracy: 0.12745\n",
            "Epoch: 1/10, step: 3465, loss: 2.58529, accuracy: 0.12753\n",
            "Epoch: 1/10, step: 3466, loss: 2.58540, accuracy: 0.12756\n",
            "Epoch: 1/10, step: 3467, loss: 2.58531, accuracy: 0.12763\n",
            "Epoch: 1/10, step: 3468, loss: 2.58530, accuracy: 0.12767\n",
            "Epoch: 1/10, step: 3469, loss: 2.58534, accuracy: 0.12767\n",
            "Epoch: 1/10, step: 3470, loss: 2.58528, accuracy: 0.12763\n",
            "Epoch: 1/10, step: 3471, loss: 2.58534, accuracy: 0.12763\n",
            "Epoch: 1/10, step: 3472, loss: 2.58524, accuracy: 0.12766\n",
            "Epoch: 1/10, step: 3473, loss: 2.58527, accuracy: 0.12763\n",
            "Epoch: 1/10, step: 3474, loss: 2.58549, accuracy: 0.12763\n",
            "Epoch: 1/10, step: 3475, loss: 2.58543, accuracy: 0.12759\n",
            "Epoch: 1/10, step: 3476, loss: 2.58544, accuracy: 0.12759\n",
            "Epoch: 1/10, step: 3477, loss: 2.58542, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3478, loss: 2.58546, accuracy: 0.12755\n",
            "Epoch: 1/10, step: 3479, loss: 2.58540, accuracy: 0.12759\n",
            "Epoch: 1/10, step: 3480, loss: 2.58533, accuracy: 0.12759\n",
            "Epoch: 1/10, step: 3481, loss: 2.58527, accuracy: 0.12766\n",
            "Epoch: 1/10, step: 3482, loss: 2.58539, accuracy: 0.12762\n",
            "Epoch: 1/10, step: 3483, loss: 2.58531, accuracy: 0.12766\n",
            "Epoch: 1/10, step: 3484, loss: 2.58543, accuracy: 0.12765\n",
            "Epoch: 1/10, step: 3485, loss: 2.58549, accuracy: 0.12769\n",
            "Epoch: 1/10, step: 3486, loss: 2.58563, accuracy: 0.12765\n",
            "Epoch: 1/10, step: 3487, loss: 2.58562, accuracy: 0.12762\n",
            "Epoch: 1/10, step: 3488, loss: 2.58554, accuracy: 0.12762\n",
            "Epoch: 1/10, step: 3489, loss: 2.58554, accuracy: 0.12762\n",
            "Epoch: 1/10, step: 3490, loss: 2.58551, accuracy: 0.12758\n",
            "Epoch: 1/10, step: 3491, loss: 2.58554, accuracy: 0.12758\n",
            "Epoch: 1/10, step: 3492, loss: 2.58535, accuracy: 0.12758\n",
            "Epoch: 1/10, step: 3493, loss: 2.58510, accuracy: 0.12765\n",
            "Epoch: 1/10, step: 3494, loss: 2.58509, accuracy: 0.12765\n",
            "Epoch: 1/10, step: 3495, loss: 2.58497, accuracy: 0.12768\n",
            "Epoch: 1/10, step: 3496, loss: 2.58480, accuracy: 0.12768\n",
            "Epoch: 1/10, step: 3497, loss: 2.58472, accuracy: 0.12772\n",
            "Epoch: 1/10, step: 3498, loss: 2.58461, accuracy: 0.12779\n",
            "Epoch: 1/10, step: 3499, loss: 2.58471, accuracy: 0.12779\n",
            "Epoch: 1/10, step: 3500, loss: 2.58459, accuracy: 0.12782\n",
            "Epoch: 1/10, step: 3501, loss: 2.58455, accuracy: 0.12786\n",
            "Epoch: 1/10, step: 3502, loss: 2.58445, accuracy: 0.12793\n",
            "Epoch: 1/10, step: 3503, loss: 2.58434, accuracy: 0.12793\n",
            "Epoch: 1/10, step: 3504, loss: 2.58419, accuracy: 0.12793\n",
            "Epoch: 1/10, step: 3505, loss: 2.58406, accuracy: 0.12789\n",
            "Epoch: 1/10, step: 3506, loss: 2.58399, accuracy: 0.12789\n",
            "Epoch: 1/10, step: 3507, loss: 2.58392, accuracy: 0.12792\n",
            "Epoch: 1/10, step: 3508, loss: 2.58374, accuracy: 0.12799\n",
            "Epoch: 1/10, step: 3509, loss: 2.58356, accuracy: 0.12806\n",
            "Epoch: 1/10, step: 3510, loss: 2.58356, accuracy: 0.12810\n",
            "Epoch: 1/10, step: 3511, loss: 2.58358, accuracy: 0.12810\n",
            "Epoch: 1/10, step: 3512, loss: 2.58350, accuracy: 0.12813\n",
            "Epoch: 1/10, step: 3513, loss: 2.58351, accuracy: 0.12817\n",
            "Epoch: 1/10, step: 3514, loss: 2.58350, accuracy: 0.12817\n",
            "Epoch: 1/10, step: 3515, loss: 2.58361, accuracy: 0.12817\n",
            "Epoch: 1/10, step: 3516, loss: 2.58361, accuracy: 0.12820\n",
            "Epoch: 1/10, step: 3517, loss: 2.58367, accuracy: 0.12820\n",
            "Epoch: 1/10, step: 3518, loss: 2.58347, accuracy: 0.12823\n",
            "Epoch: 1/10, step: 3519, loss: 2.58327, accuracy: 0.12827\n",
            "Epoch: 1/10, step: 3520, loss: 2.58304, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3521, loss: 2.58298, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3522, loss: 2.58304, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3523, loss: 2.58315, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3524, loss: 2.58332, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3525, loss: 2.58338, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3526, loss: 2.58324, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3527, loss: 2.58327, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3528, loss: 2.58338, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3529, loss: 2.58330, accuracy: 0.12844\n",
            "Epoch: 1/10, step: 3530, loss: 2.58321, accuracy: 0.12847\n",
            "Epoch: 1/10, step: 3531, loss: 2.58298, accuracy: 0.12858\n",
            "Epoch: 1/10, step: 3532, loss: 2.58311, accuracy: 0.12854\n",
            "Epoch: 1/10, step: 3533, loss: 2.58309, accuracy: 0.12850\n",
            "Epoch: 1/10, step: 3534, loss: 2.58306, accuracy: 0.12847\n",
            "Epoch: 1/10, step: 3535, loss: 2.58297, accuracy: 0.12847\n",
            "Epoch: 1/10, step: 3536, loss: 2.58297, accuracy: 0.12846\n",
            "Epoch: 1/10, step: 3537, loss: 2.58287, accuracy: 0.12853\n",
            "Epoch: 1/10, step: 3538, loss: 2.58286, accuracy: 0.12850\n",
            "Epoch: 1/10, step: 3539, loss: 2.58271, accuracy: 0.12853\n",
            "Epoch: 1/10, step: 3540, loss: 2.58277, accuracy: 0.12853\n",
            "Epoch: 1/10, step: 3541, loss: 2.58258, accuracy: 0.12853\n",
            "Epoch: 1/10, step: 3542, loss: 2.58253, accuracy: 0.12853\n",
            "Epoch: 1/10, step: 3543, loss: 2.58235, accuracy: 0.12860\n",
            "Epoch: 1/10, step: 3544, loss: 2.58237, accuracy: 0.12863\n",
            "Epoch: 1/10, step: 3545, loss: 2.58238, accuracy: 0.12860\n",
            "Epoch: 1/10, step: 3546, loss: 2.58233, accuracy: 0.12860\n",
            "Epoch: 1/10, step: 3547, loss: 2.58217, accuracy: 0.12863\n",
            "Epoch: 1/10, step: 3548, loss: 2.58206, accuracy: 0.12863\n",
            "Epoch: 1/10, step: 3549, loss: 2.58204, accuracy: 0.12859\n",
            "Epoch: 1/10, step: 3550, loss: 2.58213, accuracy: 0.12856\n",
            "Epoch: 1/10, step: 3551, loss: 2.58210, accuracy: 0.12852\n",
            "Epoch: 1/10, step: 3552, loss: 2.58213, accuracy: 0.12848\n",
            "Epoch: 1/10, step: 3553, loss: 2.58225, accuracy: 0.12848\n",
            "Epoch: 1/10, step: 3554, loss: 2.58230, accuracy: 0.12845\n",
            "Epoch: 1/10, step: 3555, loss: 2.58236, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3556, loss: 2.58237, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3557, loss: 2.58229, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3558, loss: 2.58228, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3559, loss: 2.58228, accuracy: 0.12841\n",
            "Epoch: 1/10, step: 3560, loss: 2.58223, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3561, loss: 2.58216, accuracy: 0.12844\n",
            "Epoch: 1/10, step: 3562, loss: 2.58201, accuracy: 0.12844\n",
            "Epoch: 1/10, step: 3563, loss: 2.58209, accuracy: 0.12840\n",
            "Epoch: 1/10, step: 3564, loss: 2.58197, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3565, loss: 2.58188, accuracy: 0.12840\n",
            "Epoch: 1/10, step: 3566, loss: 2.58211, accuracy: 0.12837\n",
            "Epoch: 1/10, step: 3567, loss: 2.58198, accuracy: 0.12840\n",
            "Epoch: 1/10, step: 3568, loss: 2.58199, accuracy: 0.12840\n",
            "Epoch: 1/10, step: 3569, loss: 2.58200, accuracy: 0.12847\n",
            "Epoch: 1/10, step: 3570, loss: 2.58198, accuracy: 0.12843\n",
            "Epoch: 1/10, step: 3571, loss: 2.58215, accuracy: 0.12840\n",
            "Epoch: 1/10, step: 3572, loss: 2.58211, accuracy: 0.12843\n",
            "Epoch: 1/10, step: 3573, loss: 2.58203, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3574, loss: 2.58214, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3575, loss: 2.58227, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3576, loss: 2.58237, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3577, loss: 2.58236, accuracy: 0.12835\n",
            "Epoch: 1/10, step: 3578, loss: 2.58236, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3579, loss: 2.58229, accuracy: 0.12842\n",
            "Epoch: 1/10, step: 3580, loss: 2.58228, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3581, loss: 2.58238, accuracy: 0.12839\n",
            "Epoch: 1/10, step: 3582, loss: 2.58239, accuracy: 0.12838\n",
            "Epoch: 1/10, step: 3583, loss: 2.58222, accuracy: 0.12845\n",
            "Epoch: 1/10, step: 3584, loss: 2.58215, accuracy: 0.12845\n",
            "Epoch: 1/10, step: 3585, loss: 2.58249, accuracy: 0.12845\n",
            "Epoch: 1/10, step: 3586, loss: 2.58267, accuracy: 0.12845\n",
            "Epoch: 1/10, step: 3587, loss: 2.58269, accuracy: 0.12852\n",
            "Epoch: 1/10, step: 3588, loss: 2.58268, accuracy: 0.12855\n",
            "Epoch: 1/10, step: 3589, loss: 2.58264, accuracy: 0.12855\n",
            "Epoch: 1/10, step: 3590, loss: 2.58259, accuracy: 0.12855\n",
            "Epoch: 1/10, step: 3591, loss: 2.58253, accuracy: 0.12859\n",
            "Epoch: 1/10, step: 3592, loss: 2.58252, accuracy: 0.12858\n",
            "Epoch: 1/10, step: 3593, loss: 2.58247, accuracy: 0.12855\n",
            "Epoch: 1/10, step: 3594, loss: 2.58237, accuracy: 0.12858\n",
            "Epoch: 1/10, step: 3595, loss: 2.58229, accuracy: 0.12862\n",
            "Epoch: 1/10, step: 3596, loss: 2.58225, accuracy: 0.12862\n",
            "Epoch: 1/10, step: 3597, loss: 2.58219, accuracy: 0.12861\n",
            "Epoch: 1/10, step: 3598, loss: 2.58220, accuracy: 0.12861\n",
            "Epoch: 1/10, step: 3599, loss: 2.58199, accuracy: 0.12872\n",
            "Epoch: 1/10, step: 3600, loss: 2.58201, accuracy: 0.12872\n",
            "Epoch: 1/10, step: 3601, loss: 2.58191, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3602, loss: 2.58178, accuracy: 0.12875\n",
            "Epoch: 1/10, step: 3603, loss: 2.58190, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3604, loss: 2.58182, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3605, loss: 2.58173, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3606, loss: 2.58173, accuracy: 0.12874\n",
            "Epoch: 1/10, step: 3607, loss: 2.58161, accuracy: 0.12874\n",
            "Epoch: 1/10, step: 3608, loss: 2.58158, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3609, loss: 2.58154, accuracy: 0.12871\n",
            "Epoch: 1/10, step: 3610, loss: 2.58154, accuracy: 0.12870\n",
            "Epoch: 1/10, step: 3611, loss: 2.58146, accuracy: 0.12877\n",
            "Epoch: 1/10, step: 3612, loss: 2.58147, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3613, loss: 2.58140, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3614, loss: 2.58137, accuracy: 0.12880\n",
            "Epoch: 1/10, step: 3615, loss: 2.58132, accuracy: 0.12877\n",
            "Epoch: 1/10, step: 3616, loss: 2.58150, accuracy: 0.12873\n",
            "Epoch: 1/10, step: 3617, loss: 2.58139, accuracy: 0.12870\n",
            "Epoch: 1/10, step: 3618, loss: 2.58133, accuracy: 0.12870\n",
            "Epoch: 1/10, step: 3619, loss: 2.58133, accuracy: 0.12873\n",
            "Epoch: 1/10, step: 3620, loss: 2.58122, accuracy: 0.12880\n",
            "Epoch: 1/10, step: 3621, loss: 2.58100, accuracy: 0.12883\n",
            "Epoch: 1/10, step: 3622, loss: 2.58105, accuracy: 0.12880\n",
            "Epoch: 1/10, step: 3623, loss: 2.58094, accuracy: 0.12883\n",
            "Epoch: 1/10, step: 3624, loss: 2.58080, accuracy: 0.12883\n",
            "Epoch: 1/10, step: 3625, loss: 2.58082, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3626, loss: 2.58065, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3627, loss: 2.58064, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3628, loss: 2.58068, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3629, loss: 2.58063, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3630, loss: 2.58079, accuracy: 0.12875\n",
            "Epoch: 1/10, step: 3631, loss: 2.58073, accuracy: 0.12879\n",
            "Epoch: 1/10, step: 3632, loss: 2.58074, accuracy: 0.12875\n",
            "Epoch: 1/10, step: 3633, loss: 2.58054, accuracy: 0.12878\n",
            "Epoch: 1/10, step: 3634, loss: 2.58039, accuracy: 0.12882\n",
            "Epoch: 1/10, step: 3635, loss: 2.58032, accuracy: 0.12882\n",
            "Epoch: 1/10, step: 3636, loss: 2.58013, accuracy: 0.12888\n",
            "Epoch: 1/10, step: 3637, loss: 2.58002, accuracy: 0.12888\n",
            "Epoch: 1/10, step: 3638, loss: 2.57996, accuracy: 0.12885\n",
            "Epoch: 1/10, step: 3639, loss: 2.57999, accuracy: 0.12885\n",
            "Epoch: 1/10, step: 3640, loss: 2.57999, accuracy: 0.12885\n",
            "Epoch: 1/10, step: 3641, loss: 2.57999, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3642, loss: 2.57995, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3643, loss: 2.57994, accuracy: 0.12877\n",
            "Epoch: 1/10, step: 3644, loss: 2.57986, accuracy: 0.12877\n",
            "Epoch: 1/10, step: 3645, loss: 2.57981, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3646, loss: 2.57980, accuracy: 0.12881\n",
            "Epoch: 1/10, step: 3647, loss: 2.57978, accuracy: 0.12880\n",
            "Epoch: 1/10, step: 3648, loss: 2.57971, accuracy: 0.12884\n",
            "Epoch: 1/10, step: 3649, loss: 2.57957, accuracy: 0.12887\n",
            "Epoch: 1/10, step: 3650, loss: 2.57972, accuracy: 0.12884\n",
            "Epoch: 1/10, step: 3651, loss: 2.57968, accuracy: 0.12887\n",
            "Epoch: 1/10, step: 3652, loss: 2.57956, accuracy: 0.12890\n",
            "Epoch: 1/10, step: 3653, loss: 2.57955, accuracy: 0.12890\n",
            "Epoch: 1/10, step: 3654, loss: 2.57941, accuracy: 0.12893\n",
            "Epoch: 1/10, step: 3655, loss: 2.57938, accuracy: 0.12897\n",
            "Epoch: 1/10, step: 3656, loss: 2.57933, accuracy: 0.12897\n",
            "Epoch: 1/10, step: 3657, loss: 2.57925, accuracy: 0.12897\n",
            "Epoch: 1/10, step: 3658, loss: 2.57929, accuracy: 0.12900\n",
            "Epoch: 1/10, step: 3659, loss: 2.57920, accuracy: 0.12903\n",
            "Epoch: 1/10, step: 3660, loss: 2.57904, accuracy: 0.12910\n",
            "Epoch: 1/10, step: 3661, loss: 2.57898, accuracy: 0.12913\n",
            "Epoch: 1/10, step: 3662, loss: 2.57900, accuracy: 0.12913\n",
            "Epoch: 1/10, step: 3663, loss: 2.57889, accuracy: 0.12910\n",
            "Epoch: 1/10, step: 3664, loss: 2.57875, accuracy: 0.12909\n",
            "Epoch: 1/10, step: 3665, loss: 2.57869, accuracy: 0.12906\n",
            "Epoch: 1/10, step: 3666, loss: 2.57865, accuracy: 0.12909\n",
            "Epoch: 1/10, step: 3667, loss: 2.57875, accuracy: 0.12906\n",
            "Epoch: 1/10, step: 3668, loss: 2.57880, accuracy: 0.12902\n",
            "Epoch: 1/10, step: 3669, loss: 2.57889, accuracy: 0.12899\n",
            "Epoch: 1/10, step: 3670, loss: 2.57877, accuracy: 0.12899\n",
            "Epoch: 1/10, step: 3671, loss: 2.57872, accuracy: 0.12902\n",
            "Epoch: 1/10, step: 3672, loss: 2.57872, accuracy: 0.12898\n",
            "Epoch: 1/10, step: 3673, loss: 2.57884, accuracy: 0.12895\n",
            "Epoch: 1/10, step: 3674, loss: 2.57884, accuracy: 0.12895\n",
            "Epoch: 1/10, step: 3675, loss: 2.57886, accuracy: 0.12895\n",
            "Epoch: 1/10, step: 3676, loss: 2.57883, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3677, loss: 2.57873, accuracy: 0.12898\n",
            "Epoch: 1/10, step: 3678, loss: 2.57876, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3679, loss: 2.57877, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3680, loss: 2.57875, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3681, loss: 2.57874, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3682, loss: 2.57859, accuracy: 0.12894\n",
            "Epoch: 1/10, step: 3683, loss: 2.57851, accuracy: 0.12900\n",
            "Epoch: 1/10, step: 3684, loss: 2.57854, accuracy: 0.12900\n",
            "Epoch: 1/10, step: 3685, loss: 2.57845, accuracy: 0.12904\n",
            "Epoch: 1/10, step: 3686, loss: 2.57851, accuracy: 0.12904\n",
            "Epoch: 1/10, step: 3687, loss: 2.57829, accuracy: 0.12917\n",
            "Epoch: 1/10, step: 3688, loss: 2.57838, accuracy: 0.12914\n",
            "Epoch: 1/10, step: 3689, loss: 2.57846, accuracy: 0.12913\n",
            "Epoch: 1/10, step: 3690, loss: 2.57829, accuracy: 0.12920\n",
            "Epoch: 1/10, step: 3691, loss: 2.57823, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3692, loss: 2.57818, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3693, loss: 2.57799, accuracy: 0.12926\n",
            "Epoch: 1/10, step: 3694, loss: 2.57796, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3695, loss: 2.57787, accuracy: 0.12926\n",
            "Epoch: 1/10, step: 3696, loss: 2.57798, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3697, loss: 2.57799, accuracy: 0.12926\n",
            "Epoch: 1/10, step: 3698, loss: 2.57781, accuracy: 0.12933\n",
            "Epoch: 1/10, step: 3699, loss: 2.57780, accuracy: 0.12929\n",
            "Epoch: 1/10, step: 3700, loss: 2.57780, accuracy: 0.12929\n",
            "Epoch: 1/10, step: 3701, loss: 2.57774, accuracy: 0.12929\n",
            "Epoch: 1/10, step: 3702, loss: 2.57772, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3703, loss: 2.57773, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3704, loss: 2.57759, accuracy: 0.12939\n",
            "Epoch: 1/10, step: 3705, loss: 2.57771, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3706, loss: 2.57767, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3707, loss: 2.57766, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3708, loss: 2.57759, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3709, loss: 2.57779, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3710, loss: 2.57774, accuracy: 0.12938\n",
            "Epoch: 1/10, step: 3711, loss: 2.57791, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3712, loss: 2.57782, accuracy: 0.12938\n",
            "Epoch: 1/10, step: 3713, loss: 2.57768, accuracy: 0.12938\n",
            "Epoch: 1/10, step: 3714, loss: 2.57758, accuracy: 0.12938\n",
            "Epoch: 1/10, step: 3715, loss: 2.57757, accuracy: 0.12934\n",
            "Epoch: 1/10, step: 3716, loss: 2.57746, accuracy: 0.12941\n",
            "Epoch: 1/10, step: 3717, loss: 2.57750, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3718, loss: 2.57741, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3719, loss: 2.57746, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3720, loss: 2.57734, accuracy: 0.12940\n",
            "Epoch: 1/10, step: 3721, loss: 2.57738, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3722, loss: 2.57744, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3723, loss: 2.57744, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3724, loss: 2.57730, accuracy: 0.12933\n",
            "Epoch: 1/10, step: 3725, loss: 2.57719, accuracy: 0.12940\n",
            "Epoch: 1/10, step: 3726, loss: 2.57710, accuracy: 0.12946\n",
            "Epoch: 1/10, step: 3727, loss: 2.57717, accuracy: 0.12943\n",
            "Epoch: 1/10, step: 3728, loss: 2.57714, accuracy: 0.12943\n",
            "Epoch: 1/10, step: 3729, loss: 2.57708, accuracy: 0.12942\n",
            "Epoch: 1/10, step: 3730, loss: 2.57702, accuracy: 0.12939\n",
            "Epoch: 1/10, step: 3731, loss: 2.57691, accuracy: 0.12946\n",
            "Epoch: 1/10, step: 3732, loss: 2.57688, accuracy: 0.12942\n",
            "Epoch: 1/10, step: 3733, loss: 2.57695, accuracy: 0.12939\n",
            "Epoch: 1/10, step: 3734, loss: 2.57714, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3735, loss: 2.57713, accuracy: 0.12935\n",
            "Epoch: 1/10, step: 3736, loss: 2.57721, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3737, loss: 2.57707, accuracy: 0.12931\n",
            "Epoch: 1/10, step: 3738, loss: 2.57700, accuracy: 0.12931\n",
            "Epoch: 1/10, step: 3739, loss: 2.57704, accuracy: 0.12928\n",
            "Epoch: 1/10, step: 3740, loss: 2.57699, accuracy: 0.12931\n",
            "Epoch: 1/10, step: 3741, loss: 2.57702, accuracy: 0.12931\n",
            "Epoch: 1/10, step: 3742, loss: 2.57712, accuracy: 0.12931\n",
            "Epoch: 1/10, step: 3743, loss: 2.57700, accuracy: 0.12934\n",
            "Epoch: 1/10, step: 3744, loss: 2.57700, accuracy: 0.12934\n",
            "Epoch: 1/10, step: 3745, loss: 2.57692, accuracy: 0.12934\n",
            "Epoch: 1/10, step: 3746, loss: 2.57695, accuracy: 0.12930\n",
            "Epoch: 1/10, step: 3747, loss: 2.57693, accuracy: 0.12927\n",
            "Epoch: 1/10, step: 3748, loss: 2.57716, accuracy: 0.12924\n",
            "Epoch: 1/10, step: 3749, loss: 2.57709, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3750, loss: 2.57711, accuracy: 0.12920\n",
            "Epoch: 1/10, step: 3751, loss: 2.57702, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3752, loss: 2.57703, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3753, loss: 2.57698, accuracy: 0.12926\n",
            "Epoch: 1/10, step: 3754, loss: 2.57679, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3755, loss: 2.57673, accuracy: 0.12933\n",
            "Epoch: 1/10, step: 3756, loss: 2.57670, accuracy: 0.12933\n",
            "Epoch: 1/10, step: 3757, loss: 2.57680, accuracy: 0.12929\n",
            "Epoch: 1/10, step: 3758, loss: 2.57683, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3759, loss: 2.57698, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3760, loss: 2.57704, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3761, loss: 2.57707, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3762, loss: 2.57706, accuracy: 0.12929\n",
            "Epoch: 1/10, step: 3763, loss: 2.57706, accuracy: 0.12925\n",
            "Epoch: 1/10, step: 3764, loss: 2.57710, accuracy: 0.12928\n",
            "Epoch: 1/10, step: 3765, loss: 2.57716, accuracy: 0.12925\n",
            "Epoch: 1/10, step: 3766, loss: 2.57715, accuracy: 0.12922\n",
            "Epoch: 1/10, step: 3767, loss: 2.57723, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3768, loss: 2.57729, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3769, loss: 2.57742, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3770, loss: 2.57748, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3771, loss: 2.57749, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3772, loss: 2.57744, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3773, loss: 2.57745, accuracy: 0.12917\n",
            "Epoch: 1/10, step: 3774, loss: 2.57759, accuracy: 0.12914\n",
            "Epoch: 1/10, step: 3775, loss: 2.57766, accuracy: 0.12911\n",
            "Epoch: 1/10, step: 3776, loss: 2.57773, accuracy: 0.12910\n",
            "Epoch: 1/10, step: 3777, loss: 2.57763, accuracy: 0.12910\n",
            "Epoch: 1/10, step: 3778, loss: 2.57763, accuracy: 0.12910\n",
            "Epoch: 1/10, step: 3779, loss: 2.57763, accuracy: 0.12907\n",
            "Epoch: 1/10, step: 3780, loss: 2.57760, accuracy: 0.12907\n",
            "Epoch: 1/10, step: 3781, loss: 2.57752, accuracy: 0.12903\n",
            "Epoch: 1/10, step: 3782, loss: 2.57743, accuracy: 0.12903\n",
            "Epoch: 1/10, step: 3783, loss: 2.57744, accuracy: 0.12906\n",
            "Epoch: 1/10, step: 3784, loss: 2.57740, accuracy: 0.12906\n",
            "Epoch: 1/10, step: 3785, loss: 2.57735, accuracy: 0.12906\n",
            "Epoch: 1/10, step: 3786, loss: 2.57720, accuracy: 0.12913\n",
            "Epoch: 1/10, step: 3787, loss: 2.57716, accuracy: 0.12909\n",
            "Epoch: 1/10, step: 3788, loss: 2.57710, accuracy: 0.12912\n",
            "Epoch: 1/10, step: 3789, loss: 2.57712, accuracy: 0.12916\n",
            "Epoch: 1/10, step: 3790, loss: 2.57703, accuracy: 0.12916\n",
            "Epoch: 1/10, step: 3791, loss: 2.57703, accuracy: 0.12915\n",
            "Epoch: 1/10, step: 3792, loss: 2.57699, accuracy: 0.12919\n",
            "Epoch: 1/10, step: 3793, loss: 2.57723, accuracy: 0.12915\n",
            "Epoch: 1/10, step: 3794, loss: 2.57711, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3795, loss: 2.57698, accuracy: 0.12922\n",
            "Epoch: 1/10, step: 3796, loss: 2.57689, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3797, loss: 2.57676, accuracy: 0.12925\n",
            "Epoch: 1/10, step: 3798, loss: 2.57682, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3799, loss: 2.57694, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3800, loss: 2.57690, accuracy: 0.12918\n",
            "Epoch: 1/10, step: 3801, loss: 2.57678, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3802, loss: 2.57668, accuracy: 0.12924\n",
            "Epoch: 1/10, step: 3803, loss: 2.57666, accuracy: 0.12921\n",
            "Epoch: 1/10, step: 3804, loss: 2.57678, accuracy: 0.12924\n",
            "Epoch: 1/10, step: 3805, loss: 2.57667, accuracy: 0.12927\n",
            "Epoch: 1/10, step: 3806, loss: 2.57675, accuracy: 0.12924\n",
            "Epoch: 1/10, step: 3807, loss: 2.57668, accuracy: 0.12924\n",
            "Epoch: 1/10, step: 3808, loss: 2.57662, accuracy: 0.12920\n",
            "Epoch: 1/10, step: 3809, loss: 2.57670, accuracy: 0.12920\n",
            "Epoch: 1/10, step: 3810, loss: 2.57682, accuracy: 0.12923\n",
            "Epoch: 1/10, step: 3811, loss: 2.57675, accuracy: 0.12930\n",
            "Epoch: 1/10, step: 3812, loss: 2.57656, accuracy: 0.12933\n",
            "Epoch: 1/10, step: 3813, loss: 2.57653, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3814, loss: 2.57649, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3815, loss: 2.57644, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3816, loss: 2.57638, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3817, loss: 2.57635, accuracy: 0.12936\n",
            "Epoch: 1/10, step: 3818, loss: 2.57634, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3819, loss: 2.57630, accuracy: 0.12932\n",
            "Epoch: 1/10, step: 3820, loss: 2.57620, accuracy: 0.12938\n",
            "Epoch: 1/10, step: 3821, loss: 2.57606, accuracy: 0.12942\n",
            "Epoch: 1/10, step: 3822, loss: 2.57607, accuracy: 0.12942\n",
            "Epoch: 1/10, step: 3823, loss: 2.57606, accuracy: 0.12948\n",
            "Epoch: 1/10, step: 3824, loss: 2.57602, accuracy: 0.12948\n",
            "Epoch: 1/10, step: 3825, loss: 2.57601, accuracy: 0.12948\n",
            "Epoch: 1/10, step: 3826, loss: 2.57597, accuracy: 0.12944\n",
            "Epoch: 1/10, step: 3827, loss: 2.57605, accuracy: 0.12941\n",
            "Epoch: 1/10, step: 3828, loss: 2.57619, accuracy: 0.12941\n",
            "Epoch: 1/10, step: 3829, loss: 2.57620, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3830, loss: 2.57624, accuracy: 0.12934\n",
            "Epoch: 1/10, step: 3831, loss: 2.57627, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3832, loss: 2.57613, accuracy: 0.12940\n",
            "Epoch: 1/10, step: 3833, loss: 2.57618, accuracy: 0.12940\n",
            "Epoch: 1/10, step: 3834, loss: 2.57608, accuracy: 0.12937\n",
            "Epoch: 1/10, step: 3835, loss: 2.57603, accuracy: 0.12940\n",
            "Epoch: 1/10, step: 3836, loss: 2.57600, accuracy: 0.12943\n",
            "Epoch: 1/10, step: 3837, loss: 2.57580, accuracy: 0.12950\n",
            "Epoch: 1/10, step: 3838, loss: 2.57586, accuracy: 0.12946\n",
            "Epoch: 1/10, step: 3839, loss: 2.57578, accuracy: 0.12953\n",
            "Epoch: 1/10, step: 3840, loss: 2.57560, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3841, loss: 2.57561, accuracy: 0.12956\n",
            "Epoch: 1/10, step: 3842, loss: 2.57548, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3843, loss: 2.57552, accuracy: 0.12955\n",
            "Epoch: 1/10, step: 3844, loss: 2.57561, accuracy: 0.12955\n",
            "Epoch: 1/10, step: 3845, loss: 2.57569, accuracy: 0.12955\n",
            "Epoch: 1/10, step: 3846, loss: 2.57579, accuracy: 0.12952\n",
            "Epoch: 1/10, step: 3847, loss: 2.57567, accuracy: 0.12952\n",
            "Epoch: 1/10, step: 3848, loss: 2.57550, accuracy: 0.12952\n",
            "Epoch: 1/10, step: 3849, loss: 2.57541, accuracy: 0.12951\n",
            "Epoch: 1/10, step: 3850, loss: 2.57556, accuracy: 0.12948\n",
            "Epoch: 1/10, step: 3851, loss: 2.57554, accuracy: 0.12951\n",
            "Epoch: 1/10, step: 3852, loss: 2.57556, accuracy: 0.12951\n",
            "Epoch: 1/10, step: 3853, loss: 2.57541, accuracy: 0.12957\n",
            "Epoch: 1/10, step: 3854, loss: 2.57525, accuracy: 0.12957\n",
            "Epoch: 1/10, step: 3855, loss: 2.57529, accuracy: 0.12957\n",
            "Epoch: 1/10, step: 3856, loss: 2.57519, accuracy: 0.12964\n",
            "Epoch: 1/10, step: 3857, loss: 2.57519, accuracy: 0.12960\n",
            "Epoch: 1/10, step: 3858, loss: 2.57516, accuracy: 0.12960\n",
            "Epoch: 1/10, step: 3859, loss: 2.57508, accuracy: 0.12960\n",
            "Epoch: 1/10, step: 3860, loss: 2.57506, accuracy: 0.12960\n",
            "Epoch: 1/10, step: 3861, loss: 2.57488, accuracy: 0.12969\n",
            "Epoch: 1/10, step: 3862, loss: 2.57494, accuracy: 0.12966\n",
            "Epoch: 1/10, step: 3863, loss: 2.57485, accuracy: 0.12966\n",
            "Epoch: 1/10, step: 3864, loss: 2.57490, accuracy: 0.12963\n",
            "Epoch: 1/10, step: 3865, loss: 2.57500, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3866, loss: 2.57502, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3867, loss: 2.57502, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3868, loss: 2.57505, accuracy: 0.12959\n",
            "Epoch: 1/10, step: 3869, loss: 2.57497, accuracy: 0.12962\n",
            "Epoch: 1/10, step: 3870, loss: 2.57475, accuracy: 0.12972\n",
            "Epoch: 1/10, step: 3871, loss: 2.57473, accuracy: 0.12971\n",
            "Epoch: 1/10, step: 3872, loss: 2.57476, accuracy: 0.12971\n",
            "Epoch: 1/10, step: 3873, loss: 2.57477, accuracy: 0.12974\n",
            "Epoch: 1/10, step: 3874, loss: 2.57481, accuracy: 0.12974\n",
            "Epoch: 1/10, step: 3875, loss: 2.57479, accuracy: 0.12977\n",
            "Epoch: 1/10, step: 3876, loss: 2.57480, accuracy: 0.12981\n",
            "Epoch: 1/10, step: 3877, loss: 2.57474, accuracy: 0.12984\n",
            "Epoch: 1/10, step: 3878, loss: 2.57470, accuracy: 0.12983\n",
            "Epoch: 1/10, step: 3879, loss: 2.57468, accuracy: 0.12987\n",
            "Epoch: 1/10, step: 3880, loss: 2.57475, accuracy: 0.12986\n",
            "Epoch: 1/10, step: 3881, loss: 2.57467, accuracy: 0.12990\n",
            "Epoch: 1/10, step: 3882, loss: 2.57472, accuracy: 0.12989\n",
            "Epoch: 1/10, step: 3883, loss: 2.57463, accuracy: 0.12993\n",
            "Epoch: 1/10, step: 3884, loss: 2.57456, accuracy: 0.12996\n",
            "Epoch: 1/10, step: 3885, loss: 2.57470, accuracy: 0.12992\n",
            "Epoch: 1/10, step: 3886, loss: 2.57471, accuracy: 0.12989\n",
            "Epoch: 1/10, step: 3887, loss: 2.57465, accuracy: 0.12989\n",
            "Epoch: 1/10, step: 3888, loss: 2.57457, accuracy: 0.12989\n",
            "Epoch: 1/10, step: 3889, loss: 2.57452, accuracy: 0.12989\n",
            "Epoch: 1/10, step: 3890, loss: 2.57463, accuracy: 0.12985\n",
            "Epoch: 1/10, step: 3891, loss: 2.57450, accuracy: 0.12992\n",
            "Epoch: 1/10, step: 3892, loss: 2.57432, accuracy: 0.12998\n",
            "Epoch: 1/10, step: 3893, loss: 2.57424, accuracy: 0.13001\n",
            "Epoch: 1/10, step: 3894, loss: 2.57419, accuracy: 0.13001\n",
            "Epoch: 1/10, step: 3895, loss: 2.57425, accuracy: 0.12997\n",
            "Epoch: 1/10, step: 3896, loss: 2.57420, accuracy: 0.12994\n",
            "Epoch: 1/10, step: 3897, loss: 2.57397, accuracy: 0.12997\n",
            "Epoch: 1/10, step: 3898, loss: 2.57388, accuracy: 0.12997\n",
            "Epoch: 1/10, step: 3899, loss: 2.57389, accuracy: 0.13000\n",
            "Epoch: 1/10, step: 3900, loss: 2.57384, accuracy: 0.13000\n",
            "Epoch: 1/10, step: 3901, loss: 2.57395, accuracy: 0.12997\n",
            "Epoch: 1/10, step: 3902, loss: 2.57384, accuracy: 0.12997\n",
            "Epoch: 1/10, step: 3903, loss: 2.57382, accuracy: 0.12993\n",
            "Epoch: 1/10, step: 3904, loss: 2.57394, accuracy: 0.12993\n",
            "Epoch: 1/10, step: 3905, loss: 2.57381, accuracy: 0.12996\n",
            "Epoch: 1/10, step: 3906, loss: 2.57377, accuracy: 0.12996\n",
            "Epoch: 1/10, step: 3907, loss: 2.57373, accuracy: 0.12996\n",
            "Epoch: 1/10, step: 3908, loss: 2.57365, accuracy: 0.12999\n",
            "Epoch: 1/10, step: 3909, loss: 2.57361, accuracy: 0.12999\n",
            "Epoch: 1/10, step: 3910, loss: 2.57363, accuracy: 0.12996\n",
            "Epoch: 1/10, step: 3911, loss: 2.57370, accuracy: 0.12992\n",
            "Epoch: 1/10, step: 3912, loss: 2.57362, accuracy: 0.12995\n",
            "Epoch: 1/10, step: 3913, loss: 2.57346, accuracy: 0.13002\n",
            "Epoch: 1/10, step: 3914, loss: 2.57346, accuracy: 0.12998\n",
            "Epoch: 1/10, step: 3915, loss: 2.57347, accuracy: 0.13001\n",
            "Epoch: 1/10, step: 3916, loss: 2.57331, accuracy: 0.12998\n",
            "Epoch: 1/10, step: 3917, loss: 2.57337, accuracy: 0.12998\n",
            "Epoch: 1/10, step: 3918, loss: 2.57341, accuracy: 0.12995\n",
            "Epoch: 1/10, step: 3919, loss: 2.57324, accuracy: 0.13004\n",
            "Epoch: 1/10, step: 3920, loss: 2.57321, accuracy: 0.13004\n",
            "Epoch: 1/10, step: 3921, loss: 2.57323, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3922, loss: 2.57314, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3923, loss: 2.57314, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3924, loss: 2.57321, accuracy: 0.13003\n",
            "Epoch: 1/10, step: 3925, loss: 2.57320, accuracy: 0.13003\n",
            "Epoch: 1/10, step: 3926, loss: 2.57327, accuracy: 0.13006\n",
            "Epoch: 1/10, step: 3927, loss: 2.57326, accuracy: 0.13003\n",
            "Epoch: 1/10, step: 3928, loss: 2.57325, accuracy: 0.13006\n",
            "Epoch: 1/10, step: 3929, loss: 2.57342, accuracy: 0.13003\n",
            "Epoch: 1/10, step: 3930, loss: 2.57334, accuracy: 0.13006\n",
            "Epoch: 1/10, step: 3931, loss: 2.57318, accuracy: 0.13012\n",
            "Epoch: 1/10, step: 3932, loss: 2.57327, accuracy: 0.13009\n",
            "Epoch: 1/10, step: 3933, loss: 2.57335, accuracy: 0.13009\n",
            "Epoch: 1/10, step: 3934, loss: 2.57335, accuracy: 0.13015\n",
            "Epoch: 1/10, step: 3935, loss: 2.57335, accuracy: 0.13015\n",
            "Epoch: 1/10, step: 3936, loss: 2.57335, accuracy: 0.13018\n",
            "Epoch: 1/10, step: 3937, loss: 2.57332, accuracy: 0.13014\n",
            "Epoch: 1/10, step: 3938, loss: 2.57333, accuracy: 0.13014\n",
            "Epoch: 1/10, step: 3939, loss: 2.57340, accuracy: 0.13011\n",
            "Epoch: 1/10, step: 3940, loss: 2.57343, accuracy: 0.13008\n",
            "Epoch: 1/10, step: 3941, loss: 2.57338, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3942, loss: 2.57340, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3943, loss: 2.57329, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3944, loss: 2.57322, accuracy: 0.13007\n",
            "Epoch: 1/10, step: 3945, loss: 2.57319, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3946, loss: 2.57323, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3947, loss: 2.57313, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3948, loss: 2.57304, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3949, loss: 2.57310, accuracy: 0.13010\n",
            "Epoch: 1/10, step: 3950, loss: 2.57290, accuracy: 0.13013\n",
            "Epoch: 1/10, step: 3951, loss: 2.57282, accuracy: 0.13016\n",
            "Epoch: 1/10, step: 3952, loss: 2.57278, accuracy: 0.13019\n",
            "Epoch: 1/10, step: 3953, loss: 2.57286, accuracy: 0.13015\n",
            "Epoch: 1/10, step: 3954, loss: 2.57286, accuracy: 0.13018\n",
            "Epoch: 1/10, step: 3955, loss: 2.57278, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3956, loss: 2.57271, accuracy: 0.13018\n",
            "Epoch: 1/10, step: 3957, loss: 2.57259, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3958, loss: 2.57240, accuracy: 0.13024\n",
            "Epoch: 1/10, step: 3959, loss: 2.57242, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3960, loss: 2.57252, accuracy: 0.13018\n",
            "Epoch: 1/10, step: 3961, loss: 2.57255, accuracy: 0.13018\n",
            "Epoch: 1/10, step: 3962, loss: 2.57253, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3963, loss: 2.57250, accuracy: 0.13027\n",
            "Epoch: 1/10, step: 3964, loss: 2.57257, accuracy: 0.13023\n",
            "Epoch: 1/10, step: 3965, loss: 2.57254, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3966, loss: 2.57250, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3967, loss: 2.57243, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3968, loss: 2.57257, accuracy: 0.13017\n",
            "Epoch: 1/10, step: 3969, loss: 2.57250, accuracy: 0.13013\n",
            "Epoch: 1/10, step: 3970, loss: 2.57238, accuracy: 0.13016\n",
            "Epoch: 1/10, step: 3971, loss: 2.57228, accuracy: 0.13023\n",
            "Epoch: 1/10, step: 3972, loss: 2.57219, accuracy: 0.13026\n",
            "Epoch: 1/10, step: 3973, loss: 2.57210, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3974, loss: 2.57211, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3975, loss: 2.57215, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3976, loss: 2.57215, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3977, loss: 2.57209, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3978, loss: 2.57219, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3979, loss: 2.57220, accuracy: 0.13025\n",
            "Epoch: 1/10, step: 3980, loss: 2.57213, accuracy: 0.13028\n",
            "Epoch: 1/10, step: 3981, loss: 2.57202, accuracy: 0.13031\n",
            "Epoch: 1/10, step: 3982, loss: 2.57191, accuracy: 0.13027\n",
            "Epoch: 1/10, step: 3983, loss: 2.57192, accuracy: 0.13024\n",
            "Epoch: 1/10, step: 3984, loss: 2.57196, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3985, loss: 2.57193, accuracy: 0.13021\n",
            "Epoch: 1/10, step: 3986, loss: 2.57188, accuracy: 0.13024\n",
            "Epoch: 1/10, step: 3987, loss: 2.57185, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3988, loss: 2.57188, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3989, loss: 2.57194, accuracy: 0.13023\n",
            "Epoch: 1/10, step: 3990, loss: 2.57196, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3991, loss: 2.57204, accuracy: 0.13020\n",
            "Epoch: 1/10, step: 3992, loss: 2.57193, accuracy: 0.13023\n",
            "Epoch: 1/10, step: 3993, loss: 2.57205, accuracy: 0.13023\n",
            "Epoch: 1/10, step: 3994, loss: 2.57194, accuracy: 0.13029\n",
            "Epoch: 1/10, step: 3995, loss: 2.57200, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 3996, loss: 2.57186, accuracy: 0.13041\n",
            "Epoch: 1/10, step: 3997, loss: 2.57198, accuracy: 0.13038\n",
            "Epoch: 1/10, step: 3998, loss: 2.57213, accuracy: 0.13035\n",
            "Epoch: 1/10, step: 3999, loss: 2.57217, accuracy: 0.13031\n",
            "Epoch: 1/10, step: 4000, loss: 2.57226, accuracy: 0.13031\n",
            "Epoch: 1/10, step: 4001, loss: 2.57231, accuracy: 0.13031\n",
            "Epoch: 1/10, step: 4002, loss: 2.57222, accuracy: 0.13034\n",
            "Epoch: 1/10, step: 4003, loss: 2.57220, accuracy: 0.13034\n",
            "Epoch: 1/10, step: 4004, loss: 2.57207, accuracy: 0.13037\n",
            "Epoch: 1/10, step: 4005, loss: 2.57220, accuracy: 0.13034\n",
            "Epoch: 1/10, step: 4006, loss: 2.57214, accuracy: 0.13037\n",
            "Epoch: 1/10, step: 4007, loss: 2.57220, accuracy: 0.13033\n",
            "Epoch: 1/10, step: 4008, loss: 2.57228, accuracy: 0.13030\n",
            "Epoch: 1/10, step: 4009, loss: 2.57219, accuracy: 0.13039\n",
            "Epoch: 1/10, step: 4010, loss: 2.57212, accuracy: 0.13039\n",
            "Epoch: 1/10, step: 4011, loss: 2.57201, accuracy: 0.13042\n",
            "Epoch: 1/10, step: 4012, loss: 2.57211, accuracy: 0.13045\n",
            "Epoch: 1/10, step: 4013, loss: 2.57212, accuracy: 0.13042\n",
            "Epoch: 1/10, step: 4014, loss: 2.57198, accuracy: 0.13045\n",
            "Epoch: 1/10, step: 4015, loss: 2.57192, accuracy: 0.13045\n",
            "Epoch: 1/10, step: 4016, loss: 2.57192, accuracy: 0.13045\n",
            "Epoch: 1/10, step: 4017, loss: 2.57184, accuracy: 0.13041\n",
            "Epoch: 1/10, step: 4018, loss: 2.57192, accuracy: 0.13041\n",
            "Epoch: 1/10, step: 4019, loss: 2.57180, accuracy: 0.13047\n",
            "Epoch: 1/10, step: 4020, loss: 2.57173, accuracy: 0.13047\n",
            "Epoch: 1/10, step: 4021, loss: 2.57186, accuracy: 0.13044\n",
            "Epoch: 1/10, step: 4022, loss: 2.57195, accuracy: 0.13047\n",
            "Epoch: 1/10, step: 4023, loss: 2.57194, accuracy: 0.13047\n",
            "Epoch: 1/10, step: 4024, loss: 2.57192, accuracy: 0.13047\n",
            "Epoch: 1/10, step: 4025, loss: 2.57193, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4026, loss: 2.57179, accuracy: 0.13046\n",
            "Epoch: 1/10, step: 4027, loss: 2.57180, accuracy: 0.13046\n",
            "Epoch: 1/10, step: 4028, loss: 2.57171, accuracy: 0.13049\n",
            "Epoch: 1/10, step: 4029, loss: 2.57167, accuracy: 0.13049\n",
            "Epoch: 1/10, step: 4030, loss: 2.57161, accuracy: 0.13046\n",
            "Epoch: 1/10, step: 4031, loss: 2.57162, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4032, loss: 2.57164, accuracy: 0.13039\n",
            "Epoch: 1/10, step: 4033, loss: 2.57173, accuracy: 0.13036\n",
            "Epoch: 1/10, step: 4034, loss: 2.57174, accuracy: 0.13033\n",
            "Epoch: 1/10, step: 4035, loss: 2.57179, accuracy: 0.13030\n",
            "Epoch: 1/10, step: 4036, loss: 2.57179, accuracy: 0.13033\n",
            "Epoch: 1/10, step: 4037, loss: 2.57185, accuracy: 0.13033\n",
            "Epoch: 1/10, step: 4038, loss: 2.57183, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 4039, loss: 2.57189, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 4040, loss: 2.57185, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 4041, loss: 2.57195, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 4042, loss: 2.57202, accuracy: 0.13029\n",
            "Epoch: 1/10, step: 4043, loss: 2.57195, accuracy: 0.13032\n",
            "Epoch: 1/10, step: 4044, loss: 2.57195, accuracy: 0.13029\n",
            "Epoch: 1/10, step: 4045, loss: 2.57183, accuracy: 0.13038\n",
            "Epoch: 1/10, step: 4046, loss: 2.57181, accuracy: 0.13034\n",
            "Epoch: 1/10, step: 4047, loss: 2.57182, accuracy: 0.13034\n",
            "Epoch: 1/10, step: 4048, loss: 2.57185, accuracy: 0.13037\n",
            "Epoch: 1/10, step: 4049, loss: 2.57175, accuracy: 0.13040\n",
            "Epoch: 1/10, step: 4050, loss: 2.57180, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4051, loss: 2.57184, accuracy: 0.13040\n",
            "Epoch: 1/10, step: 4052, loss: 2.57179, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4053, loss: 2.57175, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4054, loss: 2.57171, accuracy: 0.13043\n",
            "Epoch: 1/10, step: 4055, loss: 2.57164, accuracy: 0.13039\n",
            "Epoch: 1/10, step: 4056, loss: 2.57168, accuracy: 0.13039\n",
            "Epoch: 1/10, step: 4057, loss: 2.57181, accuracy: 0.13036\n",
            "Epoch: 1/10, step: 4058, loss: 2.57165, accuracy: 0.13042\n",
            "Epoch: 1/10, step: 4059, loss: 2.57160, accuracy: 0.13042\n",
            "Epoch: 1/10, step: 4060, loss: 2.57137, accuracy: 0.13051\n",
            "Epoch: 1/10, step: 4061, loss: 2.57132, accuracy: 0.13054\n",
            "Epoch: 1/10, step: 4062, loss: 2.57126, accuracy: 0.13060\n",
            "Epoch: 1/10, step: 4063, loss: 2.57123, accuracy: 0.13060\n",
            "Epoch: 1/10, step: 4064, loss: 2.57106, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4065, loss: 2.57106, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4066, loss: 2.57108, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4067, loss: 2.57100, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4068, loss: 2.57096, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4069, loss: 2.57104, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4070, loss: 2.57115, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4071, loss: 2.57110, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4072, loss: 2.57105, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4073, loss: 2.57113, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4074, loss: 2.57114, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4075, loss: 2.57110, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4076, loss: 2.57108, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4077, loss: 2.57103, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4078, loss: 2.57096, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4079, loss: 2.57089, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4080, loss: 2.57088, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4081, loss: 2.57089, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4082, loss: 2.57089, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4083, loss: 2.57083, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4084, loss: 2.57071, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4085, loss: 2.57058, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4086, loss: 2.57070, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4087, loss: 2.57081, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4088, loss: 2.57096, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4089, loss: 2.57102, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4090, loss: 2.57108, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4091, loss: 2.57106, accuracy: 0.13062\n",
            "Epoch: 1/10, step: 4092, loss: 2.57117, accuracy: 0.13062\n",
            "Epoch: 1/10, step: 4093, loss: 2.57103, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4094, loss: 2.57114, accuracy: 0.13062\n",
            "Epoch: 1/10, step: 4095, loss: 2.57107, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4096, loss: 2.57109, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4097, loss: 2.57112, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4098, loss: 2.57124, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4099, loss: 2.57133, accuracy: 0.13064\n",
            "Epoch: 1/10, step: 4100, loss: 2.57138, accuracy: 0.13064\n",
            "Epoch: 1/10, step: 4101, loss: 2.57135, accuracy: 0.13061\n",
            "Epoch: 1/10, step: 4102, loss: 2.57125, accuracy: 0.13061\n",
            "Epoch: 1/10, step: 4103, loss: 2.57118, accuracy: 0.13064\n",
            "Epoch: 1/10, step: 4104, loss: 2.57120, accuracy: 0.13060\n",
            "Epoch: 1/10, step: 4105, loss: 2.57115, accuracy: 0.13060\n",
            "Epoch: 1/10, step: 4106, loss: 2.57106, accuracy: 0.13063\n",
            "Epoch: 1/10, step: 4107, loss: 2.57103, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4108, loss: 2.57096, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4109, loss: 2.57092, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4110, loss: 2.57083, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4111, loss: 2.57072, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4112, loss: 2.57078, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4113, loss: 2.57078, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4114, loss: 2.57080, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4115, loss: 2.57086, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4116, loss: 2.57092, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4117, loss: 2.57093, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4118, loss: 2.57083, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4119, loss: 2.57095, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4120, loss: 2.57096, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4121, loss: 2.57092, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4122, loss: 2.57083, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4123, loss: 2.57080, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4124, loss: 2.57082, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4125, loss: 2.57085, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4126, loss: 2.57092, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4127, loss: 2.57089, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4128, loss: 2.57080, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4129, loss: 2.57085, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4130, loss: 2.57092, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4131, loss: 2.57092, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4132, loss: 2.57090, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4133, loss: 2.57074, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4134, loss: 2.57074, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4135, loss: 2.57073, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4136, loss: 2.57086, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4137, loss: 2.57072, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4138, loss: 2.57070, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4139, loss: 2.57080, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4140, loss: 2.57081, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4141, loss: 2.57071, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4142, loss: 2.57064, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4143, loss: 2.57067, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4144, loss: 2.57065, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4145, loss: 2.57051, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4146, loss: 2.57047, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4147, loss: 2.57046, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4148, loss: 2.57043, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4149, loss: 2.57044, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4150, loss: 2.57041, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4151, loss: 2.57051, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4152, loss: 2.57040, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4153, loss: 2.57030, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4154, loss: 2.57010, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4155, loss: 2.57005, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4156, loss: 2.57012, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4157, loss: 2.57008, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4158, loss: 2.57010, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4159, loss: 2.57014, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4160, loss: 2.57018, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4161, loss: 2.57006, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4162, loss: 2.57011, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4163, loss: 2.57000, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4164, loss: 2.56993, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4165, loss: 2.56980, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4166, loss: 2.56983, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4167, loss: 2.56981, accuracy: 0.13082\n",
            "Epoch: 1/10, step: 4168, loss: 2.56972, accuracy: 0.13088\n",
            "Epoch: 1/10, step: 4169, loss: 2.56974, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4170, loss: 2.56973, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4171, loss: 2.56962, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4172, loss: 2.56973, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4173, loss: 2.56963, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4174, loss: 2.56961, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4175, loss: 2.56950, accuracy: 0.13087\n",
            "Epoch: 1/10, step: 4176, loss: 2.56955, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4177, loss: 2.56957, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4178, loss: 2.56959, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4179, loss: 2.56949, accuracy: 0.13083\n",
            "Epoch: 1/10, step: 4180, loss: 2.56942, accuracy: 0.13086\n",
            "Epoch: 1/10, step: 4181, loss: 2.56945, accuracy: 0.13083\n",
            "Epoch: 1/10, step: 4182, loss: 2.56938, accuracy: 0.13086\n",
            "Epoch: 1/10, step: 4183, loss: 2.56951, accuracy: 0.13083\n",
            "Epoch: 1/10, step: 4184, loss: 2.56964, accuracy: 0.13083\n",
            "Epoch: 1/10, step: 4185, loss: 2.56958, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4186, loss: 2.56956, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4187, loss: 2.56956, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4188, loss: 2.56951, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4189, loss: 2.56947, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4190, loss: 2.56946, accuracy: 0.13088\n",
            "Epoch: 1/10, step: 4191, loss: 2.56949, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4192, loss: 2.56941, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4193, loss: 2.56945, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4194, loss: 2.56937, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4195, loss: 2.56935, accuracy: 0.13099\n",
            "Epoch: 1/10, step: 4196, loss: 2.56923, accuracy: 0.13105\n",
            "Epoch: 1/10, step: 4197, loss: 2.56928, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4198, loss: 2.56926, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4199, loss: 2.56922, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4200, loss: 2.56925, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4201, loss: 2.56922, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4202, loss: 2.56908, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4203, loss: 2.56917, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4204, loss: 2.56918, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4205, loss: 2.56913, accuracy: 0.13103\n",
            "Epoch: 1/10, step: 4206, loss: 2.56920, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4207, loss: 2.56927, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4208, loss: 2.56933, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4209, loss: 2.56938, accuracy: 0.13103\n",
            "Epoch: 1/10, step: 4210, loss: 2.56934, accuracy: 0.13103\n",
            "Epoch: 1/10, step: 4211, loss: 2.56940, accuracy: 0.13103\n",
            "Epoch: 1/10, step: 4212, loss: 2.56940, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4213, loss: 2.56946, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4214, loss: 2.56946, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4215, loss: 2.56947, accuracy: 0.13099\n",
            "Epoch: 1/10, step: 4216, loss: 2.56934, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4217, loss: 2.56915, accuracy: 0.13114\n",
            "Epoch: 1/10, step: 4218, loss: 2.56905, accuracy: 0.13119\n",
            "Epoch: 1/10, step: 4219, loss: 2.56909, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4220, loss: 2.56907, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4221, loss: 2.56902, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4222, loss: 2.56895, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4223, loss: 2.56891, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4224, loss: 2.56897, accuracy: 0.13113\n",
            "Epoch: 1/10, step: 4225, loss: 2.56895, accuracy: 0.13112\n",
            "Epoch: 1/10, step: 4226, loss: 2.56890, accuracy: 0.13109\n",
            "Epoch: 1/10, step: 4227, loss: 2.56875, accuracy: 0.13112\n",
            "Epoch: 1/10, step: 4228, loss: 2.56865, accuracy: 0.13115\n",
            "Epoch: 1/10, step: 4229, loss: 2.56867, accuracy: 0.13112\n",
            "Epoch: 1/10, step: 4230, loss: 2.56868, accuracy: 0.13112\n",
            "Epoch: 1/10, step: 4231, loss: 2.56874, accuracy: 0.13109\n",
            "Epoch: 1/10, step: 4232, loss: 2.56866, accuracy: 0.13114\n",
            "Epoch: 1/10, step: 4233, loss: 2.56861, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4234, loss: 2.56867, accuracy: 0.13114\n",
            "Epoch: 1/10, step: 4235, loss: 2.56877, accuracy: 0.13111\n",
            "Epoch: 1/10, step: 4236, loss: 2.56889, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4237, loss: 2.56879, accuracy: 0.13105\n",
            "Epoch: 1/10, step: 4238, loss: 2.56877, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4239, loss: 2.56890, accuracy: 0.13105\n",
            "Epoch: 1/10, step: 4240, loss: 2.56882, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4241, loss: 2.56880, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4242, loss: 2.56874, accuracy: 0.13110\n",
            "Epoch: 1/10, step: 4243, loss: 2.56861, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4244, loss: 2.56857, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4245, loss: 2.56854, accuracy: 0.13121\n",
            "Epoch: 1/10, step: 4246, loss: 2.56849, accuracy: 0.13124\n",
            "Epoch: 1/10, step: 4247, loss: 2.56855, accuracy: 0.13121\n",
            "Epoch: 1/10, step: 4248, loss: 2.56861, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4249, loss: 2.56863, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4250, loss: 2.56862, accuracy: 0.13121\n",
            "Epoch: 1/10, step: 4251, loss: 2.56858, accuracy: 0.13120\n",
            "Epoch: 1/10, step: 4252, loss: 2.56859, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4253, loss: 2.56857, accuracy: 0.13114\n",
            "Epoch: 1/10, step: 4254, loss: 2.56860, accuracy: 0.13111\n",
            "Epoch: 1/10, step: 4255, loss: 2.56864, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4256, loss: 2.56875, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4257, loss: 2.56865, accuracy: 0.13111\n",
            "Epoch: 1/10, step: 4258, loss: 2.56886, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4259, loss: 2.56880, accuracy: 0.13105\n",
            "Epoch: 1/10, step: 4260, loss: 2.56891, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4261, loss: 2.56886, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4262, loss: 2.56866, accuracy: 0.13110\n",
            "Epoch: 1/10, step: 4263, loss: 2.56866, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4264, loss: 2.56851, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4265, loss: 2.56851, accuracy: 0.13110\n",
            "Epoch: 1/10, step: 4266, loss: 2.56841, accuracy: 0.13109\n",
            "Epoch: 1/10, step: 4267, loss: 2.56821, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4268, loss: 2.56820, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4269, loss: 2.56817, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4270, loss: 2.56815, accuracy: 0.13115\n",
            "Epoch: 1/10, step: 4271, loss: 2.56810, accuracy: 0.13118\n",
            "Epoch: 1/10, step: 4272, loss: 2.56806, accuracy: 0.13120\n",
            "Epoch: 1/10, step: 4273, loss: 2.56800, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4274, loss: 2.56801, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4275, loss: 2.56813, accuracy: 0.13114\n",
            "Epoch: 1/10, step: 4276, loss: 2.56808, accuracy: 0.13111\n",
            "Epoch: 1/10, step: 4277, loss: 2.56809, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4278, loss: 2.56810, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4279, loss: 2.56809, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4280, loss: 2.56806, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4281, loss: 2.56801, accuracy: 0.13113\n",
            "Epoch: 1/10, step: 4282, loss: 2.56807, accuracy: 0.13110\n",
            "Epoch: 1/10, step: 4283, loss: 2.56802, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4284, loss: 2.56818, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4285, loss: 2.56805, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4286, loss: 2.56795, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4287, loss: 2.56798, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4288, loss: 2.56790, accuracy: 0.13103\n",
            "Epoch: 1/10, step: 4289, loss: 2.56784, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4290, loss: 2.56781, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4291, loss: 2.56784, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4292, loss: 2.56797, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4293, loss: 2.56791, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4294, loss: 2.56786, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4295, loss: 2.56773, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4296, loss: 2.56781, accuracy: 0.13099\n",
            "Epoch: 1/10, step: 4297, loss: 2.56790, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4298, loss: 2.56777, accuracy: 0.13099\n",
            "Epoch: 1/10, step: 4299, loss: 2.56759, accuracy: 0.13108\n",
            "Epoch: 1/10, step: 4300, loss: 2.56758, accuracy: 0.13105\n",
            "Epoch: 1/10, step: 4301, loss: 2.56767, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4302, loss: 2.56763, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4303, loss: 2.56758, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4304, loss: 2.56745, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4305, loss: 2.56747, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4306, loss: 2.56749, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4307, loss: 2.56751, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4308, loss: 2.56747, accuracy: 0.13101\n",
            "Epoch: 1/10, step: 4309, loss: 2.56750, accuracy: 0.13098\n",
            "Epoch: 1/10, step: 4310, loss: 2.56752, accuracy: 0.13095\n",
            "Epoch: 1/10, step: 4311, loss: 2.56743, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4312, loss: 2.56753, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4313, loss: 2.56764, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4314, loss: 2.56782, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4315, loss: 2.56774, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4316, loss: 2.56761, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4317, loss: 2.56754, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4318, loss: 2.56752, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4319, loss: 2.56743, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4320, loss: 2.56745, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4321, loss: 2.56738, accuracy: 0.13099\n",
            "Epoch: 1/10, step: 4322, loss: 2.56746, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4323, loss: 2.56734, accuracy: 0.13096\n",
            "Epoch: 1/10, step: 4324, loss: 2.56742, accuracy: 0.13093\n",
            "Epoch: 1/10, step: 4325, loss: 2.56749, accuracy: 0.13090\n",
            "Epoch: 1/10, step: 4326, loss: 2.56740, accuracy: 0.13092\n",
            "Epoch: 1/10, step: 4327, loss: 2.56741, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4328, loss: 2.56743, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4329, loss: 2.56747, accuracy: 0.13086\n",
            "Epoch: 1/10, step: 4330, loss: 2.56738, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4331, loss: 2.56749, accuracy: 0.13086\n",
            "Epoch: 1/10, step: 4332, loss: 2.56738, accuracy: 0.13086\n",
            "Epoch: 1/10, step: 4333, loss: 2.56735, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4334, loss: 2.56744, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4335, loss: 2.56742, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4336, loss: 2.56749, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4337, loss: 2.56749, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4338, loss: 2.56746, accuracy: 0.13091\n",
            "Epoch: 1/10, step: 4339, loss: 2.56743, accuracy: 0.13088\n",
            "Epoch: 1/10, step: 4340, loss: 2.56742, accuracy: 0.13090\n",
            "Epoch: 1/10, step: 4341, loss: 2.56740, accuracy: 0.13090\n",
            "Epoch: 1/10, step: 4342, loss: 2.56741, accuracy: 0.13087\n",
            "Epoch: 1/10, step: 4343, loss: 2.56740, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4344, loss: 2.56737, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4345, loss: 2.56747, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4346, loss: 2.56745, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4347, loss: 2.56747, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4348, loss: 2.56748, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4349, loss: 2.56744, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4350, loss: 2.56740, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4351, loss: 2.56730, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4352, loss: 2.56725, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4353, loss: 2.56738, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4354, loss: 2.56756, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4355, loss: 2.56740, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4356, loss: 2.56735, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4357, loss: 2.56739, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4358, loss: 2.56739, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4359, loss: 2.56736, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4360, loss: 2.56739, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4361, loss: 2.56753, accuracy: 0.13062\n",
            "Epoch: 1/10, step: 4362, loss: 2.56744, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4363, loss: 2.56733, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4364, loss: 2.56734, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4365, loss: 2.56724, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4366, loss: 2.56728, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4367, loss: 2.56730, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4368, loss: 2.56715, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4369, loss: 2.56728, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4370, loss: 2.56729, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4371, loss: 2.56739, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4372, loss: 2.56733, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4373, loss: 2.56737, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4374, loss: 2.56740, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4375, loss: 2.56726, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4376, loss: 2.56738, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4377, loss: 2.56730, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4378, loss: 2.56721, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4379, loss: 2.56723, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4380, loss: 2.56719, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4381, loss: 2.56723, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4382, loss: 2.56727, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4383, loss: 2.56724, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4384, loss: 2.56716, accuracy: 0.13065\n",
            "Epoch: 1/10, step: 4385, loss: 2.56716, accuracy: 0.13064\n",
            "Epoch: 1/10, step: 4386, loss: 2.56704, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4387, loss: 2.56701, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4388, loss: 2.56700, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4389, loss: 2.56702, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4390, loss: 2.56699, accuracy: 0.13067\n",
            "Epoch: 1/10, step: 4391, loss: 2.56698, accuracy: 0.13066\n",
            "Epoch: 1/10, step: 4392, loss: 2.56693, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4393, loss: 2.56700, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4394, loss: 2.56700, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4395, loss: 2.56692, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4396, loss: 2.56685, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4397, loss: 2.56674, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4398, loss: 2.56662, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4399, loss: 2.56665, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4400, loss: 2.56674, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4401, loss: 2.56673, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4402, loss: 2.56668, accuracy: 0.13071\n",
            "Epoch: 1/10, step: 4403, loss: 2.56676, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4404, loss: 2.56684, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4405, loss: 2.56684, accuracy: 0.13068\n",
            "Epoch: 1/10, step: 4406, loss: 2.56676, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4407, loss: 2.56668, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4408, loss: 2.56652, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4409, loss: 2.56654, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4410, loss: 2.56643, accuracy: 0.13073\n",
            "Epoch: 1/10, step: 4411, loss: 2.56642, accuracy: 0.13070\n",
            "Epoch: 1/10, step: 4412, loss: 2.56639, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4413, loss: 2.56632, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4414, loss: 2.56631, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4415, loss: 2.56632, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4416, loss: 2.56633, accuracy: 0.13075\n",
            "Epoch: 1/10, step: 4417, loss: 2.56629, accuracy: 0.13072\n",
            "Epoch: 1/10, step: 4418, loss: 2.56626, accuracy: 0.13069\n",
            "Epoch: 1/10, step: 4419, loss: 2.56615, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4420, loss: 2.56616, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4421, loss: 2.56618, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4422, loss: 2.56603, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4423, loss: 2.56593, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4424, loss: 2.56571, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4425, loss: 2.56572, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4426, loss: 2.56577, accuracy: 0.13085\n",
            "Epoch: 1/10, step: 4427, loss: 2.56574, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4428, loss: 2.56571, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4429, loss: 2.56575, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4430, loss: 2.56565, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4431, loss: 2.56564, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4432, loss: 2.56560, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4433, loss: 2.56567, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4434, loss: 2.56556, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4435, loss: 2.56554, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4436, loss: 2.56554, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4437, loss: 2.56560, accuracy: 0.13078\n",
            "Epoch: 1/10, step: 4438, loss: 2.56551, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4439, loss: 2.56564, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4440, loss: 2.56558, accuracy: 0.13080\n",
            "Epoch: 1/10, step: 4441, loss: 2.56567, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4442, loss: 2.56564, accuracy: 0.13077\n",
            "Epoch: 1/10, step: 4443, loss: 2.56561, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4444, loss: 2.56552, accuracy: 0.13074\n",
            "Epoch: 1/10, step: 4445, loss: 2.56545, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4446, loss: 2.56543, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4447, loss: 2.56537, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4448, loss: 2.56532, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4449, loss: 2.56530, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4450, loss: 2.56523, accuracy: 0.13079\n",
            "Epoch: 1/10, step: 4451, loss: 2.56521, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4452, loss: 2.56519, accuracy: 0.13076\n",
            "Epoch: 1/10, step: 4453, loss: 2.56510, accuracy: 0.13081\n",
            "Epoch: 1/10, step: 4454, loss: 2.56503, accuracy: 0.13084\n",
            "Epoch: 1/10, step: 4455, loss: 2.56497, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4456, loss: 2.56491, accuracy: 0.13089\n",
            "Epoch: 1/10, step: 4457, loss: 2.56479, accuracy: 0.13092\n",
            "Epoch: 1/10, step: 4458, loss: 2.56467, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4459, loss: 2.56456, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4460, loss: 2.56458, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4461, loss: 2.56444, accuracy: 0.13100\n",
            "Epoch: 1/10, step: 4462, loss: 2.56441, accuracy: 0.13097\n",
            "Epoch: 1/10, step: 4463, loss: 2.56442, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4464, loss: 2.56435, accuracy: 0.13094\n",
            "Epoch: 1/10, step: 4465, loss: 2.56417, accuracy: 0.13102\n",
            "Epoch: 1/10, step: 4466, loss: 2.56404, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4467, loss: 2.56410, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4468, loss: 2.56408, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4469, loss: 2.56396, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4470, loss: 2.56393, accuracy: 0.13104\n",
            "Epoch: 1/10, step: 4471, loss: 2.56386, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4472, loss: 2.56396, accuracy: 0.13107\n",
            "Epoch: 1/10, step: 4473, loss: 2.56394, accuracy: 0.13109\n",
            "Epoch: 1/10, step: 4474, loss: 2.56397, accuracy: 0.13106\n",
            "Epoch: 1/10, step: 4475, loss: 2.56381, accuracy: 0.13112\n",
            "Epoch: 1/10, step: 4476, loss: 2.56365, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4477, loss: 2.56355, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4478, loss: 2.56352, accuracy: 0.13117\n",
            "Epoch: 1/10, step: 4479, loss: 2.56346, accuracy: 0.13120\n",
            "Epoch: 1/10, step: 4480, loss: 2.56349, accuracy: 0.13119\n",
            "Epoch: 1/10, step: 4481, loss: 2.56353, accuracy: 0.13119\n",
            "Epoch: 1/10, step: 4482, loss: 2.56356, accuracy: 0.13116\n",
            "Epoch: 1/10, step: 4483, loss: 2.56344, accuracy: 0.13119\n",
            "Epoch: 1/10, step: 4484, loss: 2.56333, accuracy: 0.13124\n",
            "Epoch: 1/10, step: 4485, loss: 2.56331, accuracy: 0.13124\n",
            "Epoch: 1/10, step: 4486, loss: 2.56321, accuracy: 0.13127\n",
            "Epoch: 1/10, step: 4487, loss: 2.56317, accuracy: 0.13127\n",
            "Epoch: 1/10, step: 4488, loss: 2.56317, accuracy: 0.13127\n",
            "Epoch: 1/10, step: 4489, loss: 2.56311, accuracy: 0.13124\n",
            "Epoch: 1/10, step: 4490, loss: 2.56297, accuracy: 0.13129\n",
            "Epoch: 1/10, step: 4491, loss: 2.56288, accuracy: 0.13129\n",
            "Epoch: 1/10, step: 4492, loss: 2.56289, accuracy: 0.13129\n",
            "Epoch: 1/10, step: 4493, loss: 2.56291, accuracy: 0.13126\n",
            "Epoch: 1/10, step: 4494, loss: 2.56287, accuracy: 0.13126\n",
            "Epoch: 1/10, step: 4495, loss: 2.56294, accuracy: 0.13126\n",
            "Epoch: 1/10, step: 4496, loss: 2.56289, accuracy: 0.13128\n",
            "Epoch: 1/10, step: 4497, loss: 2.56289, accuracy: 0.13128\n",
            "Epoch: 1/10, step: 4498, loss: 2.56290, accuracy: 0.13125\n",
            "Epoch: 1/10, step: 4499, loss: 2.56293, accuracy: 0.13128\n",
            "Epoch: 1/10, step: 4500, loss: 2.56290, accuracy: 0.13133\n",
            "Epoch: 1/10, step: 4501, loss: 2.56276, accuracy: 0.13133\n",
            "Epoch: 1/10, step: 4502, loss: 2.56276, accuracy: 0.13136\n",
            "Epoch: 1/10, step: 4503, loss: 2.56269, accuracy: 0.13141\n",
            "Epoch: 1/10, step: 4504, loss: 2.56266, accuracy: 0.13138\n",
            "Epoch: 1/10, step: 4505, loss: 2.56266, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4506, loss: 2.56269, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4507, loss: 2.56266, accuracy: 0.13138\n",
            "Epoch: 1/10, step: 4508, loss: 2.56278, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4509, loss: 2.56281, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4510, loss: 2.56274, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4511, loss: 2.56263, accuracy: 0.13143\n",
            "Epoch: 1/10, step: 4512, loss: 2.56275, accuracy: 0.13140\n",
            "Epoch: 1/10, step: 4513, loss: 2.56268, accuracy: 0.13143\n",
            "Epoch: 1/10, step: 4514, loss: 2.56260, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4515, loss: 2.56261, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4516, loss: 2.56260, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4517, loss: 2.56246, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4518, loss: 2.56249, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4519, loss: 2.56240, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4520, loss: 2.56234, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4521, loss: 2.56231, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4522, loss: 2.56231, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4523, loss: 2.56222, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4524, loss: 2.56230, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4525, loss: 2.56233, accuracy: 0.13144\n",
            "Epoch: 1/10, step: 4526, loss: 2.56225, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4527, loss: 2.56211, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4528, loss: 2.56217, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4529, loss: 2.56221, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4530, loss: 2.56225, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4531, loss: 2.56224, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4532, loss: 2.56223, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4533, loss: 2.56228, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4534, loss: 2.56230, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4535, loss: 2.56243, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4536, loss: 2.56230, accuracy: 0.13148\n",
            "Epoch: 1/10, step: 4537, loss: 2.56222, accuracy: 0.13153\n",
            "Epoch: 1/10, step: 4538, loss: 2.56226, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4539, loss: 2.56246, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4540, loss: 2.56240, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4541, loss: 2.56239, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4542, loss: 2.56240, accuracy: 0.13152\n",
            "Epoch: 1/10, step: 4543, loss: 2.56244, accuracy: 0.13152\n",
            "Epoch: 1/10, step: 4544, loss: 2.56250, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4545, loss: 2.56257, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4546, loss: 2.56261, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4547, loss: 2.56262, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4548, loss: 2.56270, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4549, loss: 2.56272, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4550, loss: 2.56272, accuracy: 0.13143\n",
            "Epoch: 1/10, step: 4551, loss: 2.56277, accuracy: 0.13143\n",
            "Epoch: 1/10, step: 4552, loss: 2.56289, accuracy: 0.13140\n",
            "Epoch: 1/10, step: 4553, loss: 2.56288, accuracy: 0.13140\n",
            "Epoch: 1/10, step: 4554, loss: 2.56298, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4555, loss: 2.56293, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4556, loss: 2.56292, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4557, loss: 2.56289, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4558, loss: 2.56293, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4559, loss: 2.56291, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4560, loss: 2.56284, accuracy: 0.13141\n",
            "Epoch: 1/10, step: 4561, loss: 2.56271, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4562, loss: 2.56262, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4563, loss: 2.56252, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4564, loss: 2.56245, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4565, loss: 2.56243, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4566, loss: 2.56239, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4567, loss: 2.56246, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4568, loss: 2.56235, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4569, loss: 2.56231, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4570, loss: 2.56228, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4571, loss: 2.56229, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4572, loss: 2.56235, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4573, loss: 2.56230, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4574, loss: 2.56230, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4575, loss: 2.56230, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4576, loss: 2.56220, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4577, loss: 2.56212, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4578, loss: 2.56202, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4579, loss: 2.56207, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4580, loss: 2.56206, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4581, loss: 2.56195, accuracy: 0.13136\n",
            "Epoch: 1/10, step: 4582, loss: 2.56189, accuracy: 0.13138\n",
            "Epoch: 1/10, step: 4583, loss: 2.56193, accuracy: 0.13136\n",
            "Epoch: 1/10, step: 4584, loss: 2.56183, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4585, loss: 2.56182, accuracy: 0.13135\n",
            "Epoch: 1/10, step: 4586, loss: 2.56182, accuracy: 0.13132\n",
            "Epoch: 1/10, step: 4587, loss: 2.56189, accuracy: 0.13129\n",
            "Epoch: 1/10, step: 4588, loss: 2.56182, accuracy: 0.13129\n",
            "Epoch: 1/10, step: 4589, loss: 2.56173, accuracy: 0.13132\n",
            "Epoch: 1/10, step: 4590, loss: 2.56176, accuracy: 0.13132\n",
            "Epoch: 1/10, step: 4591, loss: 2.56168, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4592, loss: 2.56168, accuracy: 0.13137\n",
            "Epoch: 1/10, step: 4593, loss: 2.56159, accuracy: 0.13140\n",
            "Epoch: 1/10, step: 4594, loss: 2.56158, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4595, loss: 2.56152, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4596, loss: 2.56144, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4597, loss: 2.56147, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4598, loss: 2.56143, accuracy: 0.13150\n",
            "Epoch: 1/10, step: 4599, loss: 2.56146, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4600, loss: 2.56148, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4601, loss: 2.56144, accuracy: 0.13147\n",
            "Epoch: 1/10, step: 4602, loss: 2.56134, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4603, loss: 2.56146, accuracy: 0.13144\n",
            "Epoch: 1/10, step: 4604, loss: 2.56131, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4605, loss: 2.56119, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4606, loss: 2.56113, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4607, loss: 2.56110, accuracy: 0.13146\n",
            "Epoch: 1/10, step: 4608, loss: 2.56109, accuracy: 0.13143\n",
            "Epoch: 1/10, step: 4609, loss: 2.56116, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4610, loss: 2.56117, accuracy: 0.13145\n",
            "Epoch: 1/10, step: 4611, loss: 2.56121, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4612, loss: 2.56122, accuracy: 0.13140\n",
            "Epoch: 1/10, step: 4613, loss: 2.56119, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4614, loss: 2.56112, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4615, loss: 2.56107, accuracy: 0.13139\n",
            "Epoch: 1/10, step: 4616, loss: 2.56107, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4617, loss: 2.56103, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4618, loss: 2.56101, accuracy: 0.13142\n",
            "Epoch: 1/10, step: 4619, loss: 2.56094, accuracy: 0.13141\n",
            "Epoch: 1/10, step: 4620, loss: 2.56101, accuracy: 0.13144\n",
            "Epoch: 1/10, step: 4621, loss: 2.56086, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4622, loss: 2.56094, accuracy: 0.13149\n",
            "Epoch: 1/10, step: 4623, loss: 2.56098, accuracy: 0.13152\n",
            "Epoch: 1/10, step: 4624, loss: 2.56081, accuracy: 0.13157\n",
            "Epoch: 1/10, step: 4625, loss: 2.56075, accuracy: 0.13159\n",
            "Epoch: 1/10, step: 4626, loss: 2.56063, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4627, loss: 2.56054, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4628, loss: 2.56043, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4629, loss: 2.56042, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4630, loss: 2.56038, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4631, loss: 2.56039, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4632, loss: 2.56054, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4633, loss: 2.56048, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4634, loss: 2.56050, accuracy: 0.13174\n",
            "Epoch: 1/10, step: 4635, loss: 2.56054, accuracy: 0.13177\n",
            "Epoch: 1/10, step: 4636, loss: 2.56043, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4637, loss: 2.56035, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4638, loss: 2.56042, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4639, loss: 2.56033, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4640, loss: 2.56034, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4641, loss: 2.56043, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4642, loss: 2.56048, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4643, loss: 2.56045, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4644, loss: 2.56058, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4645, loss: 2.56049, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4646, loss: 2.56049, accuracy: 0.13178\n",
            "Epoch: 1/10, step: 4647, loss: 2.56059, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4648, loss: 2.56056, accuracy: 0.13178\n",
            "Epoch: 1/10, step: 4649, loss: 2.56056, accuracy: 0.13178\n",
            "Epoch: 1/10, step: 4650, loss: 2.56067, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4651, loss: 2.56075, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4652, loss: 2.56079, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4653, loss: 2.56073, accuracy: 0.13169\n",
            "Epoch: 1/10, step: 4654, loss: 2.56069, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4655, loss: 2.56077, accuracy: 0.13169\n",
            "Epoch: 1/10, step: 4656, loss: 2.56077, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4657, loss: 2.56074, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4658, loss: 2.56070, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4659, loss: 2.56061, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4660, loss: 2.56058, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4661, loss: 2.56045, accuracy: 0.13181\n",
            "Epoch: 1/10, step: 4662, loss: 2.56045, accuracy: 0.13184\n",
            "Epoch: 1/10, step: 4663, loss: 2.56042, accuracy: 0.13184\n",
            "Epoch: 1/10, step: 4664, loss: 2.56039, accuracy: 0.13181\n",
            "Epoch: 1/10, step: 4665, loss: 2.56035, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4666, loss: 2.56036, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4667, loss: 2.56025, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4668, loss: 2.56032, accuracy: 0.13180\n",
            "Epoch: 1/10, step: 4669, loss: 2.56041, accuracy: 0.13177\n",
            "Epoch: 1/10, step: 4670, loss: 2.56042, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4671, loss: 2.56043, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4672, loss: 2.56038, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4673, loss: 2.56038, accuracy: 0.13174\n",
            "Epoch: 1/10, step: 4674, loss: 2.56038, accuracy: 0.13174\n",
            "Epoch: 1/10, step: 4675, loss: 2.56037, accuracy: 0.13174\n",
            "Epoch: 1/10, step: 4676, loss: 2.56038, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4677, loss: 2.56039, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4678, loss: 2.56036, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4679, loss: 2.56043, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4680, loss: 2.56050, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4681, loss: 2.56046, accuracy: 0.13162\n",
            "Epoch: 1/10, step: 4682, loss: 2.56029, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4683, loss: 2.56038, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4684, loss: 2.56048, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4685, loss: 2.56035, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4686, loss: 2.56039, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4687, loss: 2.56043, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4688, loss: 2.56038, accuracy: 0.13172\n",
            "Epoch: 1/10, step: 4689, loss: 2.56024, accuracy: 0.13169\n",
            "Epoch: 1/10, step: 4690, loss: 2.56027, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4691, loss: 2.56033, accuracy: 0.13164\n",
            "Epoch: 1/10, step: 4692, loss: 2.56039, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4693, loss: 2.56042, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4694, loss: 2.56043, accuracy: 0.13163\n",
            "Epoch: 1/10, step: 4695, loss: 2.56048, accuracy: 0.13160\n",
            "Epoch: 1/10, step: 4696, loss: 2.56044, accuracy: 0.13163\n",
            "Epoch: 1/10, step: 4697, loss: 2.56039, accuracy: 0.13163\n",
            "Epoch: 1/10, step: 4698, loss: 2.56030, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4699, loss: 2.56020, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4700, loss: 2.56019, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4701, loss: 2.56012, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4702, loss: 2.56019, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4703, loss: 2.56017, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4704, loss: 2.56027, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4705, loss: 2.56035, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4706, loss: 2.56036, accuracy: 0.13164\n",
            "Epoch: 1/10, step: 4707, loss: 2.56034, accuracy: 0.13164\n",
            "Epoch: 1/10, step: 4708, loss: 2.56034, accuracy: 0.13161\n",
            "Epoch: 1/10, step: 4709, loss: 2.56022, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4710, loss: 2.56020, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4711, loss: 2.56030, accuracy: 0.13163\n",
            "Epoch: 1/10, step: 4712, loss: 2.56029, accuracy: 0.13163\n",
            "Epoch: 1/10, step: 4713, loss: 2.56037, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4714, loss: 2.56030, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4715, loss: 2.56025, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4716, loss: 2.56025, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4717, loss: 2.56023, accuracy: 0.13165\n",
            "Epoch: 1/10, step: 4718, loss: 2.56035, accuracy: 0.13162\n",
            "Epoch: 1/10, step: 4719, loss: 2.56025, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4720, loss: 2.56022, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4721, loss: 2.56020, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4722, loss: 2.56021, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4723, loss: 2.56015, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4724, loss: 2.56019, accuracy: 0.13164\n",
            "Epoch: 1/10, step: 4725, loss: 2.56018, accuracy: 0.13167\n",
            "Epoch: 1/10, step: 4726, loss: 2.56007, accuracy: 0.13169\n",
            "Epoch: 1/10, step: 4727, loss: 2.55997, accuracy: 0.13169\n",
            "Epoch: 1/10, step: 4728, loss: 2.56013, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4729, loss: 2.56016, accuracy: 0.13166\n",
            "Epoch: 1/10, step: 4730, loss: 2.56009, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4731, loss: 2.56002, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4732, loss: 2.56001, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4733, loss: 2.55996, accuracy: 0.13168\n",
            "Epoch: 1/10, step: 4734, loss: 2.55987, accuracy: 0.13171\n",
            "Epoch: 1/10, step: 4735, loss: 2.55984, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4736, loss: 2.55981, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4737, loss: 2.55984, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4738, loss: 2.55984, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4739, loss: 2.55981, accuracy: 0.13173\n",
            "Epoch: 1/10, step: 4740, loss: 2.55978, accuracy: 0.13170\n",
            "Epoch: 1/10, step: 4741, loss: 2.55961, accuracy: 0.13175\n",
            "Epoch: 1/10, step: 4742, loss: 2.55943, accuracy: 0.13180\n",
            "Epoch: 1/10, step: 4743, loss: 2.55936, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4744, loss: 2.55941, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4745, loss: 2.55939, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4746, loss: 2.55928, accuracy: 0.13193\n",
            "Epoch: 1/10, step: 4747, loss: 2.55937, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4748, loss: 2.55928, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4749, loss: 2.55933, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4750, loss: 2.55929, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4751, loss: 2.55931, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4752, loss: 2.55944, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4753, loss: 2.55952, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4754, loss: 2.55945, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4755, loss: 2.55943, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4756, loss: 2.55939, accuracy: 0.13191\n",
            "Epoch: 1/10, step: 4757, loss: 2.55941, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4758, loss: 2.55946, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4759, loss: 2.55946, accuracy: 0.13186\n",
            "Epoch: 1/10, step: 4760, loss: 2.55941, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4761, loss: 2.55933, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4762, loss: 2.55926, accuracy: 0.13180\n",
            "Epoch: 1/10, step: 4763, loss: 2.55923, accuracy: 0.13182\n",
            "Epoch: 1/10, step: 4764, loss: 2.55926, accuracy: 0.13182\n",
            "Epoch: 1/10, step: 4765, loss: 2.55912, accuracy: 0.13187\n",
            "Epoch: 1/10, step: 4766, loss: 2.55916, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4767, loss: 2.55905, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4768, loss: 2.55910, accuracy: 0.13189\n",
            "Epoch: 1/10, step: 4769, loss: 2.55917, accuracy: 0.13189\n",
            "Epoch: 1/10, step: 4770, loss: 2.55912, accuracy: 0.13189\n",
            "Epoch: 1/10, step: 4771, loss: 2.55912, accuracy: 0.13186\n",
            "Epoch: 1/10, step: 4772, loss: 2.55931, accuracy: 0.13186\n",
            "Epoch: 1/10, step: 4773, loss: 2.55945, accuracy: 0.13184\n",
            "Epoch: 1/10, step: 4774, loss: 2.55951, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4775, loss: 2.55948, accuracy: 0.13186\n",
            "Epoch: 1/10, step: 4776, loss: 2.55950, accuracy: 0.13183\n",
            "Epoch: 1/10, step: 4777, loss: 2.55947, accuracy: 0.13186\n",
            "Epoch: 1/10, step: 4778, loss: 2.55952, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4779, loss: 2.55953, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4780, loss: 2.55949, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4781, loss: 2.55945, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4782, loss: 2.55949, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4783, loss: 2.55954, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4784, loss: 2.55951, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4785, loss: 2.55950, accuracy: 0.13192\n",
            "Epoch: 1/10, step: 4786, loss: 2.55946, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4787, loss: 2.55954, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4788, loss: 2.55956, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4789, loss: 2.55964, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4790, loss: 2.55983, accuracy: 0.13192\n",
            "Epoch: 1/10, step: 4791, loss: 2.55976, accuracy: 0.13191\n",
            "Epoch: 1/10, step: 4792, loss: 2.55985, accuracy: 0.13189\n",
            "Epoch: 1/10, step: 4793, loss: 2.55992, accuracy: 0.13189\n",
            "Epoch: 1/10, step: 4794, loss: 2.55981, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4795, loss: 2.55978, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4796, loss: 2.55985, accuracy: 0.13193\n",
            "Epoch: 1/10, step: 4797, loss: 2.55980, accuracy: 0.13191\n",
            "Epoch: 1/10, step: 4798, loss: 2.55973, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4799, loss: 2.55975, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4800, loss: 2.55984, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4801, loss: 2.55989, accuracy: 0.13182\n",
            "Epoch: 1/10, step: 4802, loss: 2.55997, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4803, loss: 2.55988, accuracy: 0.13184\n",
            "Epoch: 1/10, step: 4804, loss: 2.55992, accuracy: 0.13184\n",
            "Epoch: 1/10, step: 4805, loss: 2.55990, accuracy: 0.13182\n",
            "Epoch: 1/10, step: 4806, loss: 2.55995, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4807, loss: 2.55992, accuracy: 0.13181\n",
            "Epoch: 1/10, step: 4808, loss: 2.55998, accuracy: 0.13179\n",
            "Epoch: 1/10, step: 4809, loss: 2.56022, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4810, loss: 2.56020, accuracy: 0.13176\n",
            "Epoch: 1/10, step: 4811, loss: 2.56017, accuracy: 0.13181\n",
            "Epoch: 1/10, step: 4812, loss: 2.56013, accuracy: 0.13181\n",
            "Epoch: 1/10, step: 4813, loss: 2.56014, accuracy: 0.13180\n",
            "Epoch: 1/10, step: 4814, loss: 2.56005, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4815, loss: 2.56006, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4816, loss: 2.56008, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4817, loss: 2.56005, accuracy: 0.13188\n",
            "Epoch: 1/10, step: 4818, loss: 2.56001, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4819, loss: 2.55999, accuracy: 0.13185\n",
            "Epoch: 1/10, step: 4820, loss: 2.55983, accuracy: 0.13190\n",
            "Epoch: 1/10, step: 4821, loss: 2.55980, accuracy: 0.13192\n",
            "Epoch: 1/10, step: 4822, loss: 2.55969, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4823, loss: 2.55983, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4824, loss: 2.55991, accuracy: 0.13192\n",
            "Epoch: 1/10, step: 4825, loss: 2.55985, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4826, loss: 2.55984, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4827, loss: 2.55969, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4828, loss: 2.55969, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4829, loss: 2.55972, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4830, loss: 2.55962, accuracy: 0.13204\n",
            "Epoch: 1/10, step: 4831, loss: 2.55960, accuracy: 0.13204\n",
            "Epoch: 1/10, step: 4832, loss: 2.55952, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4833, loss: 2.55961, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4834, loss: 2.55959, accuracy: 0.13209\n",
            "Epoch: 1/10, step: 4835, loss: 2.55972, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4836, loss: 2.55967, accuracy: 0.13203\n",
            "Epoch: 1/10, step: 4837, loss: 2.55974, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4838, loss: 2.55982, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4839, loss: 2.55983, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4840, loss: 2.55982, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4841, loss: 2.56002, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4842, loss: 2.55992, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4843, loss: 2.55991, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4844, loss: 2.56001, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4845, loss: 2.56000, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4846, loss: 2.56007, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4847, loss: 2.56007, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4848, loss: 2.56013, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4849, loss: 2.56011, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4850, loss: 2.56009, accuracy: 0.13196\n",
            "Epoch: 1/10, step: 4851, loss: 2.55997, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4852, loss: 2.55996, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4853, loss: 2.55987, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4854, loss: 2.55988, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4855, loss: 2.55988, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4856, loss: 2.55991, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4857, loss: 2.55988, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4858, loss: 2.55981, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4859, loss: 2.55980, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4860, loss: 2.55983, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4861, loss: 2.55974, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4862, loss: 2.55978, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4863, loss: 2.55966, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4864, loss: 2.55971, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4865, loss: 2.55971, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4866, loss: 2.55948, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4867, loss: 2.55950, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4868, loss: 2.55954, accuracy: 0.13204\n",
            "Epoch: 1/10, step: 4869, loss: 2.55956, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4870, loss: 2.55945, accuracy: 0.13203\n",
            "Epoch: 1/10, step: 4871, loss: 2.55945, accuracy: 0.13203\n",
            "Epoch: 1/10, step: 4872, loss: 2.55940, accuracy: 0.13203\n",
            "Epoch: 1/10, step: 4873, loss: 2.55942, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4874, loss: 2.55949, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4875, loss: 2.55946, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4876, loss: 2.55942, accuracy: 0.13200\n",
            "Epoch: 1/10, step: 4877, loss: 2.55933, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4878, loss: 2.55928, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4879, loss: 2.55919, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4880, loss: 2.55912, accuracy: 0.13204\n",
            "Epoch: 1/10, step: 4881, loss: 2.55917, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4882, loss: 2.55927, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4883, loss: 2.55918, accuracy: 0.13204\n",
            "Epoch: 1/10, step: 4884, loss: 2.55921, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4885, loss: 2.55923, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4886, loss: 2.55924, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4887, loss: 2.55919, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4888, loss: 2.55913, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4889, loss: 2.55908, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4890, loss: 2.55901, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4891, loss: 2.55908, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4892, loss: 2.55898, accuracy: 0.13198\n",
            "Epoch: 1/10, step: 4893, loss: 2.55896, accuracy: 0.13195\n",
            "Epoch: 1/10, step: 4894, loss: 2.55892, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4895, loss: 2.55891, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4896, loss: 2.55893, accuracy: 0.13197\n",
            "Epoch: 1/10, step: 4897, loss: 2.55900, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4898, loss: 2.55899, accuracy: 0.13194\n",
            "Epoch: 1/10, step: 4899, loss: 2.55899, accuracy: 0.13199\n",
            "Epoch: 1/10, step: 4900, loss: 2.55895, accuracy: 0.13202\n",
            "Epoch: 1/10, step: 4901, loss: 2.55899, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4902, loss: 2.55892, accuracy: 0.13201\n",
            "Epoch: 1/10, step: 4903, loss: 2.55890, accuracy: 0.13206\n",
            "Epoch: 1/10, step: 4904, loss: 2.55891, accuracy: 0.13209\n",
            "Epoch: 1/10, step: 4905, loss: 2.55889, accuracy: 0.13211\n",
            "Epoch: 1/10, step: 4906, loss: 2.55881, accuracy: 0.13211\n",
            "Epoch: 1/10, step: 4907, loss: 2.55881, accuracy: 0.13211\n",
            "Epoch: 1/10, step: 4908, loss: 2.55878, accuracy: 0.13211\n",
            "Epoch: 1/10, step: 4909, loss: 2.55883, accuracy: 0.13210\n",
            "Epoch: 1/10, step: 4910, loss: 2.55877, accuracy: 0.13213\n",
            "Epoch: 1/10, step: 4911, loss: 2.55869, accuracy: 0.13218\n",
            "Epoch: 1/10, step: 4912, loss: 2.55863, accuracy: 0.13218\n",
            "Epoch: 1/10, step: 4913, loss: 2.55862, accuracy: 0.13215\n",
            "Epoch: 1/10, step: 4914, loss: 2.55850, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4915, loss: 2.55842, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4916, loss: 2.55847, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4917, loss: 2.55853, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4918, loss: 2.55853, accuracy: 0.13214\n",
            "Epoch: 1/10, step: 4919, loss: 2.55843, accuracy: 0.13214\n",
            "Epoch: 1/10, step: 4920, loss: 2.55842, accuracy: 0.13216\n",
            "Epoch: 1/10, step: 4921, loss: 2.55839, accuracy: 0.13219\n",
            "Epoch: 1/10, step: 4922, loss: 2.55839, accuracy: 0.13216\n",
            "Epoch: 1/10, step: 4923, loss: 2.55830, accuracy: 0.13219\n",
            "Epoch: 1/10, step: 4924, loss: 2.55827, accuracy: 0.13218\n",
            "Epoch: 1/10, step: 4925, loss: 2.55822, accuracy: 0.13218\n",
            "Epoch: 1/10, step: 4926, loss: 2.55812, accuracy: 0.13221\n",
            "Epoch: 1/10, step: 4927, loss: 2.55811, accuracy: 0.13221\n",
            "Epoch: 1/10, step: 4928, loss: 2.55818, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4929, loss: 2.55807, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4930, loss: 2.55808, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4931, loss: 2.55804, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4932, loss: 2.55797, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4933, loss: 2.55802, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4934, loss: 2.55797, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4935, loss: 2.55799, accuracy: 0.13214\n",
            "Epoch: 1/10, step: 4936, loss: 2.55791, accuracy: 0.13219\n",
            "Epoch: 1/10, step: 4937, loss: 2.55791, accuracy: 0.13222\n",
            "Epoch: 1/10, step: 4938, loss: 2.55785, accuracy: 0.13224\n",
            "Epoch: 1/10, step: 4939, loss: 2.55778, accuracy: 0.13226\n",
            "Epoch: 1/10, step: 4940, loss: 2.55778, accuracy: 0.13224\n",
            "Epoch: 1/10, step: 4941, loss: 2.55770, accuracy: 0.13224\n",
            "Epoch: 1/10, step: 4942, loss: 2.55774, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4943, loss: 2.55776, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4944, loss: 2.55782, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4945, loss: 2.55779, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4946, loss: 2.55768, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4947, loss: 2.55763, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4948, loss: 2.55772, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4949, loss: 2.55778, accuracy: 0.13222\n",
            "Epoch: 1/10, step: 4950, loss: 2.55776, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4951, loss: 2.55770, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4952, loss: 2.55772, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4953, loss: 2.55777, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4954, loss: 2.55770, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4955, loss: 2.55773, accuracy: 0.13229\n",
            "Epoch: 1/10, step: 4956, loss: 2.55778, accuracy: 0.13226\n",
            "Epoch: 1/10, step: 4957, loss: 2.55778, accuracy: 0.13229\n",
            "Epoch: 1/10, step: 4958, loss: 2.55775, accuracy: 0.13231\n",
            "Epoch: 1/10, step: 4959, loss: 2.55779, accuracy: 0.13234\n",
            "Epoch: 1/10, step: 4960, loss: 2.55785, accuracy: 0.13231\n",
            "Epoch: 1/10, step: 4961, loss: 2.55785, accuracy: 0.13228\n",
            "Epoch: 1/10, step: 4962, loss: 2.55779, accuracy: 0.13231\n",
            "Epoch: 1/10, step: 4963, loss: 2.55773, accuracy: 0.13233\n",
            "Epoch: 1/10, step: 4964, loss: 2.55778, accuracy: 0.13233\n",
            "Epoch: 1/10, step: 4965, loss: 2.55772, accuracy: 0.13235\n",
            "Epoch: 1/10, step: 4966, loss: 2.55774, accuracy: 0.13235\n",
            "Epoch: 1/10, step: 4967, loss: 2.55777, accuracy: 0.13235\n",
            "Epoch: 1/10, step: 4968, loss: 2.55783, accuracy: 0.13232\n",
            "Epoch: 1/10, step: 4969, loss: 2.55783, accuracy: 0.13230\n",
            "Epoch: 1/10, step: 4970, loss: 2.55774, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4971, loss: 2.55767, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4972, loss: 2.55761, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4973, loss: 2.55763, accuracy: 0.13224\n",
            "Epoch: 1/10, step: 4974, loss: 2.55761, accuracy: 0.13226\n",
            "Epoch: 1/10, step: 4975, loss: 2.55758, accuracy: 0.13224\n",
            "Epoch: 1/10, step: 4976, loss: 2.55760, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4977, loss: 2.55757, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4978, loss: 2.55752, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4979, loss: 2.55756, accuracy: 0.13221\n",
            "Epoch: 1/10, step: 4980, loss: 2.55764, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4981, loss: 2.55756, accuracy: 0.13223\n",
            "Epoch: 1/10, step: 4982, loss: 2.55756, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4983, loss: 2.55747, accuracy: 0.13220\n",
            "Epoch: 1/10, step: 4984, loss: 2.55761, accuracy: 0.13217\n",
            "Epoch: 1/10, step: 4985, loss: 2.55748, accuracy: 0.13222\n",
            "Epoch: 1/10, step: 4986, loss: 2.55745, accuracy: 0.13225\n",
            "Epoch: 1/10, step: 4987, loss: 2.55739, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4988, loss: 2.55735, accuracy: 0.13227\n",
            "Epoch: 1/10, step: 4989, loss: 2.55737, accuracy: 0.13229\n",
            "Epoch: 1/10, step: 4990, loss: 2.55725, accuracy: 0.13234\n",
            "Epoch: 1/10, step: 4991, loss: 2.55721, accuracy: 0.13234\n",
            "Epoch: 1/10, step: 4992, loss: 2.55717, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 4993, loss: 2.55699, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 4994, loss: 2.55699, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 4995, loss: 2.55705, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 4996, loss: 2.55706, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 4997, loss: 2.55712, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 4998, loss: 2.55705, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 4999, loss: 2.55702, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 5000, loss: 2.55703, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 5001, loss: 2.55690, accuracy: 0.13250\n",
            "Epoch: 1/10, step: 5002, loss: 2.55695, accuracy: 0.13247\n",
            "Epoch: 1/10, step: 5003, loss: 2.55697, accuracy: 0.13247\n",
            "Epoch: 1/10, step: 5004, loss: 2.55687, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5005, loss: 2.55682, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5006, loss: 2.55695, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5007, loss: 2.55683, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5008, loss: 2.55673, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5009, loss: 2.55673, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5010, loss: 2.55668, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5011, loss: 2.55679, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5012, loss: 2.55676, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5013, loss: 2.55683, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5014, loss: 2.55681, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5015, loss: 2.55689, accuracy: 0.13250\n",
            "Epoch: 1/10, step: 5016, loss: 2.55683, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5017, loss: 2.55686, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5018, loss: 2.55682, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5019, loss: 2.55696, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5020, loss: 2.55689, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5021, loss: 2.55698, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5022, loss: 2.55696, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5023, loss: 2.55702, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5024, loss: 2.55699, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5025, loss: 2.55697, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5026, loss: 2.55691, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5027, loss: 2.55695, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5028, loss: 2.55694, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5029, loss: 2.55693, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5030, loss: 2.55692, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5031, loss: 2.55690, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5032, loss: 2.55684, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5033, loss: 2.55680, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5034, loss: 2.55676, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5035, loss: 2.55670, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5036, loss: 2.55670, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5037, loss: 2.55674, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5038, loss: 2.55678, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5039, loss: 2.55674, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5040, loss: 2.55677, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5041, loss: 2.55680, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5042, loss: 2.55682, accuracy: 0.13246\n",
            "Epoch: 1/10, step: 5043, loss: 2.55665, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5044, loss: 2.55667, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5045, loss: 2.55659, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5046, loss: 2.55672, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 5047, loss: 2.55676, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5048, loss: 2.55673, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5049, loss: 2.55669, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5050, loss: 2.55665, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5051, loss: 2.55663, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5052, loss: 2.55661, accuracy: 0.13242\n",
            "Epoch: 1/10, step: 5053, loss: 2.55652, accuracy: 0.13245\n",
            "Epoch: 1/10, step: 5054, loss: 2.55660, accuracy: 0.13244\n",
            "Epoch: 1/10, step: 5055, loss: 2.55652, accuracy: 0.13242\n",
            "Epoch: 1/10, step: 5056, loss: 2.55647, accuracy: 0.13244\n",
            "Epoch: 1/10, step: 5057, loss: 2.55645, accuracy: 0.13244\n",
            "Epoch: 1/10, step: 5058, loss: 2.55643, accuracy: 0.13244\n",
            "Epoch: 1/10, step: 5059, loss: 2.55643, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5060, loss: 2.55649, accuracy: 0.13239\n",
            "Epoch: 1/10, step: 5061, loss: 2.55652, accuracy: 0.13236\n",
            "Epoch: 1/10, step: 5062, loss: 2.55645, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5063, loss: 2.55650, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5064, loss: 2.55640, accuracy: 0.13243\n",
            "Epoch: 1/10, step: 5065, loss: 2.55646, accuracy: 0.13243\n",
            "Epoch: 1/10, step: 5066, loss: 2.55645, accuracy: 0.13243\n",
            "Epoch: 1/10, step: 5067, loss: 2.55638, accuracy: 0.13240\n",
            "Epoch: 1/10, step: 5068, loss: 2.55636, accuracy: 0.13240\n",
            "Epoch: 1/10, step: 5069, loss: 2.55645, accuracy: 0.13237\n",
            "Epoch: 1/10, step: 5070, loss: 2.55636, accuracy: 0.13237\n",
            "Epoch: 1/10, step: 5071, loss: 2.55631, accuracy: 0.13237\n",
            "Epoch: 1/10, step: 5072, loss: 2.55624, accuracy: 0.13239\n",
            "Epoch: 1/10, step: 5073, loss: 2.55624, accuracy: 0.13239\n",
            "Epoch: 1/10, step: 5074, loss: 2.55629, accuracy: 0.13237\n",
            "Epoch: 1/10, step: 5075, loss: 2.55623, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5076, loss: 2.55621, accuracy: 0.13239\n",
            "Epoch: 1/10, step: 5077, loss: 2.55614, accuracy: 0.13244\n",
            "Epoch: 1/10, step: 5078, loss: 2.55610, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5079, loss: 2.55617, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5080, loss: 2.55611, accuracy: 0.13241\n",
            "Epoch: 1/10, step: 5081, loss: 2.55604, accuracy: 0.13243\n",
            "Epoch: 1/10, step: 5082, loss: 2.55599, accuracy: 0.13248\n",
            "Epoch: 1/10, step: 5083, loss: 2.55600, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5084, loss: 2.55595, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5085, loss: 2.55600, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5086, loss: 2.55594, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5087, loss: 2.55592, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5088, loss: 2.55590, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5089, loss: 2.55590, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5090, loss: 2.55597, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5091, loss: 2.55595, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5092, loss: 2.55588, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5093, loss: 2.55589, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5094, loss: 2.55585, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5095, loss: 2.55583, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5096, loss: 2.55581, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5097, loss: 2.55584, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5098, loss: 2.55583, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5099, loss: 2.55589, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5100, loss: 2.55588, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5101, loss: 2.55587, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5102, loss: 2.55584, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5103, loss: 2.55578, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5104, loss: 2.55580, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5105, loss: 2.55575, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5106, loss: 2.55574, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5107, loss: 2.55568, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5108, loss: 2.55564, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5109, loss: 2.55545, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5110, loss: 2.55539, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5111, loss: 2.55538, accuracy: 0.13266\n",
            "Epoch: 1/10, step: 5112, loss: 2.55546, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5113, loss: 2.55537, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5114, loss: 2.55550, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5115, loss: 2.55548, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5116, loss: 2.55540, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5117, loss: 2.55542, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5118, loss: 2.55539, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5119, loss: 2.55535, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5120, loss: 2.55541, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5121, loss: 2.55533, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5122, loss: 2.55531, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5123, loss: 2.55542, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5124, loss: 2.55530, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5125, loss: 2.55529, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5126, loss: 2.55529, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5127, loss: 2.55521, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5128, loss: 2.55515, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5129, loss: 2.55516, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5130, loss: 2.55507, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5131, loss: 2.55498, accuracy: 0.13272\n",
            "Epoch: 1/10, step: 5132, loss: 2.55502, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5133, loss: 2.55495, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5134, loss: 2.55491, accuracy: 0.13272\n",
            "Epoch: 1/10, step: 5135, loss: 2.55500, accuracy: 0.13272\n",
            "Epoch: 1/10, step: 5136, loss: 2.55507, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5137, loss: 2.55493, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5138, loss: 2.55497, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5139, loss: 2.55494, accuracy: 0.13266\n",
            "Epoch: 1/10, step: 5140, loss: 2.55492, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5141, loss: 2.55490, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5142, loss: 2.55498, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5143, loss: 2.55500, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5144, loss: 2.55510, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5145, loss: 2.55511, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5146, loss: 2.55510, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5147, loss: 2.55505, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5148, loss: 2.55505, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5149, loss: 2.55500, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5150, loss: 2.55504, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5151, loss: 2.55501, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5152, loss: 2.55502, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5153, loss: 2.55502, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5154, loss: 2.55502, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5155, loss: 2.55504, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5156, loss: 2.55494, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5157, loss: 2.55497, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5158, loss: 2.55497, accuracy: 0.13266\n",
            "Epoch: 1/10, step: 5159, loss: 2.55495, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5160, loss: 2.55485, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5161, loss: 2.55477, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5162, loss: 2.55480, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5163, loss: 2.55486, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5164, loss: 2.55498, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5165, loss: 2.55497, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5166, loss: 2.55499, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5167, loss: 2.55496, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5168, loss: 2.55499, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5169, loss: 2.55494, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5170, loss: 2.55505, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5171, loss: 2.55502, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5172, loss: 2.55497, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5173, loss: 2.55501, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5174, loss: 2.55507, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5175, loss: 2.55500, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5176, loss: 2.55507, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5177, loss: 2.55494, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5178, loss: 2.55488, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5179, loss: 2.55481, accuracy: 0.13258\n",
            "Epoch: 1/10, step: 5180, loss: 2.55487, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5181, loss: 2.55482, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5182, loss: 2.55488, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5183, loss: 2.55494, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5184, loss: 2.55488, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5185, loss: 2.55483, accuracy: 0.13250\n",
            "Epoch: 1/10, step: 5186, loss: 2.55483, accuracy: 0.13247\n",
            "Epoch: 1/10, step: 5187, loss: 2.55483, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5188, loss: 2.55486, accuracy: 0.13249\n",
            "Epoch: 1/10, step: 5189, loss: 2.55475, accuracy: 0.13252\n",
            "Epoch: 1/10, step: 5190, loss: 2.55470, accuracy: 0.13251\n",
            "Epoch: 1/10, step: 5191, loss: 2.55466, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5192, loss: 2.55459, accuracy: 0.13254\n",
            "Epoch: 1/10, step: 5193, loss: 2.55457, accuracy: 0.13253\n",
            "Epoch: 1/10, step: 5194, loss: 2.55465, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5195, loss: 2.55465, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5196, loss: 2.55464, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5197, loss: 2.55466, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5198, loss: 2.55471, accuracy: 0.13255\n",
            "Epoch: 1/10, step: 5199, loss: 2.55463, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5200, loss: 2.55456, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5201, loss: 2.55450, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5202, loss: 2.55445, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5203, loss: 2.55434, accuracy: 0.13266\n",
            "Epoch: 1/10, step: 5204, loss: 2.55431, accuracy: 0.13264\n",
            "Epoch: 1/10, step: 5205, loss: 2.55414, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5206, loss: 2.55408, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5207, loss: 2.55400, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5208, loss: 2.55398, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5209, loss: 2.55397, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5210, loss: 2.55405, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5211, loss: 2.55400, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5212, loss: 2.55391, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5213, loss: 2.55400, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5214, loss: 2.55396, accuracy: 0.13279\n",
            "Epoch: 1/10, step: 5215, loss: 2.55399, accuracy: 0.13277\n",
            "Epoch: 1/10, step: 5216, loss: 2.55405, accuracy: 0.13276\n",
            "Epoch: 1/10, step: 5217, loss: 2.55404, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5218, loss: 2.55403, accuracy: 0.13276\n",
            "Epoch: 1/10, step: 5219, loss: 2.55410, accuracy: 0.13276\n",
            "Epoch: 1/10, step: 5220, loss: 2.55422, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5221, loss: 2.55424, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5222, loss: 2.55426, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5223, loss: 2.55425, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5224, loss: 2.55416, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5225, loss: 2.55425, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5226, loss: 2.55434, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5227, loss: 2.55424, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5228, loss: 2.55439, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5229, loss: 2.55436, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5230, loss: 2.55437, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5231, loss: 2.55441, accuracy: 0.13260\n",
            "Epoch: 1/10, step: 5232, loss: 2.55447, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5233, loss: 2.55441, accuracy: 0.13262\n",
            "Epoch: 1/10, step: 5234, loss: 2.55452, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5235, loss: 2.55456, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5236, loss: 2.55453, accuracy: 0.13257\n",
            "Epoch: 1/10, step: 5237, loss: 2.55449, accuracy: 0.13259\n",
            "Epoch: 1/10, step: 5238, loss: 2.55452, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5239, loss: 2.55447, accuracy: 0.13256\n",
            "Epoch: 1/10, step: 5240, loss: 2.55438, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5241, loss: 2.55435, accuracy: 0.13263\n",
            "Epoch: 1/10, step: 5242, loss: 2.55431, accuracy: 0.13261\n",
            "Epoch: 1/10, step: 5243, loss: 2.55421, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5244, loss: 2.55413, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5245, loss: 2.55420, accuracy: 0.13265\n",
            "Epoch: 1/10, step: 5246, loss: 2.55408, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5247, loss: 2.55406, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5248, loss: 2.55397, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5249, loss: 2.55395, accuracy: 0.13276\n",
            "Epoch: 1/10, step: 5250, loss: 2.55395, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5251, loss: 2.55380, accuracy: 0.13278\n",
            "Epoch: 1/10, step: 5252, loss: 2.55378, accuracy: 0.13281\n",
            "Epoch: 1/10, step: 5253, loss: 2.55367, accuracy: 0.13283\n",
            "Epoch: 1/10, step: 5254, loss: 2.55364, accuracy: 0.13280\n",
            "Epoch: 1/10, step: 5255, loss: 2.55379, accuracy: 0.13278\n",
            "Epoch: 1/10, step: 5256, loss: 2.55378, accuracy: 0.13280\n",
            "Epoch: 1/10, step: 5257, loss: 2.55369, accuracy: 0.13280\n",
            "Epoch: 1/10, step: 5258, loss: 2.55378, accuracy: 0.13277\n",
            "Epoch: 1/10, step: 5259, loss: 2.55376, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5260, loss: 2.55366, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5261, loss: 2.55366, accuracy: 0.13277\n",
            "Epoch: 1/10, step: 5262, loss: 2.55368, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5263, loss: 2.55356, accuracy: 0.13277\n",
            "Epoch: 1/10, step: 5264, loss: 2.55352, accuracy: 0.13277\n",
            "Epoch: 1/10, step: 5265, loss: 2.55354, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5266, loss: 2.55354, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5267, loss: 2.55364, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5268, loss: 2.55369, accuracy: 0.13274\n",
            "Epoch: 1/10, step: 5269, loss: 2.55365, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5270, loss: 2.55362, accuracy: 0.13271\n",
            "Epoch: 1/10, step: 5271, loss: 2.55353, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5272, loss: 2.55349, accuracy: 0.13275\n",
            "Epoch: 1/10, step: 5273, loss: 2.55348, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5274, loss: 2.55347, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5275, loss: 2.55349, accuracy: 0.13268\n",
            "Epoch: 1/10, step: 5276, loss: 2.55344, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5277, loss: 2.55343, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5278, loss: 2.55346, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5279, loss: 2.55339, accuracy: 0.13270\n",
            "Epoch: 1/10, step: 5280, loss: 2.55339, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5281, loss: 2.55343, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5282, loss: 2.55335, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5283, loss: 2.55328, accuracy: 0.13267\n",
            "Epoch: 1/10, step: 5284, loss: 2.55327, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5285, loss: 2.55330, accuracy: 0.13266\n",
            "Epoch: 1/10, step: 5286, loss: 2.55327, accuracy: 0.13269\n",
            "Epoch: 1/10, step: 5287, loss: 2.55315, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5288, loss: 2.55313, accuracy: 0.13273\n",
            "Epoch: 1/10, step: 5289, loss: 2.55299, accuracy: 0.13278\n",
            "Epoch: 1/10, step: 5290, loss: 2.55294, accuracy: 0.13282\n",
            "Epoch: 1/10, step: 5291, loss: 2.55277, accuracy: 0.13289\n",
            "Epoch: 1/10, step: 5292, loss: 2.55281, accuracy: 0.13287\n",
            "Epoch: 1/10, step: 5293, loss: 2.55270, accuracy: 0.13291\n",
            "Epoch: 1/10, step: 5294, loss: 2.55271, accuracy: 0.13293\n",
            "Epoch: 1/10, step: 5295, loss: 2.55272, accuracy: 0.13291\n",
            "Epoch: 1/10, step: 5296, loss: 2.55271, accuracy: 0.13291\n",
            "Epoch: 1/10, step: 5297, loss: 2.55268, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5298, loss: 2.55269, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5299, loss: 2.55275, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5300, loss: 2.55270, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5301, loss: 2.55266, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5302, loss: 2.55265, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5303, loss: 2.55259, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5304, loss: 2.55257, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5305, loss: 2.55250, accuracy: 0.13299\n",
            "Epoch: 1/10, step: 5306, loss: 2.55250, accuracy: 0.13301\n",
            "Epoch: 1/10, step: 5307, loss: 2.55249, accuracy: 0.13298\n",
            "Epoch: 1/10, step: 5308, loss: 2.55252, accuracy: 0.13296\n",
            "Epoch: 1/10, step: 5309, loss: 2.55254, accuracy: 0.13296\n",
            "Epoch: 1/10, step: 5310, loss: 2.55254, accuracy: 0.13296\n",
            "Epoch: 1/10, step: 5311, loss: 2.55264, accuracy: 0.13298\n",
            "Epoch: 1/10, step: 5312, loss: 2.55263, accuracy: 0.13298\n",
            "Epoch: 1/10, step: 5313, loss: 2.55267, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5314, loss: 2.55278, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5315, loss: 2.55278, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5316, loss: 2.55270, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5317, loss: 2.55263, accuracy: 0.13295\n",
            "Epoch: 1/10, step: 5318, loss: 2.55268, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5319, loss: 2.55261, accuracy: 0.13297\n",
            "Epoch: 1/10, step: 5320, loss: 2.55256, accuracy: 0.13299\n",
            "Epoch: 1/10, step: 5321, loss: 2.55252, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5322, loss: 2.55261, accuracy: 0.13301\n",
            "Epoch: 1/10, step: 5323, loss: 2.55255, accuracy: 0.13298\n",
            "Epoch: 1/10, step: 5324, loss: 2.55253, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5325, loss: 2.55257, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5326, loss: 2.55264, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5327, loss: 2.55272, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5328, loss: 2.55266, accuracy: 0.13305\n",
            "Epoch: 1/10, step: 5329, loss: 2.55259, accuracy: 0.13307\n",
            "Epoch: 1/10, step: 5330, loss: 2.55257, accuracy: 0.13304\n",
            "Epoch: 1/10, step: 5331, loss: 2.55259, accuracy: 0.13304\n",
            "Epoch: 1/10, step: 5332, loss: 2.55262, accuracy: 0.13302\n",
            "Epoch: 1/10, step: 5333, loss: 2.55262, accuracy: 0.13302\n",
            "Epoch: 1/10, step: 5334, loss: 2.55260, accuracy: 0.13301\n",
            "Epoch: 1/10, step: 5335, loss: 2.55262, accuracy: 0.13301\n",
            "Epoch: 1/10, step: 5336, loss: 2.55263, accuracy: 0.13301\n",
            "Epoch: 1/10, step: 5337, loss: 2.55259, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5338, loss: 2.55255, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5339, loss: 2.55259, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5340, loss: 2.55269, accuracy: 0.13303\n",
            "Epoch: 1/10, step: 5341, loss: 2.55252, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5342, loss: 2.55249, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5343, loss: 2.55250, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5344, loss: 2.55249, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5345, loss: 2.55245, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5346, loss: 2.55249, accuracy: 0.13309\n",
            "Epoch: 1/10, step: 5347, loss: 2.55233, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5348, loss: 2.55229, accuracy: 0.13313\n",
            "Epoch: 1/10, step: 5349, loss: 2.55220, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5350, loss: 2.55210, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5351, loss: 2.55213, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5352, loss: 2.55215, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5353, loss: 2.55210, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5354, loss: 2.55216, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5355, loss: 2.55215, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5356, loss: 2.55211, accuracy: 0.13315\n",
            "Epoch: 1/10, step: 5357, loss: 2.55202, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5358, loss: 2.55203, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5359, loss: 2.55195, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5360, loss: 2.55205, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5361, loss: 2.55209, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5362, loss: 2.55206, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5363, loss: 2.55201, accuracy: 0.13313\n",
            "Epoch: 1/10, step: 5364, loss: 2.55197, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5365, loss: 2.55184, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5366, loss: 2.55190, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5367, loss: 2.55194, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5368, loss: 2.55194, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5369, loss: 2.55192, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5370, loss: 2.55194, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5371, loss: 2.55190, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5372, loss: 2.55179, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5373, loss: 2.55168, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5374, loss: 2.55175, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5375, loss: 2.55162, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5376, loss: 2.55167, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5377, loss: 2.55172, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5378, loss: 2.55169, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5379, loss: 2.55169, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5380, loss: 2.55170, accuracy: 0.13313\n",
            "Epoch: 1/10, step: 5381, loss: 2.55167, accuracy: 0.13311\n",
            "Epoch: 1/10, step: 5382, loss: 2.55160, accuracy: 0.13311\n",
            "Epoch: 1/10, step: 5383, loss: 2.55165, accuracy: 0.13308\n",
            "Epoch: 1/10, step: 5384, loss: 2.55162, accuracy: 0.13308\n",
            "Epoch: 1/10, step: 5385, loss: 2.55168, accuracy: 0.13308\n",
            "Epoch: 1/10, step: 5386, loss: 2.55161, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5387, loss: 2.55162, accuracy: 0.13312\n",
            "Epoch: 1/10, step: 5388, loss: 2.55157, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5389, loss: 2.55153, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5390, loss: 2.55145, accuracy: 0.13316\n",
            "Epoch: 1/10, step: 5391, loss: 2.55135, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5392, loss: 2.55120, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5393, loss: 2.55116, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5394, loss: 2.55113, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5395, loss: 2.55104, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5396, loss: 2.55092, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5397, loss: 2.55096, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5398, loss: 2.55095, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5399, loss: 2.55083, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5400, loss: 2.55076, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5401, loss: 2.55069, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5402, loss: 2.55068, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5403, loss: 2.55071, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5404, loss: 2.55071, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5405, loss: 2.55062, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5406, loss: 2.55065, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5407, loss: 2.55065, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5408, loss: 2.55063, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5409, loss: 2.55069, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5410, loss: 2.55062, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5411, loss: 2.55054, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5412, loss: 2.55041, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5413, loss: 2.55037, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5414, loss: 2.55048, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5415, loss: 2.55043, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5416, loss: 2.55049, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5417, loss: 2.55034, accuracy: 0.13338\n",
            "Epoch: 1/10, step: 5418, loss: 2.55035, accuracy: 0.13337\n",
            "Epoch: 1/10, step: 5419, loss: 2.55028, accuracy: 0.13340\n",
            "Epoch: 1/10, step: 5420, loss: 2.55028, accuracy: 0.13339\n",
            "Epoch: 1/10, step: 5421, loss: 2.55023, accuracy: 0.13337\n",
            "Epoch: 1/10, step: 5422, loss: 2.55024, accuracy: 0.13339\n",
            "Epoch: 1/10, step: 5423, loss: 2.55031, accuracy: 0.13337\n",
            "Epoch: 1/10, step: 5424, loss: 2.55035, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5425, loss: 2.55039, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5426, loss: 2.55041, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5427, loss: 2.55042, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5428, loss: 2.55043, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5429, loss: 2.55045, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5430, loss: 2.55042, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5431, loss: 2.55046, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5432, loss: 2.55037, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5433, loss: 2.55034, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5434, loss: 2.55034, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5435, loss: 2.55029, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5436, loss: 2.55022, accuracy: 0.13337\n",
            "Epoch: 1/10, step: 5437, loss: 2.55023, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5438, loss: 2.55025, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5439, loss: 2.55025, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5440, loss: 2.55017, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5441, loss: 2.55023, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5442, loss: 2.55019, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5443, loss: 2.55024, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5444, loss: 2.55031, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5445, loss: 2.55035, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5446, loss: 2.55040, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5447, loss: 2.55052, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5448, loss: 2.55050, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5449, loss: 2.55049, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5450, loss: 2.55058, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5451, loss: 2.55058, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5452, loss: 2.55058, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5453, loss: 2.55050, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5454, loss: 2.55046, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5455, loss: 2.55048, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5456, loss: 2.55039, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5457, loss: 2.55030, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5458, loss: 2.55021, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5459, loss: 2.55018, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5460, loss: 2.55015, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5461, loss: 2.55009, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5462, loss: 2.55004, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5463, loss: 2.55011, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5464, loss: 2.55006, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5465, loss: 2.55013, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5466, loss: 2.55015, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5467, loss: 2.55014, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5468, loss: 2.55013, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5469, loss: 2.55014, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5470, loss: 2.55016, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5471, loss: 2.55006, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5472, loss: 2.55002, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5473, loss: 2.55000, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5474, loss: 2.54992, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5475, loss: 2.55005, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5476, loss: 2.55004, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5477, loss: 2.54994, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5478, loss: 2.54992, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5479, loss: 2.54985, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5480, loss: 2.54985, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5481, loss: 2.54992, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5482, loss: 2.54993, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5483, loss: 2.54987, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5484, loss: 2.54985, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5485, loss: 2.54980, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5486, loss: 2.54991, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5487, loss: 2.54988, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5488, loss: 2.54995, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5489, loss: 2.54999, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5490, loss: 2.54989, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5491, loss: 2.54995, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5492, loss: 2.54999, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5493, loss: 2.54997, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5494, loss: 2.54990, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5495, loss: 2.54997, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5496, loss: 2.54994, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5497, loss: 2.54992, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5498, loss: 2.54990, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5499, loss: 2.54997, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5500, loss: 2.55009, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5501, loss: 2.55008, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5502, loss: 2.55001, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5503, loss: 2.55005, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5504, loss: 2.55009, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5505, loss: 2.55013, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5506, loss: 2.55007, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5507, loss: 2.55005, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5508, loss: 2.55007, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5509, loss: 2.55001, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5510, loss: 2.54999, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5511, loss: 2.54998, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5512, loss: 2.54993, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5513, loss: 2.54991, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5514, loss: 2.54983, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5515, loss: 2.54982, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5516, loss: 2.54981, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5517, loss: 2.54969, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5518, loss: 2.54965, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5519, loss: 2.54967, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5520, loss: 2.54967, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5521, loss: 2.54964, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5522, loss: 2.54969, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5523, loss: 2.54964, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5524, loss: 2.54964, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5525, loss: 2.54960, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5526, loss: 2.54962, accuracy: 0.13314\n",
            "Epoch: 1/10, step: 5527, loss: 2.54954, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5528, loss: 2.54960, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5529, loss: 2.54957, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5530, loss: 2.54964, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5531, loss: 2.54959, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5532, loss: 2.54957, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5533, loss: 2.54959, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5534, loss: 2.54957, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5535, loss: 2.54954, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5536, loss: 2.54950, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5537, loss: 2.54946, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5538, loss: 2.54943, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5539, loss: 2.54953, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5540, loss: 2.54954, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5541, loss: 2.54950, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5542, loss: 2.54946, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5543, loss: 2.54942, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5544, loss: 2.54948, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5545, loss: 2.54944, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5546, loss: 2.54946, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5547, loss: 2.54939, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5548, loss: 2.54936, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5549, loss: 2.54930, accuracy: 0.13318\n",
            "Epoch: 1/10, step: 5550, loss: 2.54932, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5551, loss: 2.54935, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5552, loss: 2.54933, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5553, loss: 2.54927, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5554, loss: 2.54930, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5555, loss: 2.54926, accuracy: 0.13319\n",
            "Epoch: 1/10, step: 5556, loss: 2.54929, accuracy: 0.13317\n",
            "Epoch: 1/10, step: 5557, loss: 2.54923, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5558, loss: 2.54918, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5559, loss: 2.54927, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5560, loss: 2.54928, accuracy: 0.13321\n",
            "Epoch: 1/10, step: 5561, loss: 2.54919, accuracy: 0.13320\n",
            "Epoch: 1/10, step: 5562, loss: 2.54919, accuracy: 0.13323\n",
            "Epoch: 1/10, step: 5563, loss: 2.54924, accuracy: 0.13322\n",
            "Epoch: 1/10, step: 5564, loss: 2.54913, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5565, loss: 2.54916, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5566, loss: 2.54906, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5567, loss: 2.54914, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5568, loss: 2.54916, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5569, loss: 2.54920, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5570, loss: 2.54917, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5571, loss: 2.54912, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5572, loss: 2.54904, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5573, loss: 2.54897, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5574, loss: 2.54897, accuracy: 0.13325\n",
            "Epoch: 1/10, step: 5575, loss: 2.54887, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5576, loss: 2.54880, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5577, loss: 2.54877, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5578, loss: 2.54876, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5579, loss: 2.54880, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5580, loss: 2.54878, accuracy: 0.13327\n",
            "Epoch: 1/10, step: 5581, loss: 2.54885, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5582, loss: 2.54876, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5583, loss: 2.54878, accuracy: 0.13324\n",
            "Epoch: 1/10, step: 5584, loss: 2.54878, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5585, loss: 2.54867, accuracy: 0.13328\n",
            "Epoch: 1/10, step: 5586, loss: 2.54872, accuracy: 0.13326\n",
            "Epoch: 1/10, step: 5587, loss: 2.54878, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5588, loss: 2.54872, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5589, loss: 2.54870, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5590, loss: 2.54872, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5591, loss: 2.54868, accuracy: 0.13329\n",
            "Epoch: 1/10, step: 5592, loss: 2.54855, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5593, loss: 2.54854, accuracy: 0.13331\n",
            "Epoch: 1/10, step: 5594, loss: 2.54845, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5595, loss: 2.54840, accuracy: 0.13338\n",
            "Epoch: 1/10, step: 5596, loss: 2.54848, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5597, loss: 2.54849, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5598, loss: 2.54856, accuracy: 0.13333\n",
            "Epoch: 1/10, step: 5599, loss: 2.54846, accuracy: 0.13337\n",
            "Epoch: 1/10, step: 5600, loss: 2.54850, accuracy: 0.13335\n",
            "Epoch: 1/10, step: 5601, loss: 2.54853, accuracy: 0.13332\n",
            "Epoch: 1/10, step: 5602, loss: 2.54853, accuracy: 0.13330\n",
            "Epoch: 1/10, step: 5603, loss: 2.54845, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5604, loss: 2.54847, accuracy: 0.13334\n",
            "Epoch: 1/10, step: 5605, loss: 2.54841, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5606, loss: 2.54851, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5607, loss: 2.54848, accuracy: 0.13338\n",
            "Epoch: 1/10, step: 5608, loss: 2.54852, accuracy: 0.13338\n",
            "Epoch: 1/10, step: 5609, loss: 2.54850, accuracy: 0.13336\n",
            "Epoch: 1/10, step: 5610, loss: 2.54846, accuracy: 0.13338\n",
            "Epoch: 1/10, step: 5611, loss: 2.54838, accuracy: 0.13342\n",
            "Epoch: 1/10, step: 5612, loss: 2.54830, accuracy: 0.13342\n",
            "Epoch: 1/10, step: 5613, loss: 2.54823, accuracy: 0.13344\n",
            "Epoch: 1/10, step: 5614, loss: 2.54819, accuracy: 0.13344\n",
            "Epoch: 1/10, step: 5615, loss: 2.54833, accuracy: 0.13344\n",
            "Epoch: 1/10, step: 5616, loss: 2.54837, accuracy: 0.13341\n",
            "Epoch: 1/10, step: 5617, loss: 2.54833, accuracy: 0.13343\n",
            "Epoch: 1/10, step: 5618, loss: 2.54816, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5619, loss: 2.54810, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5620, loss: 2.54810, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5621, loss: 2.54803, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5622, loss: 2.54794, accuracy: 0.13356\n",
            "Epoch: 1/10, step: 5623, loss: 2.54784, accuracy: 0.13358\n",
            "Epoch: 1/10, step: 5624, loss: 2.54779, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5625, loss: 2.54775, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5626, loss: 2.54768, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5627, loss: 2.54769, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5628, loss: 2.54759, accuracy: 0.13364\n",
            "Epoch: 1/10, step: 5629, loss: 2.54758, accuracy: 0.13366\n",
            "Epoch: 1/10, step: 5630, loss: 2.54755, accuracy: 0.13368\n",
            "Epoch: 1/10, step: 5631, loss: 2.54752, accuracy: 0.13366\n",
            "Epoch: 1/10, step: 5632, loss: 2.54753, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5633, loss: 2.54749, accuracy: 0.13365\n",
            "Epoch: 1/10, step: 5634, loss: 2.54748, accuracy: 0.13365\n",
            "Epoch: 1/10, step: 5635, loss: 2.54753, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5636, loss: 2.54758, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5637, loss: 2.54757, accuracy: 0.13358\n",
            "Epoch: 1/10, step: 5638, loss: 2.54760, accuracy: 0.13356\n",
            "Epoch: 1/10, step: 5639, loss: 2.54757, accuracy: 0.13358\n",
            "Epoch: 1/10, step: 5640, loss: 2.54752, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5641, loss: 2.54760, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5642, loss: 2.54762, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5643, loss: 2.54765, accuracy: 0.13364\n",
            "Epoch: 1/10, step: 5644, loss: 2.54775, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5645, loss: 2.54772, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5646, loss: 2.54771, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5647, loss: 2.54765, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5648, loss: 2.54757, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5649, loss: 2.54765, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5650, loss: 2.54774, accuracy: 0.13358\n",
            "Epoch: 1/10, step: 5651, loss: 2.54777, accuracy: 0.13356\n",
            "Epoch: 1/10, step: 5652, loss: 2.54784, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5653, loss: 2.54784, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5654, loss: 2.54783, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5655, loss: 2.54774, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5656, loss: 2.54776, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5657, loss: 2.54788, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5658, loss: 2.54781, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5659, loss: 2.54777, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5660, loss: 2.54784, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5661, loss: 2.54784, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5662, loss: 2.54778, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5663, loss: 2.54780, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5664, loss: 2.54776, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5665, loss: 2.54785, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5666, loss: 2.54784, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5667, loss: 2.54783, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5668, loss: 2.54776, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5669, loss: 2.54778, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5670, loss: 2.54773, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5671, loss: 2.54771, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5672, loss: 2.54758, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5673, loss: 2.54760, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5674, loss: 2.54766, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5675, loss: 2.54763, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5676, loss: 2.54760, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5677, loss: 2.54758, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5678, loss: 2.54760, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5679, loss: 2.54756, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5680, loss: 2.54749, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5681, loss: 2.54750, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5682, loss: 2.54745, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5683, loss: 2.54754, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5684, loss: 2.54754, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5685, loss: 2.54750, accuracy: 0.13347\n",
            "Epoch: 1/10, step: 5686, loss: 2.54756, accuracy: 0.13344\n",
            "Epoch: 1/10, step: 5687, loss: 2.54751, accuracy: 0.13344\n",
            "Epoch: 1/10, step: 5688, loss: 2.54739, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5689, loss: 2.54746, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5690, loss: 2.54759, accuracy: 0.13346\n",
            "Epoch: 1/10, step: 5691, loss: 2.54758, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5692, loss: 2.54763, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5693, loss: 2.54760, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5694, loss: 2.54766, accuracy: 0.13350\n",
            "Epoch: 1/10, step: 5695, loss: 2.54763, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5696, loss: 2.54764, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5697, loss: 2.54763, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5698, loss: 2.54761, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5699, loss: 2.54766, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5700, loss: 2.54756, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5701, loss: 2.54757, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5702, loss: 2.54759, accuracy: 0.13348\n",
            "Epoch: 1/10, step: 5703, loss: 2.54751, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5704, loss: 2.54747, accuracy: 0.13357\n",
            "Epoch: 1/10, step: 5705, loss: 2.54752, accuracy: 0.13357\n",
            "Epoch: 1/10, step: 5706, loss: 2.54748, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5707, loss: 2.54759, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5708, loss: 2.54755, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5709, loss: 2.54768, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5710, loss: 2.54767, accuracy: 0.13354\n",
            "Epoch: 1/10, step: 5711, loss: 2.54766, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5712, loss: 2.54777, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5713, loss: 2.54778, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5714, loss: 2.54770, accuracy: 0.13351\n",
            "Epoch: 1/10, step: 5715, loss: 2.54769, accuracy: 0.13349\n",
            "Epoch: 1/10, step: 5716, loss: 2.54760, accuracy: 0.13353\n",
            "Epoch: 1/10, step: 5717, loss: 2.54755, accuracy: 0.13357\n",
            "Epoch: 1/10, step: 5718, loss: 2.54762, accuracy: 0.13355\n",
            "Epoch: 1/10, step: 5719, loss: 2.54757, accuracy: 0.13352\n",
            "Epoch: 1/10, step: 5720, loss: 2.54752, accuracy: 0.13357\n",
            "Epoch: 1/10, step: 5721, loss: 2.54744, accuracy: 0.13356\n",
            "Epoch: 1/10, step: 5722, loss: 2.54746, accuracy: 0.13359\n",
            "Epoch: 1/10, step: 5723, loss: 2.54742, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5724, loss: 2.54738, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5725, loss: 2.54738, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5726, loss: 2.54736, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5727, loss: 2.54744, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5728, loss: 2.54746, accuracy: 0.13364\n",
            "Epoch: 1/10, step: 5729, loss: 2.54751, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5730, loss: 2.54752, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5731, loss: 2.54748, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5732, loss: 2.54735, accuracy: 0.13366\n",
            "Epoch: 1/10, step: 5733, loss: 2.54738, accuracy: 0.13368\n",
            "Epoch: 1/10, step: 5734, loss: 2.54734, accuracy: 0.13368\n",
            "Epoch: 1/10, step: 5735, loss: 2.54730, accuracy: 0.13367\n",
            "Epoch: 1/10, step: 5736, loss: 2.54728, accuracy: 0.13367\n",
            "Epoch: 1/10, step: 5737, loss: 2.54730, accuracy: 0.13367\n",
            "Epoch: 1/10, step: 5738, loss: 2.54729, accuracy: 0.13365\n",
            "Epoch: 1/10, step: 5739, loss: 2.54731, accuracy: 0.13365\n",
            "Epoch: 1/10, step: 5740, loss: 2.54739, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5741, loss: 2.54741, accuracy: 0.13360\n",
            "Epoch: 1/10, step: 5742, loss: 2.54749, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5743, loss: 2.54755, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5744, loss: 2.54756, accuracy: 0.13362\n",
            "Epoch: 1/10, step: 5745, loss: 2.54745, accuracy: 0.13366\n",
            "Epoch: 1/10, step: 5746, loss: 2.54745, accuracy: 0.13366\n",
            "Epoch: 1/10, step: 5747, loss: 2.54742, accuracy: 0.13363\n",
            "Epoch: 1/10, step: 5748, loss: 2.54756, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5749, loss: 2.54744, accuracy: 0.13361\n",
            "Epoch: 1/10, step: 5750, loss: 2.54736, accuracy: 0.13365\n",
            "Epoch: 1/10, step: 5751, loss: 2.54720, accuracy: 0.13367\n",
            "Epoch: 1/10, step: 5752, loss: 2.54710, accuracy: 0.13371\n",
            "Epoch: 1/10, step: 5753, loss: 2.54707, accuracy: 0.13371\n",
            "Epoch: 1/10, step: 5754, loss: 2.54697, accuracy: 0.13373\n",
            "Epoch: 1/10, step: 5755, loss: 2.54698, accuracy: 0.13373\n",
            "Epoch: 1/10, step: 5756, loss: 2.54704, accuracy: 0.13373\n",
            "Epoch: 1/10, step: 5757, loss: 2.54698, accuracy: 0.13373\n",
            "Epoch: 1/10, step: 5758, loss: 2.54684, accuracy: 0.13377\n",
            "Epoch: 1/10, step: 5759, loss: 2.54695, accuracy: 0.13375\n",
            "Epoch: 1/10, step: 5760, loss: 2.54694, accuracy: 0.13377\n",
            "Epoch: 1/10, step: 5761, loss: 2.54683, accuracy: 0.13379\n",
            "Epoch: 1/10, step: 5762, loss: 2.54690, accuracy: 0.13376\n",
            "Epoch: 1/10, step: 5763, loss: 2.54694, accuracy: 0.13374\n",
            "Epoch: 1/10, step: 5764, loss: 2.54689, accuracy: 0.13374\n",
            "Epoch: 1/10, step: 5765, loss: 2.54689, accuracy: 0.13372\n",
            "Epoch: 1/10, step: 5766, loss: 2.54693, accuracy: 0.13371\n",
            "Epoch: 1/10, step: 5767, loss: 2.54687, accuracy: 0.13371\n",
            "Epoch: 1/10, step: 5768, loss: 2.54682, accuracy: 0.13373\n",
            "Epoch: 1/10, step: 5769, loss: 2.54671, accuracy: 0.13378\n",
            "Epoch: 1/10, step: 5770, loss: 2.54664, accuracy: 0.13377\n",
            "Epoch: 1/10, step: 5771, loss: 2.54654, accuracy: 0.13375\n",
            "Epoch: 1/10, step: 5772, loss: 2.54648, accuracy: 0.13379\n",
            "Epoch: 1/10, step: 5773, loss: 2.54642, accuracy: 0.13379\n",
            "Epoch: 1/10, step: 5774, loss: 2.54645, accuracy: 0.13379\n",
            "Epoch: 1/10, step: 5775, loss: 2.54644, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5776, loss: 2.54650, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5777, loss: 2.54649, accuracy: 0.13381\n",
            "Epoch: 1/10, step: 5778, loss: 2.54640, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5779, loss: 2.54638, accuracy: 0.13385\n",
            "Epoch: 1/10, step: 5780, loss: 2.54645, accuracy: 0.13385\n",
            "Epoch: 1/10, step: 5781, loss: 2.54645, accuracy: 0.13382\n",
            "Epoch: 1/10, step: 5782, loss: 2.54639, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5783, loss: 2.54638, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5784, loss: 2.54632, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5785, loss: 2.54624, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5786, loss: 2.54623, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5787, loss: 2.54617, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5788, loss: 2.54619, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5789, loss: 2.54617, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5790, loss: 2.54617, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5791, loss: 2.54612, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5792, loss: 2.54610, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5793, loss: 2.54610, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5794, loss: 2.54615, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5795, loss: 2.54613, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5796, loss: 2.54606, accuracy: 0.13397\n",
            "Epoch: 1/10, step: 5797, loss: 2.54604, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5798, loss: 2.54605, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5799, loss: 2.54616, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5800, loss: 2.54616, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5801, loss: 2.54615, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5802, loss: 2.54619, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5803, loss: 2.54607, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5804, loss: 2.54607, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5805, loss: 2.54606, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5806, loss: 2.54604, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5807, loss: 2.54600, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5808, loss: 2.54593, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5809, loss: 2.54594, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5810, loss: 2.54598, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5811, loss: 2.54595, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5812, loss: 2.54591, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5813, loss: 2.54593, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5814, loss: 2.54599, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5815, loss: 2.54605, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5816, loss: 2.54600, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5817, loss: 2.54597, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5818, loss: 2.54593, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5819, loss: 2.54588, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5820, loss: 2.54586, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5821, loss: 2.54584, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5822, loss: 2.54583, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5823, loss: 2.54585, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5824, loss: 2.54586, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5825, loss: 2.54583, accuracy: 0.13397\n",
            "Epoch: 1/10, step: 5826, loss: 2.54582, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5827, loss: 2.54585, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5828, loss: 2.54593, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5829, loss: 2.54594, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5830, loss: 2.54594, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5831, loss: 2.54589, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5832, loss: 2.54585, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5833, loss: 2.54584, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5834, loss: 2.54580, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5835, loss: 2.54576, accuracy: 0.13400\n",
            "Epoch: 1/10, step: 5836, loss: 2.54573, accuracy: 0.13402\n",
            "Epoch: 1/10, step: 5837, loss: 2.54576, accuracy: 0.13399\n",
            "Epoch: 1/10, step: 5838, loss: 2.54570, accuracy: 0.13399\n",
            "Epoch: 1/10, step: 5839, loss: 2.54569, accuracy: 0.13399\n",
            "Epoch: 1/10, step: 5840, loss: 2.54570, accuracy: 0.13401\n",
            "Epoch: 1/10, step: 5841, loss: 2.54571, accuracy: 0.13401\n",
            "Epoch: 1/10, step: 5842, loss: 2.54564, accuracy: 0.13401\n",
            "Epoch: 1/10, step: 5843, loss: 2.54564, accuracy: 0.13399\n",
            "Epoch: 1/10, step: 5844, loss: 2.54558, accuracy: 0.13398\n",
            "Epoch: 1/10, step: 5845, loss: 2.54576, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5846, loss: 2.54574, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5847, loss: 2.54572, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5848, loss: 2.54570, accuracy: 0.13396\n",
            "Epoch: 1/10, step: 5849, loss: 2.54575, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5850, loss: 2.54572, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5851, loss: 2.54573, accuracy: 0.13393\n",
            "Epoch: 1/10, step: 5852, loss: 2.54572, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5853, loss: 2.54565, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5854, loss: 2.54572, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5855, loss: 2.54566, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5856, loss: 2.54567, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5857, loss: 2.54557, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5858, loss: 2.54552, accuracy: 0.13394\n",
            "Epoch: 1/10, step: 5859, loss: 2.54552, accuracy: 0.13392\n",
            "Epoch: 1/10, step: 5860, loss: 2.54559, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5861, loss: 2.54564, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5862, loss: 2.54563, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5863, loss: 2.54564, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5864, loss: 2.54565, accuracy: 0.13387\n",
            "Epoch: 1/10, step: 5865, loss: 2.54564, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5866, loss: 2.54564, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5867, loss: 2.54564, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5868, loss: 2.54568, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5869, loss: 2.54563, accuracy: 0.13384\n",
            "Epoch: 1/10, step: 5870, loss: 2.54562, accuracy: 0.13382\n",
            "Epoch: 1/10, step: 5871, loss: 2.54560, accuracy: 0.13384\n",
            "Epoch: 1/10, step: 5872, loss: 2.54559, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5873, loss: 2.54558, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5874, loss: 2.54560, accuracy: 0.13381\n",
            "Epoch: 1/10, step: 5875, loss: 2.54553, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5876, loss: 2.54552, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5877, loss: 2.54543, accuracy: 0.13385\n",
            "Epoch: 1/10, step: 5878, loss: 2.54542, accuracy: 0.13383\n",
            "Epoch: 1/10, step: 5879, loss: 2.54548, accuracy: 0.13382\n",
            "Epoch: 1/10, step: 5880, loss: 2.54550, accuracy: 0.13382\n",
            "Epoch: 1/10, step: 5881, loss: 2.54538, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5882, loss: 2.54542, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5883, loss: 2.54543, accuracy: 0.13384\n",
            "Epoch: 1/10, step: 5884, loss: 2.54534, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5885, loss: 2.54533, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5886, loss: 2.54531, accuracy: 0.13386\n",
            "Epoch: 1/10, step: 5887, loss: 2.54525, accuracy: 0.13388\n",
            "Epoch: 1/10, step: 5888, loss: 2.54523, accuracy: 0.13390\n",
            "Epoch: 1/10, step: 5889, loss: 2.54512, accuracy: 0.13391\n",
            "Epoch: 1/10, step: 5890, loss: 2.54510, accuracy: 0.13389\n",
            "Epoch: 1/10, step: 5891, loss: 2.54496, accuracy: 0.13395\n",
            "Epoch: 1/10, step: 5892, loss: 2.54493, accuracy: 0.13397\n",
            "Epoch: 1/10, step: 5893, loss: 2.54483, accuracy: 0.13397\n",
            "Epoch: 1/10, step: 5894, loss: 2.54477, accuracy: 0.13403\n",
            "Epoch: 1/10, step: 5895, loss: 2.54469, accuracy: 0.13405\n",
            "Epoch: 1/10, step: 5896, loss: 2.54472, accuracy: 0.13405\n",
            "Epoch: 1/10, step: 5897, loss: 2.54464, accuracy: 0.13407\n",
            "Epoch: 1/10, step: 5898, loss: 2.54445, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5899, loss: 2.54447, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5900, loss: 2.54453, accuracy: 0.13411\n",
            "Epoch: 1/10, step: 5901, loss: 2.54452, accuracy: 0.13411\n",
            "Epoch: 1/10, step: 5902, loss: 2.54448, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5903, loss: 2.54439, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5904, loss: 2.54435, accuracy: 0.13417\n",
            "Epoch: 1/10, step: 5905, loss: 2.54435, accuracy: 0.13417\n",
            "Epoch: 1/10, step: 5906, loss: 2.54433, accuracy: 0.13414\n",
            "Epoch: 1/10, step: 5907, loss: 2.54439, accuracy: 0.13412\n",
            "Epoch: 1/10, step: 5908, loss: 2.54431, accuracy: 0.13414\n",
            "Epoch: 1/10, step: 5909, loss: 2.54425, accuracy: 0.13412\n",
            "Epoch: 1/10, step: 5910, loss: 2.54423, accuracy: 0.13412\n",
            "Epoch: 1/10, step: 5911, loss: 2.54423, accuracy: 0.13409\n",
            "Epoch: 1/10, step: 5912, loss: 2.54426, accuracy: 0.13409\n",
            "Epoch: 1/10, step: 5913, loss: 2.54424, accuracy: 0.13411\n",
            "Epoch: 1/10, step: 5914, loss: 2.54420, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5915, loss: 2.54418, accuracy: 0.13413\n",
            "Epoch: 1/10, step: 5916, loss: 2.54423, accuracy: 0.13415\n",
            "Epoch: 1/10, step: 5917, loss: 2.54424, accuracy: 0.13415\n",
            "Epoch: 1/10, step: 5918, loss: 2.54415, accuracy: 0.13423\n",
            "Epoch: 1/10, step: 5919, loss: 2.54403, accuracy: 0.13425\n",
            "Epoch: 1/10, step: 5920, loss: 2.54404, accuracy: 0.13423\n",
            "Epoch: 1/10, step: 5921, loss: 2.54400, accuracy: 0.13425\n",
            "Epoch: 1/10, step: 5922, loss: 2.54396, accuracy: 0.13425\n",
            "Epoch: 1/10, step: 5923, loss: 2.54396, accuracy: 0.13422\n",
            "Epoch: 1/10, step: 5924, loss: 2.54395, accuracy: 0.13424\n",
            "Epoch: 1/10, step: 5925, loss: 2.54398, accuracy: 0.13428\n",
            "Epoch: 1/10, step: 5926, loss: 2.54397, accuracy: 0.13428\n",
            "Epoch: 1/10, step: 5927, loss: 2.54393, accuracy: 0.13430\n",
            "Epoch: 1/10, step: 5928, loss: 2.54403, accuracy: 0.13428\n",
            "Epoch: 1/10, step: 5929, loss: 2.54392, accuracy: 0.13430\n",
            "Epoch: 1/10, step: 5930, loss: 2.54388, accuracy: 0.13430\n",
            "Epoch: 1/10, step: 5931, loss: 2.54375, accuracy: 0.13436\n",
            "Epoch: 1/10, step: 5932, loss: 2.54365, accuracy: 0.13438\n",
            "Epoch: 1/10, step: 5933, loss: 2.54357, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 5934, loss: 2.54363, accuracy: 0.13440\n",
            "Epoch: 1/10, step: 5935, loss: 2.54368, accuracy: 0.13439\n",
            "Epoch: 1/10, step: 5936, loss: 2.54372, accuracy: 0.13439\n",
            "Epoch: 1/10, step: 5937, loss: 2.54375, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 5938, loss: 2.54378, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 5939, loss: 2.54376, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 5940, loss: 2.54361, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5941, loss: 2.54364, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5942, loss: 2.54353, accuracy: 0.13453\n",
            "Epoch: 1/10, step: 5943, loss: 2.54350, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 5944, loss: 2.54347, accuracy: 0.13457\n",
            "Epoch: 1/10, step: 5945, loss: 2.54340, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 5946, loss: 2.54335, accuracy: 0.13457\n",
            "Epoch: 1/10, step: 5947, loss: 2.54337, accuracy: 0.13454\n",
            "Epoch: 1/10, step: 5948, loss: 2.54344, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5949, loss: 2.54340, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5950, loss: 2.54338, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 5951, loss: 2.54342, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 5952, loss: 2.54351, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 5953, loss: 2.54345, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5954, loss: 2.54339, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5955, loss: 2.54344, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5956, loss: 2.54342, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5957, loss: 2.54343, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5958, loss: 2.54337, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5959, loss: 2.54341, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 5960, loss: 2.54338, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5961, loss: 2.54345, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 5962, loss: 2.54337, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5963, loss: 2.54329, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5964, loss: 2.54328, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 5965, loss: 2.54330, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5966, loss: 2.54337, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5967, loss: 2.54348, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5968, loss: 2.54347, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 5969, loss: 2.54338, accuracy: 0.13453\n",
            "Epoch: 1/10, step: 5970, loss: 2.54335, accuracy: 0.13453\n",
            "Epoch: 1/10, step: 5971, loss: 2.54341, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 5972, loss: 2.54331, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5973, loss: 2.54324, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5974, loss: 2.54323, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 5975, loss: 2.54317, accuracy: 0.13456\n",
            "Epoch: 1/10, step: 5976, loss: 2.54311, accuracy: 0.13458\n",
            "Epoch: 1/10, step: 5977, loss: 2.54319, accuracy: 0.13456\n",
            "Epoch: 1/10, step: 5978, loss: 2.54325, accuracy: 0.13453\n",
            "Epoch: 1/10, step: 5979, loss: 2.54322, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5980, loss: 2.54321, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5981, loss: 2.54316, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 5982, loss: 2.54318, accuracy: 0.13457\n",
            "Epoch: 1/10, step: 5983, loss: 2.54328, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 5984, loss: 2.54332, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 5985, loss: 2.54327, accuracy: 0.13454\n",
            "Epoch: 1/10, step: 5986, loss: 2.54333, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5987, loss: 2.54326, accuracy: 0.13454\n",
            "Epoch: 1/10, step: 5988, loss: 2.54330, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5989, loss: 2.54334, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5990, loss: 2.54337, accuracy: 0.13452\n",
            "Epoch: 1/10, step: 5991, loss: 2.54331, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5992, loss: 2.54337, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5993, loss: 2.54331, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5994, loss: 2.54336, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5995, loss: 2.54338, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5996, loss: 2.54335, accuracy: 0.13451\n",
            "Epoch: 1/10, step: 5997, loss: 2.54331, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 5998, loss: 2.54335, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 5999, loss: 2.54329, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6000, loss: 2.54335, accuracy: 0.13444\n",
            "Epoch: 1/10, step: 6001, loss: 2.54330, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6002, loss: 2.54330, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 6003, loss: 2.54330, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6004, loss: 2.54327, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 6005, loss: 2.54319, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6006, loss: 2.54325, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6007, loss: 2.54325, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6008, loss: 2.54331, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6009, loss: 2.54336, accuracy: 0.13444\n",
            "Epoch: 1/10, step: 6010, loss: 2.54340, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 6011, loss: 2.54338, accuracy: 0.13444\n",
            "Epoch: 1/10, step: 6012, loss: 2.54332, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6013, loss: 2.54326, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 6014, loss: 2.54324, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 6015, loss: 2.54320, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6016, loss: 2.54323, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 6017, loss: 2.54322, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 6018, loss: 2.54319, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 6019, loss: 2.54318, accuracy: 0.13443\n",
            "Epoch: 1/10, step: 6020, loss: 2.54310, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6021, loss: 2.54311, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6022, loss: 2.54307, accuracy: 0.13444\n",
            "Epoch: 1/10, step: 6023, loss: 2.54306, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 6024, loss: 2.54313, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 6025, loss: 2.54310, accuracy: 0.13444\n",
            "Epoch: 1/10, step: 6026, loss: 2.54316, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 6027, loss: 2.54319, accuracy: 0.13442\n",
            "Epoch: 1/10, step: 6028, loss: 2.54312, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6029, loss: 2.54308, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 6030, loss: 2.54309, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 6031, loss: 2.54307, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6032, loss: 2.54320, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6033, loss: 2.54319, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6034, loss: 2.54316, accuracy: 0.13445\n",
            "Epoch: 1/10, step: 6035, loss: 2.54308, accuracy: 0.13449\n",
            "Epoch: 1/10, step: 6036, loss: 2.54311, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 6037, loss: 2.54314, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6038, loss: 2.54315, accuracy: 0.13446\n",
            "Epoch: 1/10, step: 6039, loss: 2.54305, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 6040, loss: 2.54301, accuracy: 0.13450\n",
            "Epoch: 1/10, step: 6041, loss: 2.54301, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 6042, loss: 2.54288, accuracy: 0.13448\n",
            "Epoch: 1/10, step: 6043, loss: 2.54283, accuracy: 0.13447\n",
            "Epoch: 1/10, step: 6044, loss: 2.54277, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 6045, loss: 2.54275, accuracy: 0.13455\n",
            "Epoch: 1/10, step: 6046, loss: 2.54268, accuracy: 0.13457\n",
            "Epoch: 1/10, step: 6047, loss: 2.54265, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 6048, loss: 2.54263, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 6049, loss: 2.54258, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 6050, loss: 2.54264, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 6051, loss: 2.54263, accuracy: 0.13459\n",
            "Epoch: 1/10, step: 6052, loss: 2.54262, accuracy: 0.13456\n",
            "Epoch: 1/10, step: 6053, loss: 2.54270, accuracy: 0.13454\n",
            "Epoch: 1/10, step: 6054, loss: 2.54266, accuracy: 0.13454\n",
            "Epoch: 1/10, step: 6055, loss: 2.54263, accuracy: 0.13456\n",
            "Epoch: 1/10, step: 6056, loss: 2.54253, accuracy: 0.13460\n",
            "Epoch: 1/10, step: 6057, loss: 2.54248, accuracy: 0.13462\n",
            "Epoch: 1/10, step: 6058, loss: 2.54241, accuracy: 0.13462\n",
            "Epoch: 1/10, step: 6059, loss: 2.54243, accuracy: 0.13461\n",
            "Epoch: 1/10, step: 6060, loss: 2.54246, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6061, loss: 2.54260, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6062, loss: 2.54257, accuracy: 0.13465\n",
            "Epoch: 1/10, step: 6063, loss: 2.54253, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6064, loss: 2.54257, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6065, loss: 2.54257, accuracy: 0.13462\n",
            "Epoch: 1/10, step: 6066, loss: 2.54254, accuracy: 0.13462\n",
            "Epoch: 1/10, step: 6067, loss: 2.54255, accuracy: 0.13464\n",
            "Epoch: 1/10, step: 6068, loss: 2.54253, accuracy: 0.13464\n",
            "Epoch: 1/10, step: 6069, loss: 2.54251, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6070, loss: 2.54261, accuracy: 0.13464\n",
            "Epoch: 1/10, step: 6071, loss: 2.54249, accuracy: 0.13464\n",
            "Epoch: 1/10, step: 6072, loss: 2.54244, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6073, loss: 2.54239, accuracy: 0.13465\n",
            "Epoch: 1/10, step: 6074, loss: 2.54236, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6075, loss: 2.54241, accuracy: 0.13463\n",
            "Epoch: 1/10, step: 6076, loss: 2.54240, accuracy: 0.13465\n",
            "Epoch: 1/10, step: 6077, loss: 2.54234, accuracy: 0.13467\n",
            "Epoch: 1/10, step: 6078, loss: 2.54235, accuracy: 0.13467\n",
            "Epoch: 1/10, step: 6079, loss: 2.54229, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6080, loss: 2.54229, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6081, loss: 2.54226, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6082, loss: 2.54221, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6083, loss: 2.54224, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6084, loss: 2.54214, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6085, loss: 2.54212, accuracy: 0.13465\n",
            "Epoch: 1/10, step: 6086, loss: 2.54210, accuracy: 0.13467\n",
            "Epoch: 1/10, step: 6087, loss: 2.54195, accuracy: 0.13473\n",
            "Epoch: 1/10, step: 6088, loss: 2.54190, accuracy: 0.13475\n",
            "Epoch: 1/10, step: 6089, loss: 2.54186, accuracy: 0.13477\n",
            "Epoch: 1/10, step: 6090, loss: 2.54190, accuracy: 0.13475\n",
            "Epoch: 1/10, step: 6091, loss: 2.54194, accuracy: 0.13473\n",
            "Epoch: 1/10, step: 6092, loss: 2.54189, accuracy: 0.13473\n",
            "Epoch: 1/10, step: 6093, loss: 2.54187, accuracy: 0.13470\n",
            "Epoch: 1/10, step: 6094, loss: 2.54193, accuracy: 0.13468\n",
            "Epoch: 1/10, step: 6095, loss: 2.54196, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6096, loss: 2.54204, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6097, loss: 2.54201, accuracy: 0.13466\n",
            "Epoch: 1/10, step: 6098, loss: 2.54193, accuracy: 0.13468\n",
            "Epoch: 1/10, step: 6099, loss: 2.54195, accuracy: 0.13465\n",
            "Epoch: 1/10, step: 6100, loss: 2.54193, accuracy: 0.13467\n",
            "Epoch: 1/10, step: 6101, loss: 2.54191, accuracy: 0.13467\n",
            "Epoch: 1/10, step: 6102, loss: 2.54185, accuracy: 0.13469\n",
            "Epoch: 1/10, step: 6103, loss: 2.54186, accuracy: 0.13469\n",
            "Epoch: 1/10, step: 6104, loss: 2.54186, accuracy: 0.13471\n",
            "Epoch: 1/10, step: 6105, loss: 2.54182, accuracy: 0.13473\n",
            "Epoch: 1/10, step: 6106, loss: 2.54179, accuracy: 0.13472\n",
            "Epoch: 1/10, step: 6107, loss: 2.54174, accuracy: 0.13474\n",
            "Epoch: 1/10, step: 6108, loss: 2.54171, accuracy: 0.13476\n",
            "Epoch: 1/10, step: 6109, loss: 2.54170, accuracy: 0.13476\n",
            "Epoch: 1/10, step: 6110, loss: 2.54171, accuracy: 0.13480\n",
            "Epoch: 1/10, step: 6111, loss: 2.54162, accuracy: 0.13478\n",
            "Epoch: 1/10, step: 6112, loss: 2.54159, accuracy: 0.13478\n",
            "Epoch: 1/10, step: 6113, loss: 2.54163, accuracy: 0.13475\n",
            "Epoch: 1/10, step: 6114, loss: 2.54158, accuracy: 0.13475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0e09c8f120ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             print(\"Epoch: {}/{}, step: {}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n\u001b[1;32m     37\u001b[0m                                                                                      \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}